[
  {
    "title": "Which AWS service continuously monitors for malicious activity to protect your AWS accounts and workloads?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon GuardDuty",
        "status": "right",
        "explanation": "Amazon GuardDuty is a threat detection service that provides you with continuous monitoring of your AWS accounts and workloads to protect against malicious or unauthorized activities. GuardDuty analyzes and processes VPC Flow Logs and AWS CloudTrail event logs to detect unexpected and potentially unauthorized activities indicating a possible threat to your environment. This can include unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify threats more accurately. GuardDuty is easy to enable, doesnâ€™t require any additional software installation, and scales with your AWS usage."
      },
      {
        "title": "AWS Shield",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS WAF",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudWatch",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are associated with the Reliability Pillar of the AWS cloud? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Ability to recover from failure automatically.",
        "status": "right",
        "explanation": "The ability to recover from failure automatically is a key aspect of the Reliability Pillar in the AWS cloud. AWS provides various services and features that enable automated recovery mechanisms, such as Auto Scaling, Elastic Load Balancing, and Amazon CloudWatch. These services help in detecting failures, scaling resources, and distributing traffic to healthy instances, ensuring high availability and fault tolerance. By using these capabilities, applications can automatically recover from failures without manual intervention, enhancing reliability in the cloud environment."
      },
      {
        "title": "Implement the principle of least privilege to all AWS resources",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Pay only for the computing resources that a business requires.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Adopt serverless architecture whenever possible.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Scale horizontally to increase aggregate workload availability.",
        "status": "right",
        "explanation": "Horizontal scaling is another important aspect of the Reliability Pillar. AWS offers elastic scaling capabilities that allow applications to scale horizontally by adding more instances to distribute the workload. By scaling horizontally, the aggregate workload availability increases, as the workload is distributed across multiple instances. This approach helps in achieving fault tolerance, reducing the impact of failures, and maintaining consistent performance even during high traffic or demand spikes."
      }
    ]
  },
  {
    "title": "What is the main difference between Amazon EC2 and Amazon Lightsail?",
    "type": "radio",
    "options": [
      {
        "title": "EC2 provides more compute capacity and Lightsail provides less compute capacity.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "EC2 is less expensive and Lightsail is more expensive.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "EC2 is designed for high performance and Lightsail is designed for low latency.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "EC2 is a general-purpose computing service and Lightsail is a simplified computing service for small-scale web applications.",
        "status": "right",
        "explanation": "Amazon Elastic Compute Cloud (EC2) is a general-purpose computing service that provides scalable computing capacity in the cloud. EC2 provides a wide range of configuration options, including various instance types, operating systems, storage options, and network configurations. This makes EC2 suitable for a wide range of use cases, from small-scale web applications to large-scale data processing workloads. Amazon Lightsail, on the other hand, is a simplified computing service that is designed for small-scale web applications and blogs. Lightsail provides pre-configured virtual machines (VMs) that include everything needed to run a web application, such as a web server, database server, and operating system. Lightsail also provides a user-friendly management interface that makes it easy to set up and manage web applications. The main difference between EC2 and Lightsail is that EC2 is a general-purpose computing service with a wide range of configuration options, while Lightsail is a simplified compute service designed for small-scale web applications and blogs with fewer configuration options."
      }
    ]
  },
  {
    "title": "What is the primary advantage of migrating to the AWS Cloud?",
    "type": "radio",
    "options": [
      {
        "title": "Superior business excellence",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Higher employee retention",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Discounts on Amazon.com purchases",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Improved business flexibility",
        "status": "right",
        "explanation": "One of the advantages of migrating to the AWS Cloud is improved business flexibility. AWS allows businesses to quickly and easily scale resources up or down based on demand. This flexibility allows businesses to innovate faster, reduce time-to-market, and adapt to changing business requirements. With AWS, businesses can launch new applications, expand into new geographic regions, and experiment with new projects without making significant capital investments."
      }
    ]
  },
  {
    "title": "A company plans to use a NoSQL database on Amazon EC2 instances for its applications. In this setup, which responsibility falls under AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Updating the operating system inside the EC2 instances.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Setting up the firewall rules within the security groups for EC2 instances.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Ensuring the NoSQL database remains available without interruption.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Applying updates to the hardware infrastructure supporting the EC2 instances.",
        "status": "right",
        "explanation": "AWS is responsible for the patching and maintenance of the physical infrastructure that underpins its cloud services, including EC2 instances. This responsibility is part of the shared responsibility model, where AWS takes care of the cloud infrastructure, ensuring its reliability, security, and sustainability. When a company opts to run NoSQL databases on EC2 instances, AWS ensures that the physical servers, storage, and networking components are in optimal condition, which includes regular updates and patches to these physical components. This foundational support is crucial for maintaining the overall health and performance of the services that run on top of this infrastructure, such as EC2 instances. However, it's essential to understand that while AWS maintains the physical infrastructure, the customer is responsible for managing the software and configurations within their instances."
      }
    ]
  },
  {
    "title": "Which pillar of AWS Well-Architected ensures the right selection of resource types and optimized sizes for workload requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Security Pillar",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance Efficiency Pillar",
        "status": "right",
        "explanation": "The Performance Efficiency Pillar of the AWS Well-Architected Framework is all about using the right resources efficiently to meet your system requirements. It revolves around the efficient use of computing resources to meet the necessary system demands and maintaining that efficiency as demand changes and technology evolves. It involves the selection of appropriate resource types, sizes, and software configurations to achieve desired performance targets. By correctly aligning workload requirements with AWS services, you can achieve an optimal balance of performance and cost. Factors like selecting the right database, storage solutions, or compute services, and making decisions about configurations, play a crucial role in performance efficiency. This pillar guides you on how to make your application faster, leaner, and cheaper by using AWS resources efficiently."
      },
      {
        "title": "Cost Optimization Pillar",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reliability Pillar",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What are the benefits of moving from an on-premises database to an Amazon Relational Database Service (RDS)? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Apply software patching Automatically",
        "status": "right",
        "explanation": "One of the benefits of Amazon RDS is that it automatically patches the database software supporting your RDS instance, thus freeing you from the task of patching and updating your database systems. This reduces the time-consuming, often complex, process of database software patch management, enhancing system reliability and security."
      },
      {
        "title": "Scale vertically without downtime",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Support automated backup feature",
        "status": "right",
        "explanation": "Another advantage of Amazon RDS is its automatic backup feature. Amazon RDS makes it easy to go back in time with database snapshots and automated backups. It will automatically back up your database and keep your backup for a retention period that you specify. These backups include all your database transactions, allowing you to restore to any second during your retention period, up to the last five minutes."
      },
      {
        "title": "Run both SQL and NoSQL databases",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "No database administration required",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are best practices for securing data on an EBS Volume? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Encrypt Data at rest inside the EBS volume",
        "status": "right",
        "explanation": "Encrypting data at rest inside the EBS volume is a good practice to keep data safe. Encryption ensures that even if the EBS volume is compromised or stolen, the data remains unreadable and protected. By using encryption, sensitive information such as customer data, financial records, or personal information is safeguarded from unauthorized access. AWS provides the option to encrypt EBS volumes using AWS Key Management Service (KMS), which allows you to manage and control the encryption keys securely."
      },
      {
        "title": "Regularly update firmware on EBS devices",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create EBS snapshots",
        "status": "right",
        "explanation": "Creating EBS snapshots is another good practice for data protection. EBS snapshots are point-in-time copies of EBS volumes and can be used for backup and disaster recovery purposes. By regularly creating snapshots, you can ensure that you have a recent copy of your data in case of accidental deletion, hardware failure, or other data loss events. Snapshots can be used to restore EBS volumes or create new volumes in different regions or AWS accounts. This practice adds an additional layer of data protection and helps in maintaining business continuity."
      },
      {
        "title": "Removing unnecessary inbound rules of Security Groups",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Update password policy",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A firm has designed its AWS Cloud infrastructure to manage workloads and implemented a strategy for continuous process enhancement. Which AWS Well-Architected Framework pillar is illustrated by this approach?",
    "type": "radio",
    "options": [
      {
        "title": "Operational Excellence",
        "status": "right",
        "explanation": "The Operational Excellence pillar focuses on optimizing the operational aspects of a system. It emphasizes the efficient use of resources, automation, and continuous improvement. This pillar encourages organizations to adopt best practices for managing and monitoring their workloads, implementing automation to reduce manual tasks, and establishing clear operational procedures. It also emphasizes the importance of regular reviews, performance monitoring, and proactive measures to identify and address issues, ensuring that systems are efficient, reliable, and aligned with business objectives. The described scenario clearly represents the Operational Excellence Pillar of the AWS Well-Architected Framework. This pillar is about understanding and managing the operations of your workload. It also involves applying continuous improvements to operations procedures, learning from all operational failures, and keeping workloads highly available, resilient, and scalable to support business needs."
      },
      {
        "title": "Cost Optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance Efficiency",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to optimize its AWS resource configuration to minimize costs and enhance workload performance. Which AWS service can be used to meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Compute Optimizer",
        "status": "right",
        "explanation": "AWS Compute Optimizer is designed to help customers identify the optimal AWS resource configurations for their workloads. It provides recommendations to right-size resources, helping to reduce costs and increase performance by analyzing usage patterns. By leveraging machine learning algorithms, Compute Optimizer can suggest the best-fit Amazon EC2 instance types, auto-scaling groups, and EBS volumes. This service continuously monitors and evaluates the workloads, offering actionable insights that enable users to make informed decisions about their resource allocation, ensuring efficient and cost-effective cloud usage."
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A startup is running an application on multiple Amazon EC2 instances. The owner wants to receive an email notification whenever CPU usage exceeds 80%. Which AWS services should the startup use to meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon SQS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "right",
        "explanation": "The startup should also use Amazon CloudWatch to monitor the CPU usage of the EC2 instances. CloudWatch provides monitoring and observability services for resources and applications in AWS. By setting up a CloudWatch alarm to monitor the CPU utilization metric of the EC2 instances and defining a threshold of 80%, the owner can trigger actions, such as sending notifications via SNS, when the CPU usage exceeds the specified threshold."
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are best practices for AWS IAM access keys? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Create one access key to prevent theft.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use MFA for better security of access keys.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Rotate access keys periodically.",
        "status": "right",
        "explanation": "Rotating access keys periodically is a best practice for AWS IAM access keys. By regularly changing access keys, you reduce the risk of compromise and unauthorized access. It is recommended to rotate access keys on a regular basis, such as every 90 days, to enhance the security of your AWS resources."
      },
      {
        "title": "Do not use access keys.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Don't embed access keys directly into code.",
        "status": "right",
        "explanation": "It is a best practice to avoid embedding access keys directly into code. Hardcoding access keys in code can expose them to potential risks, such as accidental exposure through source code repositories or unauthorized access if the code is compromised. Instead, AWS provides mechanisms such as IAM roles, temporary security credentials, or environment variables for securely accessing AWS resources without embedding access keys directly into code."
      }
    ]
  },
  {
    "title": "Which AWS service provides decoupled communication for microservices-based applications?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Systems Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Connect",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SQS",
        "status": "right",
        "explanation": "Amazon Simple Queue Service (SQS) is another AWS service that provides decoupled communication for microservices-based applications. SQS is a fully managed message queuing service that enables you to decouple the components of a distributed application. Microservices can send messages to SQS queues, and other microservices can consume those messages at their own pace, allowing for asynchronous and decoupled communication."
      },
      {
        "title": "Amazon Pinpoint",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A business is planning to migrate its on-premises application to AWS and aiming to minimize latency by selecting infrastructure geographically close to its current operations. Which AWS feature enables the optimal placement of its application deployment area?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Direct Connect",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon VPC",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudFront",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Regions",
        "status": "right",
        "explanation": "AWS Regions are specific geographical locations around the world where Amazon Web Services hosts its cloud infrastructure. Each Region is a separate geographic area that contains multiple, isolated locations known as Availability Zones. AWS Regions enable users to deploy their applications close to their end-users, reducing latency, improving performance, and adhering to regional compliance and data residency requirements. By offering a broad selection of Regions, AWS allows customers to achieve higher fault tolerance and maintain the durability of their data. AWS Regions allow customers to choose the geographical location for deploying AWS services, including Amazon RDS, EC2. This capability is essential for businesses looking to migrate their on-premises databases to the cloud while ensuring low latency and adherence to data residency requirements. By selecting an AWS Region close to their current geographical location, the company can reduce the physical distance that data travels, resulting in faster access times for end-users."
      }
    ]
  },
  {
    "title": "What advantage does cloud computing offer over traditional on-premises IT services?",
    "type": "radio",
    "options": [
      {
        "title": "Traditional IT services offer more automation and orchestration capabilities than cloud services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cloud computing requires large upfront Capital Expenditure for infrastructure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "On-premises IT offers better scalability options than cloud computing.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cloud computing shifts IT spending from a Capital Expenditure (CapEx) to an Operational Expenditure (OpEx) model.",
        "status": "right",
        "explanation": "Cloud computing typically involves a pay-as-you-go or subscription-based pricing model, which shifts IT spending from upfront capital expenditure (CapEx) to operational expenditure (OpEx). This allows organizations to pay for only what they use and avoid the costs associated with purchasing and maintaining hardware and software."
      }
    ]
  },
  {
    "title": "A startup is planning to run an application on EC2 instances. They assume that the application needs more than 10 instances to run properly. The owner needs a solution to reduce monthly costs. As a Cloud Practitioner, which option would you suggest?",
    "type": "radio",
    "options": [
      {
        "title": "Using the AWS Network Load Balancer (NLB)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Removing Cost Allocation Tags",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Deploying across multiple Availability Zones",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enabling Auto Scaling for workloads",
        "status": "right",
        "explanation": "Auto Scaling allows the application to automatically adjust the number of instances based on demand, ensuring that the startup only runs the required number of instances at any given time. By configuring Auto Scaling, the startup can set up scaling policies based on predefined metrics such as CPU utilization, network traffic, or application response time. When the workload increases, Auto Scaling will automatically add more instances to handle the increased demand. Conversely, when the workload decreases, Auto Scaling will remove instances, reducing the number of active instances and consequently lowering costs."
      }
    ]
  },
  {
    "title": "According to security best practices, how should an EC2 instance be granted access to an S3 bucket?",
    "type": "radio",
    "options": [
      {
        "title": "Change the S3 bucket policy so that any service can access it at any time.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Write hard code IAM user's access key and secret key directly into the application.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create an IAM role for the EC2 instance to get access to the S3 bucket.",
        "status": "right",
        "explanation": "According to security best practices, the recommended approach to grant an EC2 instance access to an S3 bucket is by creating an IAM role. An IAM role allows you to grant permissions to AWS services such as EC2 instances without the need to hard code access keys or secrets into the application or EC2 instance. By creating an IAM role, you can define specific permissions for the EC2 instance to access the S3 bucket securely. The EC2 instance can then assume this role and make API calls to the S3 bucket without exposing access keys or secrets."
      },
      {
        "title": "Store access key and secret key into a text file in EC2 instance and read from the application.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is looking to migrate data to the AWS cloud using an Amazon Snowball Edge device. What activity can the company do for free?",
    "type": "radio",
    "options": [
      {
        "title": "Transferring data from Amazon S3 to Snowball Edge device.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Transferring data from Snowball Edge device to Amazon S3.",
        "status": "right",
        "explanation": "When using the Amazon Snowball Edge device, transferring data from the device into Amazon S3 incurs no additional fees. This process is integral to the value proposition of Snowball Edge, facilitating large-scale data migration into AWS with ease and efficiency. The absence of transfer costs for ingesting data into S3 from Snowball Edge encourages users to use this solution for massive data migration projects, ensuring a cost-effective and seamless transition to the cloud. This feature is designed to support extensive data transfer activities, aligning with AWS's objective to provide a streamlined and economical approach for importing large datasets into their cloud ecosystem. By eliminating the data transfer fees for uploads into S3, AWS enhances the appeal of using Snowball Edge for large-scale data migrations, aligning with the service's goal to simplify and economize the transition to cloud storage solutions."
      },
      {
        "title": "Using the Snowball Edge device for the first 10 days.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Daily use of the Snowball Edge device after the first 10 days.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement best describes Availability Zones?",
    "type": "radio",
    "options": [
      {
        "title": "Distinct locations within an AWS Region that are isolated from failures in other Availability Zones.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Content distribution network that is used to distribute content to users with fast speed and low latency.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A separate geographical location with multiple locations that are isolated from each other.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A geographic area that defines the physical locations of AWS data centers closest to target users.",
        "status": "right",
        "explanation": "Availability Zones are isolated locations within an AWS Region that are designed to be resilient and independent from one another. Each Availability Zone (AZ) is equipped with its own power, networking, and cooling infrastructure. They are physically separate from one another and connected through low-latency, high-bandwidth links. The purpose of Availability Zones is to provide fault tolerance and high availability for applications and services running in the AWS cloud. By distributing resources across multiple Availability Zones, customers can protect their applications from failures or disruptions in a single zone."
      }
    ]
  },
  {
    "title": "A company has 5 petabytes of data and needs to store it for backup, so they can restore during disaster recovery. Which S3 storage class should be used, which is cost-effective?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive)",
        "status": "right",
        "explanation": "S3 Glacier Deep Archive is the lowest-cost storage class offered by Amazon S3 and is designed for long-term data archiving and backup. It provides durability, security, and retrieval options optimized for infrequent access to the data. For cost-effective storage of 5 petabytes of data for backup purposes, the recommended option is Amazon S3 Glacier Deep Archive."
      },
      {
        "title": "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 Standard (S3 Standard)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A large global IT company is looking to enhance security. They want to ensure that all AWS IAM users can only log in using devices with MFA (multi-factor authentication). What is the best action to meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Send an email to all users requesting them to activate MFA.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use an IAM policy that denies all actions unless MFA is active on the user's account.",
        "status": "right",
        "explanation": "By using an IAM policy that denies all actions unless MFA is active, you are programmatically ensuring that users cannot perform actions in AWS unless they have MFA enabled on their account. This approach not only ensures adherence to the security policy but also removes the dependency on manual checks or user compliance."
      },
      {
        "title": "Disable all user accounts and only enable those who have MFA.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enable MFA at the AWS account level and hope users activate it.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the best practice for allowing users in AWS IAM policy management?",
    "type": "radio",
    "options": [
      {
        "title": "Ensuring users have administrative permissions to manage AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Allowing users to create and manage their own IAM policies.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Providing users with read-only access to all AWS resources.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Assigning users the minimum permissions necessary to complete their tasks.",
        "status": "right",
        "explanation": "The principle of least privilege is a fundamental security concept that involves granting users only the permissions they need to perform their job functions, and no more. This minimizes the risk of accidental or malicious actions that could compromise system security. In AWS IAM, this means carefully defining roles and policies to ensure users can access only the resources they require. By restricting permissions, organizations can significantly reduce their attack surface and limit the potential impact of a security breach."
      }
    ]
  },
  {
    "title": "Which tool allows you to upload images to S3 using programming language-specific APIs?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Software Developer Kit (SDK)",
        "status": "right",
        "explanation": "The AWS Software Developer Kit (SDK) allows you to upload images to S3 using programming language-specific APIs. The SDK provides libraries and APIs for various programming languages, such as Java, Python, Ruby, and more, which enable you to interact with AWS services, including S3. Using the SDK, you can write code that programmatically uploads images or any other files to an S3 bucket, giving you flexibility and control over the upload process. The SDK abstracts away the underlying REST API calls and provides a higher-level interface, making it easier to integrate S3 functionality into your applications."
      },
      {
        "title": "AWS Management Console (Browser)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Command Line Interface (CLI)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Integrated Development Environments (IDE)",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A startup has developed a research-based application for analyzing patient medical history. They want to deploy applications on EC2 instances. The application is designed with a fault-tolerant architecture and requires high I/O performance hardware disks. Which storage option would be the most cost-effective solution?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon S3",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EBS Provisioned IOPS SSD (io1/io2)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic File System (EFS)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Instance Store",
        "status": "right",
        "explanation": "Instance Store provides temporary block-level storage for EC2 instances. This storage is located on disks that are physically attached to the host computer and offer very high I/O performance, which is crucial for applications requiring high-speed processing of data, such as analysis of patient medical history. Since the Instance Store is directly attached to the instance, it is designed to deliver very high random and sequential I/O performance, making it suitable for tasks requiring high throughput and low latency. One of the primary advantages of using an Instance Store is cost-effectiveness. As a local storage option, it eliminates the network latency and costs associated with other types of storage. It can be particularly useful for applications where data persistence is not a primary concern, as Instance Store data is ephemeral and will be lost if the instance is stopped or terminated. Thus, it provides a compelling blend of performance and cost-efficiency for certain use cases"
      }
    ]
  },
  {
    "title": "Which AWS service would you use to keep data available even if an entire AWS region fails?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon RDS with Multi-AZ deployment",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 with Auto Scaling",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 with Cross-Region Replication (CRR)",
        "status": "right",
        "explanation": "Amazon S3's Cross-Region Replication (CRR) is designed to provide automatic and asynchronous copying of objects across buckets in different AWS regions. This ensures that data is available in another region if one region experiences a failure. By using CRR, you not only achieve high availability but also increase the durability of your data. With CRR, applications can be designed to be resilient to regional failures, ensuring uninterrupted access to critical data."
      }
    ]
  },
  {
    "title": "Which feature would you recommend to obtain a security and access audit of an Amazon S3 bucket?",
    "type": "radio",
    "options": [
      {
        "title": "VPC Endpoint",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Server Access Logging",
        "status": "right",
        "explanation": "Server Access Logging allows detailed logging of requests made to an S3 bucket. When enabled, S3 records each request made to the bucket and stores the information in another target S3 bucket. The logs capture important details such as the source IP address, request time, request type, request status, and more. This provides valuable insights into the usage of the S3 bucket, aiding in monitoring and auditing activities. Server Access Logging can help track access patterns, analyze traffic, detect anomalies, and ensure compliance with security and data governance requirements. It is a valuable tool for troubleshooting and optimizing S3 bucket usage."
      },
      {
        "title": "S3 Versioning",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Bucket Policy",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What happens to a Spot Instance if the Spot price increases and exceeds your maximum price?",
    "type": "radio",
    "options": [
      {
        "title": "The instance is terminated and cannot be recovered.",
        "status": "right",
        "explanation": "Spot Instances are an offering from AWS where you can bid for spare Amazon EC2 computing capacity. If your Spot Instance is running and the Spot price increases above your maximum price, AWS will automatically terminate your instance. The terminated instance cannot be recovered. This means that if you have not saved your work, it could potentially be lost. Therefore, it's essential to consider this factor when deciding whether to use Spot Instances for certain types of workloads."
      },
      {
        "title": "The instance is stopped and can be restarted when the price decreases.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The instance is automatically converted to an On-Demand instance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The instance is paused and can be resumed when the price decreases.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to run an application on multiple EC2 instances and needs to route traffic to these instances. Which AWS Route 53 policy should be used to route traffic to different instances and can also be chosen how much traffic is routed to each resource?",
    "type": "radio",
    "options": [
      {
        "title": "Simple routing policy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Weighted routing policy",
        "status": "right",
        "explanation": "The Weighted routing policy in AWS Route 53 should be used to route traffic to different EC2 instances and control how much traffic is routed to each resource. With the Weighted routing policy, you can assign weights to different resource records, which represent the EC2 instances in this case. The weight determines the proportion of traffic that is routed to each resource. For example, if you have two EC2 instances with weights of 70 and 30, respectively, 70% of the traffic will be directed to the first instance and 30% to the second instance. This policy provides flexibility in load balancing and allows you to distribute traffic based on your desired configuration, such as instance capacity, performance, or testing scenarios"
      },
      {
        "title": "Failover routing policy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IP-based routing policy",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "You have a microservices application running in the AWS cloud that is having performance and latency issues. Which AWS Service helps you to troubleshoot these issues?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cloud9",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS X-Ray",
        "status": "right",
        "explanation": "AWS X-Ray allows you to analyze and debug distributed applications, including microservices architectures. It provides end-to-end visibility into the application's behavior and performance by tracing requests as they flow across services. With X-Ray, you can identify bottlenecks, diagnose performance issues, and understand the dependencies and latency within your application. It helps you pinpoint the root cause of performance problems and optimize your application's performance. When troubleshooting performance and latency issues in a microservices application running in the AWS cloud, AWS X-Ray is the recommended service."
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An online streaming company is interested in deploying its application on AWS Cloud. They want to ensure the scalability of their application during peak times. How does elasticity improve the performance of an application?",
    "type": "radio",
    "options": [
      {
        "title": "It provides long-term storage solutions at low cost.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It quickly scaled up or down resources based on demand.",
        "status": "right",
        "explanation": "Amazon Web Services (AWS) offers elasticity as a core principle, allowing infrastructure resources to automatically scale up or down based on real-time demand. Elasticity ensures that applications can handle increases in traffic during demand spikes and decrease capacity during lulls, without any human intervention. This dynamic provisioning not only enhances application availability and performance but also results in cost savings. Key AWS services exemplifying elasticity include Amazon EC2 (Elastic Compute Cloud) for computing, Amazon RDS (Relational Database Service) for databases, and Amazon Auto Scaling to automatically adjust capacity. Leveraging AWS's elasticity, businesses can achieve operational efficiency and adapt to changing workloads seamlessly."
      },
      {
        "title": "It offers automatic backups for application recovery.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It offers a flat monthly rate for unlimited resources.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Under the AWS Shared Responsibility Model for containers, which statement is true?",
    "type": "radio",
    "options": [
      {
        "title": "AWS is responsible for Client and server-side encryption.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Customers are responsible for configuring firewalls and access management.",
        "status": "right",
        "explanation": "Customers using containers on AWS are responsible for configuring and managing the firewall and access controls for their containerized applications. This includes setting up security groups, network ACLs, and implementing appropriate access management policies to control inbound and outbound traffic to their containers."
      },
      {
        "title": "Both are responsible for configuring network infrastructures.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Customers are responsible for managing Platform and Guest OS.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following statements are true according to the Amazon VPC? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Create and manage policy to privilege all IAM users and groups.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Can manage security configurations for AWS infrastructure Network.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Have complete control over the virtual networking environment.",
        "status": "right",
        "explanation": "Amazon VPC allows you to configure network ACLs (Access Control Lists) that act as a firewall for controlling inbound and outbound traffic at the subnet level. Network ACLs provide an additional layer of security by allowing or denying specific types of traffic based on rules you define."
      },
      {
        "title": "Can configure failover settings so that it routes traffic to healthy resources.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Can configure network ACL that acts as a firewall for controlling traffic.",
        "status": "right",
        "explanation": "With Amazon VPC, you have complete control over the virtual networking environment. You can customize the network configuration, including IP address ranges, subnets, route tables, and network gateways. This control allows you to design and manage your network infrastructure according to your specific requirements."
      }
    ]
  },
  {
    "title": "A startup is creating a real-time online quiz platform and needs an AWS service to quickly store and access game results for a day without long-term retention. Which AWS service is suitable for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Enable Amazon DynamoDB Time to Live (TTL).",
        "status": "right",
        "explanation": "Amazon DynamoDB is a managed NoSQL database service that offers fast and predictable performance with scalability. For ephemeral data storage like game sessions, DynamoDB's TTL feature is an excellent fit. With TTL, items in the table can be automatically expired and deleted, ensuring that data isn't retained longer than necessary. Given the requirement for quick retrieval and data storage not exceeding 24 hours, enabling TTL on DynamoDB tables would allow the platform to efficiently manage the lifecycle of the session data without incurring additional costs for unnecessary storage."
      },
      {
        "title": "Deploy Amazon ElastiCache with data eviction policies.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon RDS with a daily deletion cron job.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon S3 with object expiration.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following benefits do customers get by using AWS Cloud? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Stop spending money running and maintaining data centers",
        "status": "right",
        "explanation": "By using Amazon Web Services (AWS), customers benefit from the massive economies of scale that Amazon has developed over years of running large-scale networks. AWS has invested heavily in infrastructure and passed those cost savings onto their customers. With their pay-as-you-go pricing model, customers can get the services and resources they need without having to invest in infrastructure upfront. Instead of having to estimate and plan for future needs, they can scale their usage up or down as required. This results in cost savings, increased efficiency, and the ability to focus on their core business instead of managing infrastructure."
      },
      {
        "title": "Benefit from massive economies of scale",
        "status": "right",
        "explanation": "The traditional approach to business technology infrastructure involves significant spending on building and maintaining data centers. However, with AWS, these costs and responsibilities are shifted to Amazon. AWS takes care of all the associated tasks such as hardware sourcing and installation, software patching, and network and power infrastructure management. Therefore, businesses using AWS can redirect resources and focus on projects that differentiate their business, rather than on the heavy lifting of racking, stacking, and powering servers."
      },
      {
        "title": "No responsibility for maintaining security",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Having to invest heavily in data centers and servers",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Easier to maintain a cloud network",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "In the AWS Shared Responsibility model, what responsibility falls under the customer when using Amazon DynamoDB?",
    "type": "radio",
    "options": [
      {
        "title": "Applying software updates and patches for DynamoDB",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Managing User Access Permissions to DynamoDB Tables",
        "status": "right",
        "explanation": "The responsibility of managing user access permissions to DynamoDB tables squarely falls on the customer. This involves using AWS Identity and Access Management (IAM) to control access to DynamoDB resources. Customers must carefully define IAM policies that specify who is allowed to access their DynamoDB tables and what actions they can perform. This is crucial for maintaining the security and integrity of the data stored in DynamoDB. It allows customers to implement least privilege access, ensuring that individuals and services only have the permissions necessary to perform their designated tasks."
      },
      {
        "title": "Ensuring the physical security of DynamoDB",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Ensuring encryption of data stored within DynamoDB",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to develop an application that will provide services on both web and mobile platforms. All users of this application can sign-in via social media like Facebook or Google. Which services should be used to implement this authentication?",
    "type": "radio",
    "options": [
      {
        "title": "AWS WAF",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Cognito",
        "status": "right",
        "explanation": "Amazon Cognito is a fully managed service that provides authentication, authorization, and user management for web and mobile applications. It supports social identity providers like Facebook and Google, allowing users to sign in using their social media accounts. With Amazon Cognito, developers can easily integrate user sign-up, sign-in, and access control functionalities into their applications across both web and mobile platforms. It takes care of the authentication process, token management, and user profile management, relieving developers from the complexities of building these features from scratch. Additionally, Amazon Cognito can be seamlessly integrated with other AWS services, enabling developers to leverage additional functionalities such as secure storage, data synchronization, and user management."
      },
      {
        "title": "AWS IAM",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Artifact",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the benefit of the economies of scale for cloud computing?",
    "type": "radio",
    "options": [
      {
        "title": "Increased capital expenditure for technology upgrades",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reduction in cost per unit as usage increases",
        "status": "right",
        "explanation": "The principle of economies of scale in cloud computing means that as providers like AWS grow larger and service more customers, they can achieve higher efficiency and lower the cost of operations per unit. This advantage is then passed on to the customers, who benefit from reduced costs as their usage scales. This model allows businesses to enjoy a reduction in cost per unit as their demand for computing resources increases, without the need for significant upfront capital investments in physical hardware and data centers. Cloud computing's pay-as-you-go pricing model further enhances this benefit, enabling companies to only pay for the resources they consume. This scalable and efficient approach to computing resources helps businesses optimize their IT spending, making technology more accessible and affordable, especially for startups and small to medium-sized enterprises that may not have the capital for large investments in IT infrastructure."
      },
      {
        "title": "Higher variable costs compared to fixed costs",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Requirements for long-term commitments",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service or resource provides best practices, and answers to the most frequently asked security-related questions regarding AWS accounts and resources?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Artifact",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Chatbot",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Marketplace",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Knowledge Center",
        "status": "right",
        "explanation": "The AWS Knowledge Center is a valuable resource for anyone seeking information about best practices, troubleshooting tips, and answers to frequently asked questions related to AWS accounts and resources. The Knowledge Center provides comprehensive guidance on a wide range of topics including security, billing, and technical issues. By accessing the Knowledge Center, users can benefit from detailed articles authored by AWS experts, which not only help in resolving everyday issues but also offer insights on how to enhance the security and efficiency of their AWS environments."
      }
    ]
  },
  {
    "title": "Which of the following statements are correct about the AWS VPC? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "An NACL contains only allowed rules.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A Security Group has both allowed and denied rules.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A Security Group has only allowed rules.",
        "status": "right",
        "explanation": "A Security Group in AWS VPC allows you to specify inbound and outbound rules that control the traffic flow. It operates based on \"allow\" rules, meaning you explicitly define what traffic is allowed to enter or leave the associated resources. By default, all inbound and outbound traffic is denied unless specifically allowed through the defined rules."
      },
      {
        "title": "Both Security Group and NACL contain only allowed rules.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An NACL contains both allowed and denied rules.",
        "status": "right",
        "explanation": "Network Access Control Lists (NACLs) in AWS VPC provide an additional layer of security by controlling inbound and outbound traffic at the subnet level. Unlike Security Groups, NACLs support both \"allow\" and \"deny\" rules. This means you can specify both allowed and denied rules to control traffic flow at the subnet level."
      }
    ]
  },
  {
    "title": "Which AWS service should you use to get a prediction of next month's bills for the services you use?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Simple Monthly Calculator",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Billing",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cost Explorer",
        "status": "right",
        "explanation": "AWS Cost Explorer provides comprehensive cost visibility and analysis for your AWS resources. It allows you to visualize, understand, and manage your AWS costs effectively. With AWS Cost Explorer, you can access a wide range of cost reports, including forecasted costs. The forecasted costs feature enables you to estimate your expenses for the upcoming month based on historical usage patterns and current resource utilization. It provides valuable insights into how your costs are expected to change and helps you plan your budget accordingly. By using AWS Cost Explorer's forecasted costs, you can proactively anticipate and optimize your spending. This empowers you to make informed decisions, adjust your resource allocation, and implement cost-saving measures before the next billing cycle."
      }
    ]
  },
  {
    "title": "For compliance auditing, a business needs to review how frequently passwords and access keys are updated within its AWS environment. Which AWS feature or service can provide this information?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Audit Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Artifact",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IAM Access Analyzer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IAM Credential Report",
        "status": "right",
        "explanation": "The IAM Credential Report is designed to help customers manage and audit their IAM users' security credentials, including passwords, access keys, and MFA (Multi-Factor Authentication) status. This report offers a comprehensive snapshot of the credential status for all users within the AWS account. It allows administrators to quickly assess the state of user credentials, including when passwords and access keys were last rotated, thereby ensuring adherence to security best practices and compliance requirements. The ease of accessing this detailed information simplifies the process of auditing and enforcing policies related to credential rotation, aligning with the company's need to monitor and verify compliance within its AWS environment."
      }
    ]
  },
  {
    "title": "A startup wants to develop an application using a data pattern matching algorithm. What type of Amazon EC2 instance should they purchase?",
    "type": "radio",
    "options": [
      {
        "title": "Accelerated Computing",
        "status": "right",
        "explanation": "Accelerated computing instances use hardware accelerators, or co-processors, to perform functions, such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on CPUs. Use Cases: Machine learning, high performance computing, computational fluid dynamics, computational finance, seismic analysis, speech recognition, autonomous vehicles, and drug discovery"
      },
      {
        "title": "Storage Optimized",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Memory Optimized",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Compute Optimized",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "If you're in early development on AWS and want the ability to get technical support during business hours, which AWS support plan is suitable for you?",
    "type": "radio",
    "options": [
      {
        "title": "Business",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Basic",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enterprise",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Developer",
        "status": "right",
        "explanation": "The Developer support plan is the most suitable for customers who are in the early stages of development on AWS and want the ability to get technical support during business hours. This support plan includes client-side diagnostic tools, best practice guidance, and email access to cloud support associates during local business hours, offering a response time of 24 hours. This plan is designed to meet the needs of developers and users testing and experimenting with AWS solutions."
      }
    ]
  },
  {
    "title": "A startup is planning to deploy a new e-commerce application in the US-East-1 region. But they want to deliver their products worldwide. Which AWS services can be used to provide high performance with low latency to users worldwide?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Macie",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Transit Gateway",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Route 53",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudFront",
        "status": "right",
        "explanation": "Amazon CloudFront is a content delivery network (CDN) service that can be used to deliver content with low latency and high performance to users worldwide. CloudFront has a global network of edge locations that cache and serve content from locations closer to the end-users, reducing the latency in accessing the application. By using CloudFront, the startup can distribute their e-commerce application's static and dynamic content globally, ensuring faster delivery and an improved user experience."
      }
    ]
  },
  {
    "title": "A multinational company wants to centralize and standardize security alerts across its various AWS services, including integration with third-party security solutions. Which AWS services can be used for monitoring and analysis in a standardized format?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon GuardDuty",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Security Hub",
        "status": "right",
        "explanation": "AWS Security Hub is designed as a cloud security posture management (CSPM) service that excels in aggregating, organizing, and prioritizing security alerts or findings from AWS services such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from various AWS partner solutions. It provides a comprehensive view of your security state within AWS and helps you check your environment against security industry standards and best practices. Security Hub is the best choice for company who looking to streamline their security monitoring and compliance processes across multiple AWS accounts and services. By centralizing and standardizing security findings in a standardized format, Security Hub simplifies the management of security alerts and enhances the visibility of your security posture across your AWS infrastructure. This capability is particularly valuable for organizations operating in a complex, multi-account AWS environment, enabling them to effectively identify and respond to potential security issues."
      }
    ]
  },
  {
    "title": "Which AWS service or feature enables distributed applications to send both text messages and emails?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon SES",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SNS",
        "status": "right",
        "explanation": "Amazon Simple Notification Service (SNS) is a fully managed messaging service that enables you to organize and send notifications from the cloud efficiently. This service supports multiple messaging patterns, including pub/sub, SMS, email, and mobile push notifications, making it versatile for a variety of communication needs. SNS is designed to enable applications to send messages to a large number of subscribers, including direct sends to users via SMS and email. Its ability to work with distributed systems makes it an ideal choice for applications requiring scalable and flexible messaging solutions. This wide range of messaging capabilities, coupled with high availability and reliability, ensures SNS is the best solution for applications looking to implement robust notification systems across different communication channels."
      },
      {
        "title": "Amazon SQS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch alerts",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to use Amazon CloudFront distribution for its application contents. They need to know how it impacts the cost. Which of the following impacts the cost of Amazon CloudFront?",
    "type": "radio",
    "options": [
      {
        "title": "Data transfers IN",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Total volume of data",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Data transfers OUT",
        "status": "right",
        "explanation": "Data transfers OUT AWS CloudFront is a content delivery network (CDN) service offered by Amazon Web Services (AWS) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. CloudFront integrates with AWS both physically and through the AWS network architecture, enabling dynamic content acceleration and distribution of static and dynamic web content. CloudFront charges are based on several factors: the amount of data transferred OUT to the internet, the number of HTTP/HTTPS requests made, and additional costs for optional features like custom SSL certificates or field-level encryption. Pricing can vary by geographic region, reflecting the cost of delivering content in different parts of the world."
      },
      {
        "title": "Age of caching time",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company manages multiple Amazon EC2 instances for various projects. They want to track expenses for these applications and allocate them to the relevant projects. Which of the following actions should they take to meet this requirement? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Enable billing alerts through Amazon Budget.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Run a cost allocation report from AWS Billing.",
        "status": "right",
        "explanation": "Tags are key-value pairs that can be assigned to AWS resources such as EC2 instances. By creating tags with project-specific information and associating them with the instances, the company can easily identify and categorize the expenses incurred by each project. Tags can be used for cost allocation, filtering, and organizing resources, making it convenient to track and analyze expenses at the project level."
      },
      {
        "title": "Create tags for each project and allocate them to instances.",
        "status": "right",
        "explanation": "Running a cost allocation report from AWS Billing is another action the company should take. AWS Billing provides detailed billing and cost management information, including the ability to generate cost allocation reports. These reports allow you to view and analyze costs based on various dimensions, such as projects, tags, accounts, and more. By running cost allocation reports, the company can obtain granular cost breakdowns and insights to accurately track and allocate expenses for each project."
      },
      {
        "title": "Use separate AWS accounts for each project.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create additional VPCs for each project.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A financial-based company has a large dataset stored in an Amazon S3 bucket. The security team wants to identify sensitive information to protect against data leakage. Which AWS service should you recommend to meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Firewall Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Secrets Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Macie",
        "status": "right",
        "explanation": "Amazon Macie is a powerful security service that uses machine learning and artificial intelligence to automatically discover, classify, and protect sensitive data. It can analyze the content of files stored in S3 buckets and identify various types of sensitive information, such as personally identifiable information (PII), financial data, intellectual property, and more. Amazon Macie provides visibility into your data and helps you understand the sensitivity and risk associated with it. It generates alerts and notifications for potential data leaks or unauthorized access, allowing you to take appropriate actions to protect your sensitive data."
      }
    ]
  },
  {
    "title": "A Trading company needs a cost-effective AWS solution to analyze financial markets and execute trades within milliseconds. Their applications require uninterrupted computing power during trading. Outside of these hours, the application requires minimal resources. What combination of AWS purchase options aligns with these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Spot Instances with Spot Block for trading hours and Reserved Instances for baseline usage.",
        "status": "wrong",
        "explanation": "While Spot Instances with Spot Block may offer cost savings during trading hours, there is a risk of interruption if the Spot price exceeds the bid price. This may not be suitable for applications requiring uninterrupted computing power during critical trading hours. Additionally, using Reserved Instances for baseline usage may not provide the flexibility needed for scaling resources during usage spikes."
      },
      {
        "title": "On-Demand Instances for trading hours and Spot Instances for off-hours.",
        "status": "wrong",
        "explanation": "Using On-Demand Instances for trading hours and Spot Instances for off-hours may not be the most cost-effective solution for the Trading company. Spot Instances can be interrupted at any time if the Spot price exceeds your bid price, which may not be suitable for applications requiring uninterrupted computing power during trading hours."
      },
      {
        "title": "Savings Plans for baseline usage and On-Demand Instances for trading hours.",
        "status": "wrong",
        "explanation": "Savings Plans can provide cost savings for baseline usage, but they may not offer the flexibility needed for scaling resources during trading hours. Using On-Demand Instances for trading hours may result in higher costs compared to using Reserved Instances, which offer a significant discount for continuous baseline usage. This combination may not align well with the Trading company's requirements for cost-effective and uninterrupted computing power during critical trading hours."
      },
      {
        "title": "Reserved Instances for continuous baseline usage and On-Demand Instances for usage spikes.",
        "status": "right",
        "explanation": "Reserved Instances are ideal for the company's baseline usage because they provide a discounted hourly rate for committing to a specific amount of compute capacity for a 1 or 3-year term, ensuring both cost savings and capacity reservation. Given the nature of the financial application, having a reliable and uninterrupted service during trading hours is crucial, and Reserved Instances can guarantee this for baseline usage. During sporadic usage spikes, particularly in the fast-paced financial market, the company can benefit from the flexibility of On-Demand Instances. They can instantly scale computing power without making a long-term commitment, which is essential for maintaining high availability and performance during critical periods without incurring costs during off-peak times."
      }
    ]
  },
  {
    "title": "A business has Docker containers running on Amazon EC2 instances. This business is looking for a better way to automatically adjust server numbers, manage application placement, and maintain an up-to-date computing environment with minimal effort. Which AWS service would be the most suitable for achieving these goals?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Athena",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Fargate",
        "status": "right",
        "explanation": "AWS Fargate is a serverless compute engine designed for running containers. It allows users to run containers without having to manage servers or clusters. Fargate takes care of the scaling, scheduling, and infrastructure management, which makes it an ideal solution for companies looking to automate these aspects of their Docker environments. With Fargate, you specify the CPU and memory requirements for your containers, define your networking and IAM policies, and let Fargate launch and manage the containers for you. This service eliminates the need to provision, configure, and scale clusters of virtual machines to run containers, making it a seamless and efficient choice for deploying applications in containers."
      },
      {
        "title": "Amazon RDS",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A digital retail startup wants to implement a relational database on AWS that ensures rapid data retrieval and continuous read/write operations. Which Amazon EBS volume type should they consider?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon EBS Provisioned IOPS SSD (io2)",
        "status": "right",
        "explanation": "Amazon EBS Provisioned IOPS SSD (io1/io2) is an optimal choice for latency-sensitive and high-performance workloads that require consistent I/O operations. It is designed to provide low latency and ensures a consistent number of I/O operations per second (IOPS). The \"provisioned IOPS\" feature allows users to specify the IOPS based on their workload requirements, thus making it highly suitable for transactional applications, such as relational databases, that need a predictable and fast I/O performance."
      },
      {
        "title": "Amazon EBS General Purpose SSD (gp3)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EBS Cold HDD (sc1)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EBS Throughput Optimized HDD (st1)",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the key benefit of integrating AWS IAM Identity Center (AWS Single Sign-On) with a company's Directory Services?",
    "type": "radio",
    "options": [
      {
        "title": "It ensures compliance with data residency requirements by restricting data transfer outside of company networks.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It allows employees to use their existing credentials to access AWS services and applications.",
        "status": "right",
        "explanation": "Integrating AWS IAM Identity Center with company directory services allows employees to use their existing credentials to access AWS accounts and cloud applications, simplifying the user experience and improving security. This integration eliminates the need for separate credentials for AWS services, reducing password fatigue and the risk of compromised credentials, while also streamlining the management of user access."
      },
      {
        "title": "It provides unlimited storage for any kind of company data within the AWS cloud.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It offers a direct VPN connection to the company servers for a faster network experience.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which option is the key functionality of the AWS Cloud Adoption Framework (AWS CAF)?",
    "type": "radio",
    "options": [
      {
        "title": "Governance",
        "status": "right",
        "explanation": "Governance is fundamentally about establishing and maintaining control over IT resources and services in the cloud. This includes the implementation of policies, procedures, and technologies to manage and monitor applications, data, and infrastructure. Governance is crucial for ensuring that IT operations align with business objectives, comply with regulatory requirements, and manage risk effectively. In the AWS CAF, governance encompasses a range of practices such as financial management, risk management, and compliance, as well as resource and access management within the cloud environment. These practices help organizations to maintain oversight and control over their cloud resources, ensuring that they are used efficiently, securely, and in a way that supports the organization's overall goals. The emphasis on governance in the AWS CAF highlights its role as a foundational element of cloud adoption, enabling organizations to scale their cloud use with confidence while managing costs, security, and compliance risks."
      },
      {
        "title": "Reliability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Sustainability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance efficiency",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is configuring AWS Identity and Access Management (IAM) for enhanced security. Which recommendation aligns with IAM security best practices?",
    "type": "radio",
    "options": [
      {
        "title": "Use the root account for daily operations.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Set permissions using inline policies exclusively.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Share IAM user credentials among team members.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enable multi-factor authentication (MFA) for all users.",
        "status": "right",
        "explanation": "Enabling multi-factor authentication (MFA) for all users is a critical security measure recommended by AWS IAM best practices. MFA adds an additional layer of security by requiring users to provide two or more forms of identification before being granted access. This typically involves something they know (like a password) and something they have (such as a code from a smartphone app or an SMS text message). MFA significantly reduces the risk of unauthorized access resulting from compromised credentials, as an attacker would need both the user's password and physical access to their MFA device. Implementing MFA for all users, especially for those with elevated permissions, helps protect sensitive resources and data stored in AWS services against potential breaches."
      }
    ]
  },
  {
    "title": "A company wants to pay only for the resources it uses and needs the ability to increase or decrease resource usage to meet business requirements. Which AWS Well-Architected Framework pillar aligns with these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Reliability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational excellence",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cost optimization",
        "status": "right",
        "explanation": "The Cost Optimization pillar focuses on how to design and operate workloads to deliver business value at the lowest possible cost. It includes paying only for the computing resources that you require and the ability to increase or decrease resource usage based on business requirements, without relying on elaborate forecasting. For example, development and test environments are typically used for only eight hours a day during the workweek. You can stop these resources when they are not in use to save costs."
      },
      {
        "title": "Security",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Your company has an application for internal use only, which runs on an EC2 instance. For security reasons, you need to block all other incoming requests to the EC2 instance. Which of the following will help you achieve this?",
    "type": "radio",
    "options": [
      {
        "title": "Security Group",
        "status": "right",
        "explanation": "Security Groups act as a virtual firewall at the instance level, controlling both inbound and outbound traffic to your EC2 instances. By using security groups effectively, you can secure your application's EC2 instances from unwanted incoming requests. You could set up rules to only allow traffic from specific IP addresses or ranges, such as those used internally by your company. This means that any requests originating from outside your specified range will be automatically denied. The use of security groups in this way provides a robust and customizable means of managing access to your EC2 instances. It's important to remember that Security Groups operate at the instance level, not at the subnet level, which allows for fine-tuned control over access to your instances."
      },
      {
        "title": "Internet Gateway",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IAM MFA",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IAM Policy",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to the AWS Penetration Testing Policy, which of the following statements is true?",
    "type": "radio",
    "options": [
      {
        "title": "AWS does not support Penetration testing",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Customers can perform penetration testing on their EC2 instance",
        "status": "right",
        "explanation": "According to the AWS Penetration Testing Policy, you can perform Penetration testing on EC2 instances. AWS allows customers to conduct penetration testing on their own EC2 instances to assess the security posture of their applications and environments. However, there are certain guidelines and requirements that need to be followed, such as obtaining prior authorization from AWS, performing testing only on their own resources, and adhering to the rules outlined in the AWS Penetration Testing Policy."
      },
      {
        "title": "Customers are allowed to perform penetration testing on Route 53 Hosted Zones",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Only AWS can perform Penetration testing",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the key strategy recommended by AWS for a successful cloud migration?",
    "type": "radio",
    "options": [
      {
        "title": "Use the AWS Migration Hub to track the progress of applications during migration.",
        "status": "right",
        "explanation": "AWS Migration Hub provides a central location to track the progress of application migrations across multiple AWS and partner solutions. It simplifies the process by providing visibility into the status of migrations, allowing businesses to monitor their application portfolio migrations, and ensuring that everything is proceeding as planned."
      },
      {
        "title": "Maintain legacy systems without modifications during the migration process.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Over-provision resources to ensure availability during migration.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Avoid assessing applications and databases before migration to save time.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "If your company has AWS Enterprise Support plan, who should you contact for billing or account inquiries?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Concierge Support team",
        "status": "right",
        "explanation": "The AWS Concierge Support team is a specialized support for customers with significant infrastructure or spending on AWS. It offers a personalized, high-touch experience, serving as a dedicated point of contact for customers. The Concierge Support team collaborates with customers to understand their unique requirements and challenges, provides proactive guidance and assistance to optimize AWS infrastructure, troubleshoots issues, and ensures smooth operations. They offer expertise and recommendations tailored to specific customer needs, helping them make informed decisions, improve performance, enhance security, and maximize the value of their AWS investment. The AWS Concierge team provides account assistance to Enterprise Support customers, including best practice guidance on account setup, billing analysis, and cost optimization recommendations. They are dedicated to helping you get the most out of AWS services and features."
      },
      {
        "title": "AWS Marketplace Seller",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Abuse team",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Partner Network (APN)",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has an application running in the AWS cloud and uses an Oracle database from an on-premise data center. They want to move the database to the AWS cloud to increase security and reduce costs. Which AWS service should they use to migrate the database without negatively impacting the performance of the source database?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Database Migration Service",
        "status": "right",
        "explanation": "To migrate the Oracle database from an on-premise data center to the AWS cloud without negatively impacting performance, the company should use AWS Database Migration Service (DMS). AWS Database Migration Service (DMS) is a fully managed service that simplifies the database migration process. It supports homogeneous and heterogeneous migrations, including Oracle to AWS, while minimizing downtime and reducing the impact on the source database. DMS performs schema conversion, data replication, and continuous data integration, ensuring a smooth and efficient migration. It uses a replication instance to securely transfer data from the on-premise database to the AWS cloud. DMS also supports ongoing replication to keep the target database synchronized with changes in the source database, allowing for a seamless transition."
      },
      {
        "title": "AWS Transfer Family",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS DataSync",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Application Discovery Service",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company recently migrated its workloads to the AWS cloud and realized that application deployment times reduced from 1-2 weeks to 2-3 days. What benefit does the company gain from the AWS cloud?",
    "type": "radio",
    "options": [
      {
        "title": "Resilience",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Elasticity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Agility",
        "status": "right",
        "explanation": "Agility refers to the ability to rapidly and efficiently respond to changing business needs and market conditions. It involves the capability to quickly deploy, scale, and adapt applications and services. Agility allows organizations to be more responsive, iterate faster, and seize opportunities in a dynamic and competitive landscape. It is achieved through the use of cloud technologies, such as automation, DevOps practices, and scalable infrastructure, which enable faster development, deployment, and delivery of software solutions. In our case, the application deployment time has been reduced from 1-2 weeks to 2-3 days as a demonstration of improved agility. The AWS cloud allows for quick provisioning of resources as needed, which dramatically reduces the time to deploy new applications. This allows organizations to innovate more quickly and react to changes faster. It significantly reduces the risks associated with over-provisioning or under-provisioning and allows companies to be more responsive to the needs of their business."
      },
      {
        "title": "Flexibility",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A startup is using the AWS Free Tier for an application. What happens when the AWS Free Tier expires or the application exceeds the Tree Tier limit?",
    "type": "radio",
    "options": [
      {
        "title": "The company will be billed the regular pay-as-you-go service rates for usage that exceeds the Free Tier limit.",
        "status": "right",
        "explanation": "The AWS Free Tier is a program that offers new customers the opportunity to explore and use certain AWS services at no cost for a limited time period. It provides an allocation of free usage for various AWS resources, including compute instances, storage, databases, messaging services, and more. The Free Tier allows users to experiment, learn, and build applications in the AWS environment without incurring charges. It is designed to help users get started with AWS services and understand their capabilities before moving to a paid subscription model. When the AWS Free Tier period expires or if the usage of the application exceeds the Free Tier limits, the company will be billed at the standard pay-as-you-go service rates for the excess usage. AWS Free Tier includes offers that are always free, offers that expire 12 months following sign-up, and short-term free trial offers. Once the validity of these offers expires or usage exceeds the defined limits, standard charges apply."
      },
      {
        "title": "The company will be billed for the services it used during the Free Tier period, along with additional charges for service consumption post the Free Tier period.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The company's AWS account will be put on hold and can be reactivated once a payment scheme is agreed upon.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Support will reach out to the company to establish standard service fees.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to the AWS Well-Architected Framework, which pillar emphasizes minimizing the environmental impact of operating cloud workloads?",
    "type": "radio",
    "options": [
      {
        "title": "Sustainability Pillar",
        "status": "right",
        "explanation": "The Sustainability Pillar emphasizes minimizing the environmental impact of operating cloud workloads. It underscores the importance of implementing strategies and technologies that minimize carbon footprint and other adverse environmental impacts from your cloud operations. This could mean opting for more energy-efficient infrastructure, designing systems to use resources more efficiently, or using services and features that use renewable energy sources. This pillar effectively allows organizations to enhance their environmental responsibility by ensuring their cloud operations are as sustainable as possible."
      },
      {
        "title": "Cost Optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational Excellence",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance Efficiency",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the difference between an AWS IAM user and an AWS IAM role? ",
    "type": "radio",
    "options": [
      {
        "title": "A user is a group of permissions that determines what an AWS service can do, while a role is a set of users that can access AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A user is a person or application that uses AWS services, while a role is a set of permissions that determines what an AWS service can do.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A user is a set of permissions that determines what an AWS service can do, while a role is a person or application that uses AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A user is a permanent identity that can access AWS services, while a role is a temporary identity that can be assumed by a user or AWS service.",
        "status": "right",
        "explanation": "An AWS IAM user is an identity that represents a person or application to interacts with AWS services. Users have their own set of security credentials (access keys and secret access keys) and can be assigned permissions directly to access AWS resources. On the other hand, An IAM role is an identity within your AWS account that has specific permissions. It is similar to an IAM user but is not associated with a specific person. Roles can be temporary and have a set of policies that determine what actions are allowed or denied. IAM role should be used when a service makes a request to AWS service."
      }
    ]
  },
  {
    "title": "Which of the following statements is true for Amazon Route 53?",
    "type": "radio",
    "options": [
      {
        "title": "Can configure DNS settings for health checks and use routing policy to load balancing.",
        "status": "right",
        "explanation": "Amazon Route 53 is a scalable and highly available Domain Name System (DNS) web service. You can configure DNS health checks to route traffic to healthy endpoints or independently monitor the health of your application and its endpoints. Route 53 also supports a variety of DNS routing policies that can help you configure load balancing behavior."
      },
      {
        "title": "continually scans AWS workloads for software vulnerabilities and unintended network exposure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Can provide a direct connection between AWS cloud and on-premises data center.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company plans to run a high-performance computing application that requires 2 hours of significant computing power per day. The application should be able to scale quickly when needed and shut down when the job is done to save costs. Which AWS compute service would be most cost-effective for this use case?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 On-Demand Instances",
        "status": "right",
        "explanation": "Amazon EC2 On-Demand Instances offer the ability to pay for computing capacity by the hour or second (with a minimum of 60 seconds). On-Demand Instances don't require any long-term commitments or upfront payments, making them flexible and cost-effective for applications like high-performance computing that only need to run for a limited time each day. They can scale quickly to handle the computational load and just as easily be terminated when the workload is complete, which helps in managing costs effectively. This option is ideal for those requiring significant computing power for short durations."
      },
      {
        "title": "Amazon EC2 Spot Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 Reserved Instances",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following responsibilities are managed by AWS under the AWS Shared Responsibility Model when using Amazon EC2 instances? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Disposing of obsolete EC2 instance hardware components",
        "status": "right",
        "explanation": "Physical security of the data centers where EC2 instances are hosted is AWS's responsibility. AWS manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. AWS has extensive security measures in place to maintain the integrity and security of the hardware and by extension, the services provided to the customer."
      },
      {
        "title": "Physical security of the data centers where EC2 instances are hosted",
        "status": "right",
        "explanation": "Disposing of obsolete EC2 instance hardware components is under AWS's purview. AWS is responsible for the proper disposal of hardware that supports the EC2 service. They ensure that the hardware is disposed of in compliance with all environmental and security standards, ensuring that there is no risk of customer data being compromised after the physical life of the hardware has ended."
      },
      {
        "title": "Updating the EC2 instance guest operating system",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Implementing a customer master key (CMK) for data encryption",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Configuring the security group rules for EC2 instances",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the key difference between a monolithic architecture and a microservices architecture in AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Microservices architecture allows for greater flexibility and easier maintenance than monolithic architecture.",
        "status": "right",
        "explanation": "Microservices architecture allows for greater flexibility and easier maintenance. Breaking an application into smaller services makes it easier to update or replace individual components, add new features, or change underlying technologies. Additionally, microservices can be developed and deployed independently, reducing the risk of introducing bugs or downtime."
      },
      {
        "title": "Monolithic architecture is more scalable than microservices architecture.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Microservices architecture is more tightly coupled than monolithic architecture.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Monolithic architecture is more fault-tolerant than microservices architecture.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is an effective strategy for maintaining high availability of stateful applications with persistent sessions?",
    "type": "radio",
    "options": [
      {
        "title": "Deploying applications in a single Availability Zone.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Using Elastic Load Balancing with sticky sessions.",
        "status": "right",
        "explanation": "Elastic Load Balancing (ELB) with sticky sessions is a powerful feature for maintaining user session continuity in stateful applications. Sticky sessions enable the load balancer to bind a user's session to a specific application instance. This ensures that all requests from a user during the session are sent to the same instance, maintaining session consistency. This is particularly important for applications where the user's session state is stored locally on the instance, and session data needs to be retained throughout the user's interaction with the application."
      },
      {
        "title": "Storing session data on a local file system of each EC2 instance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Using Amazon S3 for real-time session data.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the benefit of using AWS OpEx instead of capital expenditures (CapEx)?",
    "type": "radio",
    "options": [
      {
        "title": "AWS OpEx always results in lower costs than CapEx",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS OpEx allows customers to pay for only the services they need and use",
        "status": "right",
        "explanation": "Operational expenses (OpEx) refer to the ongoing costs of running and managing an application on AWS, such as the cost of using AWS services like EC2 or S3. By using OpEx, customers can pay for only the services they need and use, which makes it a more cost-efficient way to fund an application. This also provides greater flexibility to adjust spending based on changing needs and demands, which can result in cost savings over time."
      },
      {
        "title": "AWS OpEx is a more tax-efficient way to fund an application",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS OpEx provides long-term investments in hardware and infrastructure",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to AWS Cloud Economics, which accurately describes the cost-saving benefits of migration to AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Guarantee of reduced operational costs due to the inherent nature of cloud services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Transitioning to a fully variable expense model from a capital expense-heavy model.",
        "status": "right",
        "explanation": "AWS allows organizations to move to a variable expense (OpEx) model from a capital expense (CapEx) model. The OpEx is beneficial as it aligns costs directly with business usage and needs. The pay-as-you-go pricing model of AWS enables companies to only pay for the IT resources they consume. This can lead to cost savings, especially for companies with fluctuating workloads, as they can scale resources up or down as needed without incurring costs for idle infrastructure. On the other hand, The CapEx involves significant upfront investment in data centers and physical servers."
      },
      {
        "title": "Elimination of all on-premises infrastructure costs by adopting AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Automatic cost savings by transitioning any type of workload to the AWS Cloud.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to AWS Global Infrastructure, what advantage does AWS offer for those who want to serve worldwide?",
    "type": "radio",
    "options": [
      {
        "title": "Automatic Data Backup to Multiple Regions",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enhanced Security with AWS Managed Services",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Decreased Latency and Content Localization",
        "status": "right",
        "explanation": "AWS Global Infrastructure consists of multiple regions and availability zones spread across the world. This provides a significant advantage to businesses looking to serve their users worldwide, as they can use AWS's infrastructure to significantly reduce latency and provide faster service to end users. Furthermore, by serving localized or tailored content for specific regions or countries, customers are served data relevant to their geographic location, enhancing their user experience. This not only improves service efficiency but also enhances customer satisfaction."
      },
      {
        "title": "Lower Costs due to Reduced Overhead",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the purpose of the AWS Well-Architected Tool?",
    "type": "radio",
    "options": [
      {
        "title": "To provide a comprehensive checklist of AWS best practices",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "To simulate and validate cloud architectures for reliability and cost optimization",
        "status": "right",
        "explanation": "The AWS Well-Architected Tool helps customers review and improve their workloads based on the best practices defined in the AWS Well-Architected Framework. It provides guidance on designing and operating reliable, secure, efficient, and cost-effective systems in the AWS cloud. The primary purpose of the AWS Well-Architected Tool is to simulate and validate cloud architectures to help customers identify potential issues and improve the reliability and cost efficiency of their AWS workloads. This tool provides a set of questions and best practices that customers can use to assess their architectures and identify areas for improvement. It also offers recommendations and resources to help customers optimize their workloads."
      },
      {
        "title": "To automatically generate code for deploying infrastructure in AWS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "To monitor and optimize AWS resources in real-time",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the possible discount available for reserved instances compared to the on-demand pricing?",
    "type": "radio",
    "options": [
      {
        "title": "Up to 54%",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Up to 90%",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Up to 70%",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Up to 72%",
        "status": "right",
        "explanation": "Reserved Instances (RIs) in AWS provide a significant discount compared to on-demand instance pricing, with the potential to save up to 72%. This substantial saving is possible because RIs require a commitment for a certain term (1 or 3 years), which allows AWS to optimize resource allocation and planning. By committing to a Reserved Instance, users agree to use a specific instance type for the duration of the term, in exchange for this lower pricing. The discount varies based on the term length, payment option (All Upfront, Partial Upfront, or No Upfront), and the specific instance type and region. This pricing model is especially beneficial for users with predictable workloads that can commit to using a specific amount of compute capacity over a longer period. By using Reserved Instances, businesses can significantly reduce their cloud computing costs while still benefiting from the flexibility and scalability of AWS services."
      }
    ]
  },
  {
    "title": "Which statement is true about software licensing costs in the cloud?",
    "type": "radio",
    "options": [
      {
        "title": "Costs are always lower than on-premises software licensing costs",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Costs depend on the software and deployment model",
        "status": "right",
        "explanation": "The cost of software licensing in the cloud can vary depending on the software being used and the deployment model. Some cloud providers offer pay-as-you-go models, while others require upfront or subscription fees. Some software may be licensed differently in the cloud than on-premises. It's important to consider these factors when evaluating the impact of software licensing costs when moving to the cloud."
      },
      {
        "title": "Costs are not affected by the deployment model",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Costs are always higher than on-premises software licensing costs",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to the AWS cloud, which design principle reduces system interdependence?",
    "type": "radio",
    "options": [
      {
        "title": "Automation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Removing Single Points of Failure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Loosely Coupled",
        "status": "right",
        "explanation": "The principle of designing systems to be 'loosely coupled' reduces system interdependence in AWS cloud design. In a loosely coupled architecture, components are interlinked in such a way that they interact with each other without being strongly dependent or intertwined. This enables individual components to remain functional and operate independently, even if another part of the system fails or changes. By reducing the dependence of components on each other, we increase the system's resilience, flexibility, and scalability, which is a key tenet of robust cloud architecture design."
      }
    ]
  },
  {
    "title": "Which AWS Well-Architected Framework Pillar emphasizes maintaining efficiency as needs change and technology evolves?",
    "type": "radio",
    "options": [
      {
        "title": "Performance Efficiency",
        "status": "right",
        "explanation": "The Performance Efficiency pillar of the AWS Well-Architected Framework focuses on the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. This pillar encourages the use of cloud services and resources to improve the speed and efficiency of applications and to adapt to changing needs without overspending on fixed infrastructure. By following this pillar, businesses can make sure that they're getting the most out of their AWS resources."
      },
      {
        "title": "Cost Optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational Excellence",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reliability",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has a microservice application in AWS Cloud. Recently, they noticed some performance issues and need to debug them to fix these issues. As a Cloud Practitioner, which AWS service should you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS X-Ray",
        "status": "right",
        "explanation": "AWS X-Ray enables developers to analyze and debug production, distributed applications, such as those built using a microservices architecture. X-Ray helps developers understand how their applications and underlying services are performing by providing insights into the request and response behavior, pinpointing the root cause of issues, and identifying performance bottlenecks. It collects data on requests made to your application, allowing you to view, filter, and gain insights into the operation and performance of your applications and microservices. With AWS X-Ray, developers can trace and aggregate data, including latency distribution and request and response timing, across distributed systems, making it easier to identify and troubleshoot issues, improve performance, and optimize efficiency."
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which feature can generate a report that lists all users and credentials, including passwords and access keys?",
    "type": "radio",
    "options": [
      {
        "title": "IAM Credential Reports",
        "status": "right",
        "explanation": "IAM Credential Report is a document that lists all your AWS account's users and the status of their various credentials, including passwords, access keys, MFA devices, and more. This service is valuable for auditing the security status of your account and identifying any potential vulnerabilities or issues. The report doesn't actually reveal any sensitive information like passwords or access keys but rather provides an overview of their status, making it a safe and secure tool for account management."
      },
      {
        "title": "Cost and Usage Reports",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cost Allocation Reports",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Artifact Reports",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement correctly describes the characteristics and functionality of AWS Regions?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Regions are singular, massive data centers located in strategic locations around the world.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Regions are interconnected networks of data centers providing centralized computing resources globally.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Each AWS Region is a separate geographic area designed to be completely isolated to prevent regional failures.",
        "status": "right",
        "explanation": "AWS Regions consist of multiple isolated and physically separate Availability Zones (AZs) within a geographic area. Each Region is a separate geographic entity and operates independently from the other Regions. This design principle ensures that problems in one Region do not affect others, thereby enhancing the overall resilience and reliability of the AWS infrastructure. Regions are specifically designed to be completely isolated from each other to prevent shared failures, which is critical for disaster recovery and maintaining operational stability."
      },
      {
        "title": "Data stored in a specific AWS Region will be automatically replicated to other Regions for disaster recovery.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service provides cost management and billing support for AWS Marketplace?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Quicksight",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Billing and Cost Management",
        "status": "right",
        "explanation": "AWS Billing and Cost Management is a comprehensive service that enables users to monitor, manage, and optimize their AWS costs and usage. It provides a centralized platform for tracking and analyzing AWS billing information, including detailed cost breakdowns, usage reports, and cost allocation tags. Users can set up budgets, alerts, and cost controls to prevent unexpected spending and optimize resource allocation. The service also offers cost visualization tools, such as AWS Cost Explorer and AWS Budgets, to help users gain insights into their spending patterns and forecast future costs. AWS Billing and Cost Management empowers organizations to effectively manage their AWS expenses and ensure cost efficiency."
      },
      {
        "title": "AWS Marketplace Management Portal",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Organizations",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company manages a multi-layer web application on AWS using EC2, RDS, and S3. They want to ensure that the application is running efficiently and can quickly resolve any issues. What is the best approach to monitor system health and performance?",
    "type": "radio",
    "options": [
      {
        "title": "Use on AWS Service Health Dashboard for daily health check reports.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use AWS CloudTrail logs and regularly inspect for any anomalies.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon SNS to send alerts for any change in the application's environment.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon CloudWatch to monitor the resources and set up alarms for predefined thresholds.",
        "status": "right",
        "explanation": "Amazon CloudWatch is a monitoring and observability service that provides actionable insights to optimize applications, respond to system-wide performance changes, and efficiently resource utilization. By using CloudWatch, users can collect and track metrics, collect and monitor log files, and set alarms. Alarms can be configured for predefined thresholds, ensuring timely notifications when system behavior deviates from the norm. This proactive approach facilitates swift intervention, potentially averting more severe issues, and ensures the application's continued efficient operation. In essence, CloudWatch is an indispensable tool for maintaining the health and performance of AWS resources."
      }
    ]
  },
  {
    "title": "Which AWS service provides a cloud-based virtual desktop that must be persistent and a replacement for traditional desktops?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cloud9",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon AppStream 2.0",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon WorkLink",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon WorkSpaces",
        "status": "right",
        "explanation": "Amazon WorkSpaces provides a cloud-based virtual desktop infrastructure (VDI) solution. It offers a persistent and fully managed desktop experience in the cloud, acting as a replacement for traditional desktops. With WorkSpaces, users can access their desktops from anywhere using a supported device, including laptops, tablets, and thin clients. WorkSpaces allows organizations to provision and manage virtual desktops for their users, providing a consistent and secure computing environment. It supports various operating systems and offers flexibility in terms of hardware configurations and software applications. Amazon WorkSpaces simplifies desktop management, reduces hardware dependencies, and enables remote access and mobility."
      }
    ]
  },
  {
    "title": "Which design principle falls under the Reliability Pillar of AWS Well-Architected?",
    "type": "radio",
    "options": [
      {
        "title": "Automatically recover from failure",
        "status": "right",
        "explanation": "Automatic recovery from failure is an essential principle of the AWS Well-Architected Framework's Reliability Pillar. This approach involves creating systems that are resilient and capable of self-recovery in the event of a failure. By designing for automation in recovery, you reduce the need for manual intervention, which not only increases system reliability but also allows for faster recovery times. This principle often involves the use of technologies such as auto-scaling, health checks, and failover strategies. For example, if a server or service fails, the system is designed to automatically detect this failure and redirect traffic to healthy instances or restart the failed instances, ensuring minimal disruption to the end user. This is crucial for maintaining high availability and resilience in cloud-based environments."
      },
      {
        "title": "Use serverless architectures",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Protect data in transit and at rest",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Refine operations procedures frequently",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A systems engineer tries to make configuration changes to an AWS service but is denied access, even though an IAM policy allows it. What might be causing this issue?",
    "type": "radio",
    "options": [
      {
        "title": "The AWS service is currently undergoing maintenance",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The AWS account has exceeded its service limit for the month",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An explicit \"Deny\" statement exists in another attached IAM policy",
        "status": "right",
        "explanation": "In AWS's IAM, when determining whether an action is allowed, the default is to deny access. If there's an explicit \"Allow\" statement, access is granted, but an explicit \"Deny\" statement will always override any \"Allow\" statement, no matter where it is found. Thus, if a user has multiple policies attached, and even if one of them allows a specific action, the presence of a \"Deny\" for that action in any of those policies will block access. This ensures that sensitive actions or resources can be securely locked down by administrators."
      },
      {
        "title": "The AWS region for the service does not support the action",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A newspaper company wants to develop a news app that will convert news into voice so that blind people can listen to their news. As a Cloud Practitioner, which service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Elemental MediaConvert",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Transcribe",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Polly",
        "status": "right",
        "explanation": "Amazon Polly turns text into lifelike speech that allows you to create applications that talk, and build entirely new categories of speech-enabled products. Amazon Polly is perfect for the proposed use case as it supports multiple languages and offers a variety of voices to choose from. It uses advanced deep learning technologies to synthesize speech that sounds like a human voice. Hence, using Amazon Polly, the newspaper company can effectively convert news articles into audible speech, which would be highly beneficial for visually impaired individuals."
      },
      {
        "title": "Amazon WAF",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are the advantages of using AWS Cloud? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Automatic compliance with all international data sovereignty laws.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Pay-as-you-go pricing to reduce costs.",
        "status": "right",
        "explanation": "One of the key benefits of AWS Cloud is the ability to quickly deploy applications across multiple regions around the world. This is facilitated by AWS's extensive global infrastructure, which includes regions and availability zones designed to host applications with lower latency and higher fault tolerance. This allows organizations to expand their reach and operate closer to their end-users without significant investments in physical infrastructure and long lead times."
      },
      {
        "title": "Ability to deploy globally in minutes.",
        "status": "right",
        "explanation": "AWS's pay-as-you-go pricing model offers a cost-effective solution for businesses by allowing them to pay only for the resources they use, with no upfront costs or long-term commitments. This model provides flexibility and cost management that is particularly beneficial for startups and enterprises looking to optimize their budgets. It aligns costs directly with usage, preventing overprovisioning and enabling scalability."
      },
      {
        "title": "Decreased speed of innovation due to cloud complexity.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Higher upfront capital expense to scale operations.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are the advantages of AWS managed services? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Enhanced Security aligned with your controls",
        "status": "right",
        "explanation": "AWS Managed Services (AMS) builds and maintains a growing repository of compliance, operational, and security guardrails that help keep you aligned with your controls. AMS reduces the burden of meeting compliance program requirements (HIPAA, HITRUST, GDPR, SOC, NIST, ISO, PCI, FedRAMP) through automated detection and remediation automation."
      },
      {
        "title": "Increased high-level control of infrastructure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reduced operational costs for maintaining",
        "status": "right",
        "explanation": "AWS Managed Services (AMS) helps with financial optimization across your AWS estate, and any savings identified reduce your AMS fee without impacting operational outcomes. The customers have enjoyed up to 30% in operational savings and up to 25% in AWS infrastructure savings. Pay for what you use and take back operational control when you are ready."
      },
      {
        "title": "Automatically encrypted data at rest",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Provided free enterprise level supports",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "As an AWS customer, which of the following tasks fall under your responsibility in the Shared Responsibility Model? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Setting up IAM user accounts and permissions for your team members.",
        "status": "right",
        "explanation": "The AWS customer is responsible for managing the data (including encryption) before it is sent to AWS. This includes applying client-side encryption to sensitive data before uploading to Amazon S3, ensuring data confidentiality and integrity. AWS provides the infrastructure and tools, such as the S3 server-side encryption feature, but the customer must implement these security measures as part of their responsibility for data protection."
      },
      {
        "title": "Implementing client-side data encryption before uploading to Amazon S3.",
        "status": "right",
        "explanation": "Customers are responsible for identity and access management within their AWS environment. This includes creating and managing IAM users, groups, roles, and permissions to control access to AWS resources. IAM allows customers to securely manage services and resources in AWS, and setting it up correctly is crucial for ensuring that only authorized personnel have the appropriate levels of access."
      },
      {
        "title": "Configuring the physical security of the data center where your data is stored.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Upgrading AWS hardware to meet your application's compute requirements.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Patching and updating the network infrastructure of AWS.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following AWS services can quickly deploy a Node.js application to the AWS Cloud? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Amazon ECS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Elastic Beanstalk",
        "status": "right",
        "explanation": "Amazon Lightsail provides a simplified way to launch and manage virtual private servers (VPS) with pre-configured compute, storage, and networking resources. The service supports a variety of applications and platforms, including popular programming languages like Node.js, Python, Java, and more. Lightsail offers a straightforward interface and pre-configured application stacks, including Node.js, making it easy to deploy a Node.js application without the need for extensive infrastructure management or configuration. With Lightsail, developers can quickly get their Node.js application up and running in the AWS Cloud with just a few clicks."
      },
      {
        "title": "Amazon EC2",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Lightsail",
        "status": "right",
        "explanation": "AWS Elastic Beanstalk offers quick deployment of Node.js applications to the AWS Cloud. It is a fully managed service that abstracts away the underlying infrastructure complexities. It allows developers to easily deploy web applications in various programming languages, such as Java, .NET, Python, Node.js, Ruby, and more. Elastic Beanstalk automatically provisions and manages the necessary resources to run applications, including EC2 instances, load balancers, and auto-scaling groups. It simplifies the deployment process by providing a platform where developers can easily upload their application code and let Elastic Beanstalk handle the rest, including environment setup, scaling, and load balancing."
      }
    ]
  },
  {
    "title": "Which of the following is an example of a serverless architecture in AWS?",
    "type": "radio",
    "options": [
      {
        "title": "A load-balanced group of instances behind an ELB",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An EC2 instance running a web server",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A database instance running in RDS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A Lambda function triggered by an API Gateway",
        "status": "right",
        "explanation": "Serverless architecture in AWS refers to the computing model where the cloud provider, AWS in this case, manages the infrastructure required to run applications. This model allows developers to build and run applications and services without needing to provision, scale, and manage servers. AWS offers services like AWS Lambda, which executes code in response to events without requiring the user to manage the underlying compute resources. This approach enables developers to focus on writing code and adding value, rather than on the operational tasks of server management. It supports scaling automatically, from a few requests per day to thousands per second, and users only pay for the compute time they consume, making it cost-effective and efficient. AWS Lambda handles the underlying infrastructure and provides a platform for running code without provisioning or managing servers. API Gateway acts as the trigger for Lambda, allowing the function to be invoked in response to an HTTP request."
      }
    ]
  },
  {
    "title": "Which AWS Feature should be used to launch Amazon EC2 instances with pre-configured settings?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Machine Image (AMI)",
        "status": "right",
        "explanation": "An AMI is a pre-configured template that contains the necessary information to launch an instance, such as the operating system, software applications, libraries, and configurations. It serves as the foundation for creating new EC2 instances with the desired settings and configurations. AMIs provide a convenient way to replicate and share instances across different regions and accounts, allowing for consistent and efficient deployment of pre-configured environments. To launch Amazon EC2 instances with pre-configured settings, the company should use Amazon Machine Image (AMI)."
      },
      {
        "title": "WS Identity and Access Management (IAM)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security Groups",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon VPC",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service should be used to establish a dedicated network connection between AWS and on-premises data server?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Route 53",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon API Gateway",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudFront",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Direct Connect",
        "status": "right",
        "explanation": "AWS Direct Connect provides a dedicated network connection from an on-premises environment to Amazon Web Services (AWS). This connection bypasses the public internet, offering more consistent network performance, lower latency, and increased security compared to typical internet-based connections. By establishing a private link to AWS, Direct Connect enables businesses to transfer data to and from AWS services more reliably and with potentially reduced network costs. It supports various bandwidth options, allowing organizations to choose the most appropriate speed for their needs, ranging from 50 Mbps to 100 Gbps. This service is particularly beneficial for applications that require high throughput, secure data transfer, or real-time access to AWS resources."
      }
    ]
  },
  {
    "title": "Which AWS service provides an additional protection layer against distributed denial-of-service (DDoS) attacks?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Audit Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Shield",
        "status": "right",
        "explanation": "AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides automatic DDoS detection and mitigation employing always-on detection and automatic inline mitigations that minimize application downtime and latency, hence ensuring the availability of your application against DDoS attacks."
      }
    ]
  },
  {
    "title": "A company wants to migrate its business to the AWS cloud from an on-premise data center. They are seeking help for guidance on deploying popular technologies following AWS best practices. Which AWS service will meet the requirements?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Marketplace",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Support Center",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Partner Solutions",
        "status": "right",
        "explanation": "AWS Partner Solutions offers a catalog of solutions from third-party vendors that are validated by AWS for their adherence to best practices and ability to solve a variety of business challenges. These solutions can help businesses accelerate their migration to the AWS cloud, leveraging expertly crafted architectures for popular technologies. By using AWS Partner Solutions, the company can benefit from pre-built and well-architected solutions that streamline deployment, reduce risk, and enhance performance, ensuring they follow AWS best practices. This option provides a comprehensive way to leverage third-party expertise and validated designs to facilitate a successful migration to the AWS cloud."
      }
    ]
  },
  {
    "title": "Suppose you have an EC2 instance with 1TB of data. Now you want to move this data to an S3 bucket in the same region. How much will AWS charge you?",
    "type": "radio",
    "options": [
      {
        "title": "The outbound charge will be applicable for data transfer.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "There will not be charged to you for this data transfer.",
        "status": "right",
        "explanation": "AWS does not charge for data transfer between an EC2 instance and an S3 bucket within the same region. The data transfer in this case, where data is moved from the EC2 instance to the S3 bucket, is considered \"inbound\" for S3, which is free of charge."
      },
      {
        "title": "The inbound and outbound data transfer charge will be applicable for S3 bucket.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The inbound charge will be applicable for data transfer.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service can quickly detect and analyze faces in millions of images and videos in minutes?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Kendra",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Polly",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Transcribe",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Rekognition",
        "status": "right",
        "explanation": "Amazon Rekognition allows you to perform face detection and analysis from millions of images and videos in just minutes. It provides powerful computer vision capabilities for analyzing visual content. With Rekognition, you can detect and analyze faces, identify facial attributes, such as emotions and age range, perform face comparison and recognition, and even track faces in videos. Rekognition uses deep learning algorithms to deliver accurate and fast results. It is highly scalable, allowing you to process vast amounts of visual data quickly and efficiently. Amazon Rekognition is widely used in various applications, including security systems, content moderation, personalized user experiences, and social media analytics."
      }
    ]
  },
  {
    "title": "A newly joined CTO in a company wants to understand AWS' services compliance with the Business Associate Addendum (BAA) agreements. Which AWS service should be used to review BAA and HIPAA-related documents in AWS?",
    "type": "radio",
    "options": [
      {
        "title": "AWS License Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Certificate Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Artifact",
        "status": "right",
        "explanation": "AWS Artifact is a comprehensive, self-service portal that provides users with on-demand access to AWS' security and compliance documentation and agreements. It enables AWS customers to easily download AWS cloud compliance documents, such as audit artifacts, service organization control (SOC) reports, Business Associate Addendum (BAA) agreements and Payment Card Industry (PCI) compliance reports, among others. This service is particularly useful for organizations that need to meet regulatory and compliance requirements for their cloud-based workloads. Through AWS Artifact, users can also directly accept and manage agreements with AWS for the services they use, facilitating a smoother compliance and security assurance process."
      }
    ]
  },
  {
    "title": "A company wants to move 8 terabytes of data from an on-premises data center to the AWS cloud. Which of the following should be used to do this in a cost-effective way?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Snowmobile",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Snowball",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Snowcone",
        "status": "right",
        "explanation": "AWS Snowcone is a portable, rugged, and secure device for edge computing and data transfer. It is the smallest member of the AWS Snow Family of devices, and it's capable of storing up to 8 terabytes of data. The company can use Snowcone to move their data to AWS, which is practical and cost-effective for the amount of data they wants to transfer."
      },
      {
        "title": "AWS Storage Gateway",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is planning to host a new application on Amazon EC2 instances. The application will run continuously and its usage is predicted to increase over the next several years. Which EC2 instance purchasing model is the MOST cost-effective for this scenario?",
    "type": "radio",
    "options": [
      {
        "title": "On-Demand Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Spot Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Savings Plans",
        "status": "right",
        "explanation": "AWS Savings Plans offer significant cost savings for customers with predictable usage patterns. They provide flexible pricing models for EC2 instances and AWS Fargate usage, allowing customers to commit to a consistent amount of usage (measured in dollars per hour) over a 1- or 3-year period. Savings Plans automatically apply discounted rates to usage within the specified commitment, adjusting instance usage across AWS regions and instance families to maximize savings. This simplifies cost management and provides substantial discounts compared to on-demand pricing. By using Savings Plans, the company can efficiently plan and manage their budget and reduce the overall operational cost for the foreseeable future, making it the most cost-effective choice for this scenario."
      },
      {
        "title": "Dedicated Hosts",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A multinational software company has multiple development teams for different software projects. Regularly they create and launch Amazon EC2 instances to run their operations. To maintain the financial budget, they needed a solution to monitor and control EC2 spending. Which action will meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Use Amazon EC2 Spot Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon Polly for getting notifications",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use AWS Cost Explorer with tagged resources",
        "status": "right",
        "explanation": "AWS Cost Explorer is a visualization tool that helps users analyze, understand, and optimize their AWS spending. It offers detailed insights into past and forecasted costs, allowing for effective budgeting. Users can view data at a granular level, filter by various parameters, and identify cost trends or anomalies, ensuring efficient financial management of their AWS resources. Tagging resources allow users to assign metadata to their AWS resources in key-value pairs. These tags help in organizing, tracking, and managing resources by categorizing them based on purpose, owner, environment, or other criteria. They're especially useful for cost allocation, automation, and governance across complex AWS environments. By tagging EC2 instances with metadata (like project name or team name), the company can break down their costs by different projects or teams. This gives them a granular view of which team or project is incurring what expenses. Moreover, with Cost Explorer's ability to forecast future costs based on historical data, the company can make informed decisions about budgeting and spending. It's a powerful tool for organizations that want to keep a tight leash on their AWS expenditure and understand their cost drivers."
      },
      {
        "title": "Use AWS Lambda for serverless computation",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is a key cost-efficiency advantage of using AWS over traditional data centers?",
    "type": "radio",
    "options": [
      {
        "title": "AWS enables users to scale EC2 instances up or down according to demand.",
        "status": "right",
        "explanation": "AWS (Amazon Web Services) provides a robust infrastructure that allows users to scale their EC2 (Elastic Compute Cloud) instances up or down depending on the current demand. This dynamic scaling capability ensures that businesses only pay for the resources they actually use, which leads to significant cost savings. In contrast, traditional data centers often require purchasing hardware and allocating resources that may be underutilized during off-peak times. By enabling automated scaling based on real-time demand, AWS helps businesses avoid overprovisioning and underutilization, making it a cost-efficient solution compared to traditional data center models."
      },
      {
        "title": "AWS requires yearly contracts for EC2 instances.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Traditional data centers offer more competitive pricing for server resources.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Traditional data centers have fixed billing cycles regardless of usage.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is transferring sensitive data between its local servers and AWS. They want data to remain encrypted during transfer. Which AWS service can help achieve this?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Key Management Service (KMS)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Snowmobile",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS DataSync",
        "status": "right",
        "explanation": "AWS DataSync is a data transfer service that makes it simple and fast to move large amounts of data online between on-premises storage and Amazon S3, Amazon Elastic File System (EFS), or Amazon FSx for Windows File Server. AWS DataSync automatically encrypts data in transit using TLS (Transport Layer Security). This ensures the data remains confidential and tamper-proof while being moved over the internet."
      },
      {
        "title": "AWS Shield",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has a MySQL database running on a single Amazon EC2 instance. Now The database requires higher availability. As a Cloud Practitioner, which option should you suggest?",
    "type": "radio",
    "options": [
      {
        "title": "Upgrade EC2 instance size (Increase CPU & RAM).",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Add an Application Load Balancer to the EC2 instance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enable termination protection to avoid outages.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Migrate to Amazon RDS with Multi-AZ DB instance deployments.",
        "status": "right",
        "explanation": "To achieve higher availability for the MySQL database, it is recommended to migrate to Amazon RDS (Relational Database Service) with Multi-AZ (Availability Zone) DB instance deployments. Amazon RDS is a managed database service that simplifies database administration tasks. Multi-AZ deployment provides automatic synchronous replication of the database to a standby replica in a different Availability Zone. In the event of a failure, Amazon RDS automatically promotes the standby replica to the primary database, minimizing downtime and ensuring high availability. By migrating to Amazon RDS with Multi-AZ, the company can benefit from automated failover, data durability, and reduced operational overhead for managing the database."
      }
    ]
  },
  {
    "title": "Which of the following factors does AWS use to calculate costs? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Data transfer IN of AWS clouds",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Number of users who used AWS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Data transfer OUT of AWS clouds",
        "status": "right",
        "explanation": "AWS charges for the data transfer OUT of its services to the internet or between regions and Availability Zones (AZs). The charges can vary depending on the region and the total volume of data transferred."
      },
      {
        "title": "Compute & storage usage",
        "status": "right",
        "explanation": "AWS charges are based on the resources used, including compute instances (such as EC2 and Lambda) and storage services (like S3 and EBS). The cost of these resources depends on their type, size, and duration of usage."
      },
      {
        "title": "Number of the services used",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company needs to perform periodic data processing but wants to optimize costs. The processing jobs do not need to run at specific times. Which Amazon EC2 instance purchasing option will meet these requirements at the lowest cost?",
    "type": "radio",
    "options": [
      {
        "title": "On-Demand Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reserved Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Dedicated Hosts",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Spot Instances",
        "status": "right",
        "explanation": "Spot Instances are a cost-effective purchasing option provided by AWS where customers can take advantage of unused EC2 capacity in the AWS cloud. Because these instances use spare capacity, they are significantly cheaper compared to On-Demand Instancesâ€”often up to 90% less. This makes Spot Instances an ideal choice for fault-tolerant and flexible workloads such as periodic data processing jobs. Since the jobs don't need to run at specific times, utilizing Spot Instances allows the company to optimize costs without compromising operational requirements. The only drawback is that Spot Instances can be terminated by AWS when the capacity is needed, but for applications like non-time-sensitive data processing, this is usually a manageable risk."
      }
    ]
  },
  {
    "title": "A financial company wants to store decades of transaction records on AWS. These records are rarely accessed but when needed, retrieval time must be fast. Which S3 storage class would be most cost-effective for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "S3 Glacier",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "S3 Standard",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "S3 Standard-Infrequent Access",
        "status": "right",
        "explanation": "S3 Standard-IA is designed for data that is accessed less frequently, but when it's required, rapid access is necessary. This makes it suitable for the given scenario where transaction records are rarely accessed but need quick retrieval times when they are. S3 Standard-IA offers the high durability, throughput, and low latency of S3 Standard, with a lower price per GB stored. It's a cost-effective choice for the long-term storage of infrequently accessed data that needs to be retrieved quickly when it is accessed."
      },
      {
        "title": "S3 One Zone-Infrequent Access",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An e-commerce company wants to show product recommendations to users based on their history. Which AWS service will meet the requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Polly",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SageMaker",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Rekognition",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Personalize",
        "status": "right",
        "explanation": "Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications. Personalize allows developers to deliver personalized experiences to users by recommending products, content, and promotions tailored to each user's interactions, preferences, and behavior. This service automates the complex process of building, training, and deploying machine learning models, enabling developers to implement custom recommendation engines with ease, without the need for prior machine learning expertise. Amazon Personalize scales automatically to process and analyze data, generate accurate recommendations in real-time or batch mode, and improve engagement and conversion rates by offering a more personalized user experience."
      }
    ]
  },
  {
    "title": "Which benefit do customers get from TCO when using AWS services?",
    "type": "radio",
    "options": [
      {
        "title": "Cloud services can increase the risk of data breaches.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cloud services can only be used for some specific applications.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cloud services can provide more predictable and stable costs over time.",
        "status": "right",
        "explanation": "With AWS, customers can pay only for the resources they use and scale up or down based on their needs. AWS pricing is transparent and predictable, with no upfront costs or long-term commitments. This allows customers to plan and budget more accurately, resulting in more predictable and stable costs over time."
      },
      {
        "title": "Cloud services may have less control over security and compliance measures.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS services provide best practice recommendations for security and performance, fault tolerance, cost reduction, and service quota monitoring?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "right",
        "explanation": "AWS Trusted Advisor provides recommendations to help customers adhere to best practices by enhancing your AWS environment. It provides real-time guidance to help customers provision resources following AWS best practices for optimal security, high performance, service fault tolerance, and cost efficiency. Additionally, it monitors your service quotas to ensure customers don't hit limits. Therefore, AWS Trusted Advisor is a go-to service for a holistic view of your AWS services, adhering to best practices, and optimizing the use of resources."
      },
      {
        "title": "AWS IAM",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following enables encryption to enhance security for data at rest?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon S3 with server-side encryption (SSE)",
        "status": "right",
        "explanation": "Amazon Simple Storage Service (Amazon S3) offers server-side encryption that allows users to store data securely by encrypting it on the server side when writing it to disks in data centers and decrypting it for the user when they access it. S3 provides multiple encryption options including S3-managed keys (SSE-S3), AWS Key Management Service keys (SSE-KMS), and customer-provided keys (SSE-C), which helps in complying with various compliance requirements that mandate encryption of data at rest."
      },
      {
        "title": "Amazon CloudFront for encrypted content delivery",
        "status": "",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 for compute capacity with built-in encryption",
        "status": "",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic Block Store (EBS) with encryption for block storage volumes",
        "status": "",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has a growing volume of data stored in Amazon S3 and it becomes incredibly complex to identify and protect their sensitive data. As a cloud practitioner, what services would you recommend for identifying these sensitive data?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS KMS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Macie",
        "status": "right",
        "explanation": "Amazon Macie is an ideal service for managing and protecting data at scale. This service uses machine learning and pattern matching to discover and protect sensitive data like Personally Identifiable Information (PII). It helps organizations understand where sensitive information is located, how it's being accessed, and by whom. Its automated discovery of sensitive data helps mitigate the risks associated with the distribution of such data."
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the Principle of Performance Efficiency Pillar in the AWS Well-Architected Framework?",
    "type": "radio",
    "options": [
      {
        "title": "Implement strong consistency in data storage",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use serverless architectures to reduce system load",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use advanced virtualization for resource optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Stop guessing capacity",
        "status": "right",
        "explanation": "\"Stop guessing capacity\" is a key principle of the Performance Efficiency pillar, which recommends using auto-scaling features to add or remove resources to match demand without requiring upfront capacity planning. This principle is based on the idea that in cloud computing, you can measure and monitor a system's performance and add or remove resources as needed. This dynamic management of resources prevents under-provisioning (which can cause poor customer experiences) and over-provisioning (which can lead to unnecessary costs)."
      }
    ]
  },
  {
    "title": "What is the purpose of AWS KMS?",
    "type": "radio",
    "options": [
      {
        "title": "To manage cryptographic keys for data encryption",
        "status": "right",
        "explanation": "AWS Key Management Service (KMS) is designed to create and control cryptographic keys for encrypting and decrypting data across AWS services. It supports centralized control over the cryptographic keys and provides an auditable solution to satisfy compliance requirements. KMS is integrated with other AWS services to help protect data you store in these services and control access to this data by using encryption. Therefore, the correct purpose of AWS KMS is to encrypt and decrypt data stored in AWS services."
      },
      {
        "title": "To manage compliance with regulatory standards",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "To control access to AWS services and resources",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "To manage user authentication and authorization",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following is an example of shared responsibility in the AWS Shared Responsibility Model?",
    "type": "radio",
    "options": [
      {
        "title": "AWS is responsible for monitoring the performance of AWS-managed databases, and the customer is responsible for patching engines.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS is responsible for encrypting customer data at rest, and the customer is responsible for encrypting data in transit.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS is responsible for ensuring the availability of EC2 instances, and the customer is responsible for patching the operating system of EC2 instances.",
        "status": "right",
        "explanation": "The AWS Shared Responsibility Model is a cybersecurity framework where security responsibilities are shared between AWS and the customer. AWS manages the security of the cloud, including the infrastructure and services like EC2, while the customer is responsible for security in the cloud, including their data and the guest operating system. Therefore, this option perfectly exemplifies the shared responsibility model that AWS ensures EC2 instances are available, and customers are responsible for their maintenance, like patching the OS."
      },
      {
        "title": "AWS is responsible for ensuring the physical security of AWS data centers, and the customer is responsible for configuring physical network security to their AWS resources.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A streaming platform uses AWS Cloud to store and distribute its video content. All static video assets are stored in S3 across multiple regions. To simplify access while maintaining low latency, which AWS feature should they implement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon S3 Cross-Region Replication",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Global Accelerator",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 Multi-Region Access Points",
        "status": "right",
        "explanation": "Amazon S3 Multi-Region Access Points make it simpler to build applications that require global access to data by streamlining the endpoint naming convention and by automatically routing requests to data in the most optimal AWS region. Instead of managing requests across various S3 buckets and regions, S3 Multi-Region Access Points provide a singular access point to view and access data globally, reducing the application's complexity. This is particularly beneficial for the streaming company, as users from different regions can efficiently access videos with low latency without the need for the application to handle multiple S3 regional endpoints."
      },
      {
        "title": "Amazon CloudFront with S3 Origin",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service is best for managing security and compliance across multiple accounts and workloads in the AWS environment?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Identity and Access Management",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Control Tower",
        "status": "right",
        "explanation": "AWS Control Tower is the service that assists you in setting up and governing a secure, multi-account AWS environment. It provides the easiest way to set up and manage a new, secure, multi-account AWS environment based on best practices established through AWSâ€™ experience working with thousands of enterprises as they move to the cloud. It automates the process of setting up a new baseline multi-account AWS environment that is secure, well-architected, and ready to use."
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following is the customerâ€™s responsibility as part of the AWS shared responsibility model? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Make decisions around encrypting data at rest and in transit.",
        "status": "right",
        "explanation": "Customers are responsible for managing and configuring the security settings of their guest operating systems and applications. This includes applying patches, managing firewall settings, and ensuring that their applications are secure."
      },
      {
        "title": "Configure and manage the security settings of guest operating systems and applications.",
        "status": "right",
        "explanation": "Customers must decide how to encrypt their data at rest and in transit to protect sensitive information. AWS provides various tools and services to help with encryption, but choosing the appropriate encryption methods and implementing them correctly falls to the customer. This ensures that data remains secure not only when it is stored but also during its transfer between services and regions."
      },
      {
        "title": "Apply updates and patches within the underlying physical infrastructure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Configure the security settings of AWS-managed hardware.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Oversee the maintenance of AWS physical data centers.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company intends to deploy a multi-tier web application within a single VPC on AWS. The application includes a web server tier and a database tier. Both tiers need to be isolated for security reasons, yet they must communicate with one another. What is the optimal design for this architecture?",
    "type": "radio",
    "options": [
      {
        "title": "Deploy both tiers in separate VPCs and establish a peering connection.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Place both tiers in the same subnet and use network ACLs to control communication.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use separate subnets for each tier and configure security groups to control access between them.",
        "status": "right",
        "explanation": "Using separate subnets for each tier is optimal for isolating and securing the different layers of the application. By segregating the web server tier and the database tier into different subnets, you can minimize potential attack surfaces and follow best practices for VPC design. Security groups provide stateful filtering of traffic, meaning they are aware of the established connections and allow for flexible management of rules enabling communication between the web servers and database. This design aligns with AWS security best practices by implementing a multi-layered security approach and provides fine-grained control over inbound and outbound traffic."
      },
      {
        "title": "Place both tiers in the same subnet and use private IP addresses for communication.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A media company needs to use Amazon S3 for storing data but must ensure that the data never leaves a specific geographical region due to compliance rules. Which feature in Amazon S3 can guarantee this requirement is met?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon S3 Cross-Region Replication",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 Regional Restrictions",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 Bucket Policies",
        "status": "right",
        "explanation": "Amazon S3 Bucket Policies allow you to create policies that grant or restrict permissions for your S3 buckets and their objects. These policies can include rules that deny access based on specific conditions, such as geographic location or IP address. By using bucket policies, you can ensure that access to your S3 data is restricted to users, applications, or AWS services operating from within a specified region. This aligns with compliance rules requiring that data remains within a geographical region, providing fine-grained control over data storage and access."
      },
      {
        "title": "Amazon S3 Transfer Acceleration",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement is true regarding the AWS Command Line Interface (AWS CLI)?",
    "type": "radio",
    "options": [
      {
        "title": "Access key ID and secret access key are both required to access AWS CLI.",
        "status": "right",
        "explanation": "To access AWS services using the AWS Command Line Interface (AWS CLI), you need both an access key ID and a secret access key. These are provided when you create an IAM user. They are part of the security credentials that allow AWS CLI to authenticate your requests to AWS services and are absolutely necessary for accessing the AWS CLI."
      },
      {
        "title": "To access AWS CLI you must be provided a username and password.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "You can access CLI with only a secret access key.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "You can access CLI with AWS Personal Token Key.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS feature allows you to categorize and track your AWS costs at a detailed level?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Service Catalog",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "CloudWatch Logs",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cost Allocation Tags",
        "status": "right",
        "explanation": "Cost Allocation Tags help you organize your AWS resources and can assist with managing costs. It is a key-value pair that you can attach to your AWS resources. Once activated, AWS generates a cost allocation report with usage and costs aggregated by your tags, helping you to track and categorize your AWS costs in a detailed manner. By tagging resources with identifiers such as project name, department, or environment (e.g., development, testing, production), users can organize their AWS usage and costs across different dimensions. This facilitates detailed cost analysis, budgeting, and reporting, helping organizations to understand where and how they are incurring costs and to optimize their AWS spending. AWS provides two types of cost allocation tags: AWS-generated \"System Tags\" and user-defined \"User Tags,\" both of which can be used for detailed cost tracking and reporting purposes."
      }
    ]
  },
  {
    "title": "A company has moved its applications to AWS. The company aims to efficiently and securely adopt AWS on a larger scale. Which AWS service or framework should the company consider for operational support?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Marketplace",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Managed Services (AMS)",
        "status": "right",
        "explanation": "AWS Managed Services (AMS) is designed to help companies operate their AWS infrastructure more effectively and securely. AMS provides infrastructure operational support such as monitoring, incident management, security, and compliance. It follows best practices to ensure the efficient operation of cloud infrastructure, allowing the company to focus on innovation and scalability. AMS offers a consistent operational model while helping maintain security and compliance requirements, making it an ideal choice for companies looking to expand their AWS adoption securely and efficiently."
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Organizations",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What should you do if you discover that your EC2 instance is being used for suspicious activity, such as a DoS attack?",
    "type": "radio",
    "options": [
      {
        "title": "Restart/reboot your EC2 instance",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Contact Customer Service for Penetration Testing",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Contact the AWS Abuse team",
        "status": "right",
        "explanation": "If you discover that your EC2 instance is being used for suspicious activity such as a Denial of Service (DoS) attack, the appropriate action is to contact the AWS Abuse team. The AWS Abuse team is specialized in handling and investigating potential security incidents and abuse reports. By contacting them, you can provide details about the incident, and they can assist in investigating and mitigating the issue. The team can also provide guidance on how to prevent such incidents in the future. This approach ensures that AWS is aware of the incident and can take necessary actions to protect the infrastructure and maintain the integrity of their services. It is crucial to act promptly in such situations to minimize potential damage and to ensure compliance with AWS policies. The AWS Trust & Safety team offers assistance for dealing with abusive behaviors involving AWS resources, including: Spam: If you're getting unwanted emails from an AWS IP address, or AWS resources are being used to spam websites or forums. Port Scanning: When your logs indicate AWS IP addresses scanning multiple ports on your server, potentially to find unsecured ports. Denial-of-Service (DoS) Attacks: Instances where AWS IP addresses are flooding ports on your systems with excessive packets, possibly to overload or crash your server or its software. Intrusion Attempts: Situations where AWS IP addresses are trying to log into your systems, as shown in your logs. Hosting Prohibited Content: If you discover that AWS resources are hosting or distributing prohibited content, such as illegal material or copyrighted content without the holder's permission. Distributing Malware: If there's evidence that AWS resources are being used to distribute harmful software designed to compromise or damage computers or other machines."
      },
      {
        "title": "Update the Operating system of your EC2",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service can be used to establish a secure connection between multiple on-premises locations and AWS resources where data is encrypted in transit?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Transit Gateway",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS VPN",
        "status": "right",
        "explanation": "AWS VPN (Virtual Private Network) allows secure communication between remote networks and AWS resources. It establishes encrypted connections via the internet, ensuring data privacy and integrity. By leveraging industry-standard protocols like IPSec, AWS VPN enables seamless integration with existing on-premises infrastructure and extends secure access to AWS cloud services. This scalable solution offers flexible configurations, supporting various use cases such as remote access, site-to-site connectivity, and hybrid cloud deployments, empowering organizations to build robust and resilient network architectures. AWS offers two types of VPN: AWS Site-to-Site VPN and AWS Client VPN. The Site-to-Site VPN can securely connect entire networks to AWS, The Client VPN is designed for connecting individual clients. This makes AWS VPN an ideal choice that needs to securely connect multiple on-premises locations with the AWS environment."
      },
      {
        "title": "AWS CloudHSM",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Direct Connect",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "The AWS Well-Architected Framework describes key concepts, design principles, and architectural best practices for designing and running workloads in the cloud. Which of the following are pillars of the AWS Well-Architected Framework? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Sustainability",
        "status": "right",
        "explanation": "The sustainability pillar focuses on minimizing the environmental impacts of running cloud workloads. Key topics include a shared responsibility model for sustainability, understanding impact, and maximizing utilization to minimize required resources and reduce downstream impacts."
      },
      {
        "title": "Availability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Elasticity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security",
        "status": "right",
        "explanation": "Security is one of the key pillars of the AWS Well-Architected Framework. It involves understanding and applying best practices around the protection of information and systems. The security pillar provides strategies to help you protect your data, systems, and assets in the cloud."
      },
      {
        "title": "Scalability",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Where can you quickly deploy a popular IT solution and start using it right away?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Quick Start reference deployments",
        "status": "right",
        "explanation": "AWS Quick Start reference deployments are a set of automated, AWS CloudFormation templates that enable you to rapidly deploy and configure solutions on AWS, following AWS best practices. These templates allow organizations to quickly launch complex workloads within the AWS environment without needing to manually configure each component. Quick Starts are designed by AWS in collaboration with partners and cover a wide array of use cases, from setting up a Microsoft Active Directory to deploying a Kubernetes cluster. They help save time, ensure consistency, and reduce the operational overhead associated with manual deployments, thus making them the best choice for quickly deploying popular IT solutions."
      },
      {
        "title": "AWS Management Console",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Well-Architected Tool",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company plans to meet regulatory standards on AWS. Which AWS service should you recommend to get detailed insight into AWS's compliance controls?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Compliance Center",
        "status": "right",
        "explanation": "AWS Compliance Center provides a central location to research cloud-related regulatory requirements and how they impact your environment. It offers a collection of compliance-related information and tools to help you manage in the AWS Cloud. With AWS Compliance Center, customers can navigate complex compliance requirements with ease and ensure they meet specific regulatory standards."
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Systems Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Personal Health Dashboard",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following statements accurately describes the benefits of the AWS Cloud?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cloud guarantees permanent ownership of the physical infrastructure for users.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cloud services are available in a limited number of regions to maintain service exclusivity.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS cloud enables rapid deployment of resources that reduce time to market.",
        "status": "right",
        "explanation": "The AWS Cloud allows users to quickly deploy resources as needed, which means businesses can develop and deploy applications faster than if they had to procure, install, and configure hardware themselves. This agility can significantly improve time to market for new initiatives."
      },
      {
        "title": "AWS Cloud provides automated backup services across all services by default.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service allows you to publish and share software packages used in the software development process?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CodePipeline",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeBuild",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeCommit",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeArtifact",
        "status": "right",
        "explanation": "AWS CodeArtifact is a fully managed software artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in the software development process. AWS CodeArtifact works with commonly used package managers and builds tools like Maven, Gradle, npm, yarn, twine, pip, and NuGet, which means that developers can continue using the same tools they are accustomed to. It eliminates the need for the company to operate its own artifact infrastructure, thereby reducing operational overhead."
      }
    ]
  },
  {
    "title": "Which of the following are the advantages of using cloud computing over traditional on-premises? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Reduce lower latency of applications",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Trade capital expense for variable expense",
        "status": "right",
        "explanation": "Traditional on-premises infrastructures involve heavy upfront capital expenditure (CapEx) for purchasing hardware, setting up infrastructure, and maintaining physical servers. However, with cloud computing, these CapEx costs are traded for variable expenses. You only pay for the IT resources you consume, turning substantial upfront expenditure into manageable operational expenditure (OpEx). This brings in more financial flexibility and scalability."
      },
      {
        "title": "Stop spending money running and maintaining data centers",
        "status": "right",
        "explanation": "One of the significant benefits of cloud computing is cost-saving. Instead of investing in running and maintaining data centers, which includes costs for premises, hardware, utilities, and workforce, you can use cloud services. In the cloud, these operational tasks are managed by the cloud service provider. This allows businesses to focus more on their core business operations rather than maintaining data centers."
      },
      {
        "title": "More control over underlying cloud infrastructure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Virtualized compute resources",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the key advantage of decoupling components in an AWS architecture?",
    "type": "radio",
    "options": [
      {
        "title": "Reduced interdependency",
        "status": "right",
        "explanation": "In a decoupled architecture, components are separated and interact with each other through clearly defined interfaces. This means that changes to one component do not impact other components. This reduces interdependencies between components and making the system as a whole more flexible and easier to maintain. For example, if one component needs to be updated or replaced, it can be done without affecting the other parts of the system."
      },
      {
        "title": "Reduced operational costs",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Increased speed of deployment",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reduced complexity",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which type of Elastic Load Balancer can handle millions of requests per second at the TCP and UDP connection level?",
    "type": "radio",
    "options": [
      {
        "title": "Classic Load Balancer (CLB)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Network Load Balancer (NLB)",
        "status": "right",
        "explanation": "Network Load Balancers are designed to handle millions of requests per second while maintaining high throughput and ultra-low latency. It operates at the connection level (Layer 4), making it ideal for managing TCP, UDP, TCP_UDP, and TLS traffic. NLB is particularly useful for applications that require extreme performance and can handle volatile traffic patterns while scaling seamlessly. It efficiently manages sudden and volatile traffic patterns, which makes it suitable for scenarios like gaming, IoT, and real-time communication. Additionally, its ability to preserve the client IP addresses simplifies the backend configuration, ensuring seamless integration with your existing systems and logs. Overall, for high-performance, scalable, and low-latency applications operating at the transport layer, NLB is the best choice."
      },
      {
        "title": "Application Load Balancer (ALB)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Gateway Load Balancer (GLB)",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following statements accurately describes AWS Support Plans? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "AWS Basic Support includes technical support via email during business hours.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Enterprise Support includes a Well-Architected review and support concierge service.",
        "status": "right",
        "explanation": "AWS Enterprise Support is the highest level of support plan. It includes a comprehensive set of features designed to help large organizations with complex infrastructure. This plan provides access to a Technical Account Manager (TAM), a Well-Architected review of your workloads to ensure best practices, and a support concierge service that assists with account and billing inquiries. These services are critical for enterprises needing in-depth support and guidance on their AWS architecture."
      },
      {
        "title": "The AWS Business Support Plan offers access to a Technical Account Manager (TAM).",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Basic Support includes 24/7 access to customer service and documentation.",
        "status": "right",
        "explanation": "AWS Basic Support is the most fundamental support plan and it provides 24/7 access to customer service, technical forums, and documentation. This plan is designed for users who want a basic level of service and do not require personalized support. It is ideal for small businesses and developers who need reliable guidance and comprehensive documentation to troubleshoot issues on their own."
      },
      {
        "title": "The AWS Developer Support Plan offers real-time chat support for critical issues.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Within the AWS Cloud Adoption Framework (CAF), which capability is identified under the People Perspective for organizational adaptation to cloud technology?",
    "type": "radio",
    "options": [
      {
        "title": "Cloud fluency",
        "status": "right",
        "explanation": "Cloud fluency emphasizes the importance of education and knowledge across the organization about cloud technologies and AWS services. Ensuring that staff are cloud-fluent means they understand not only the technical aspects but also the operational, financial, and business implications of using AWS services. This comprehensive understanding enables the organization to maximize the benefits of cloud adoption, fostering innovation, accelerating time to market, and optimizing costs. Cloud fluency supports a culture of continuous learning, empowering teams to make informed decisions, innovate with new services, and adapt processes to use cloud advantages fully. By focusing on cloud fluency, organizations can ensure their workforce is equipped with the necessary skills and knowledge to navigate the complexities of cloud adoption and harness the full potential of AWS Cloud technologies."
      },
      {
        "title": "Infrastructure optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Financial management",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Governance and compliance",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service helps migrate an Oracle database to AWS without disrupting existing database operations?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Database Migration Service",
        "status": "right",
        "explanation": "AWS Database Migration Service (DMS) is designed to facilitate the secure, efficient, and relatively easy migration of databases to AWS, including relational databases, data warehouses, NoSQL databases, and other types of data stores. The service supports the migration of data to and from the most widely used commercial and open-source databases. It allows for continuous data replication with high availability and minimal downtime, which is crucial for maintaining the functionality of the source database during the migration process. DMS is a highly recommended solution for companies looking to migrate their Oracle database to AWS without negatively impacting the source database's operations. This service minimizes the downtime to applications that rely on the database and ensures that data is transferred securely and efficiently, enabling a smooth transition to AWS."
      },
      {
        "title": "Amazon EC2",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following statement is true about AWS WAF?",
    "type": "radio",
    "options": [
      {
        "title": "WAF continually scans AWS workloads for software vulnerabilities.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "WAF always detects and provides a safeguard from DDoS attacks.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "WAF creates security rules to protect from cross-site scripting attacks.",
        "status": "right",
        "explanation": "AWS WAF, or Web Application Firewall, is a security service that protects web applications from common web exploits, such as SQL injection and cross-site scripting (XSS) attacks. With AWS WAF, You can create custom security rules (also known as web access control lists or web ACLs) that block, allow, or monitor (count) web requests based on conditions you define. These conditions can include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection patterns, and XSS patterns, helping to protect your application from these common threats."
      },
      {
        "title": "WAF monitors the incoming requests that are coming from Amazon Route 53.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service lets you use Chef and Puppet to automatically configure, deploy, and manage servers in an Amazon EC2 instance or on-premises data center?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS OpsWorks",
        "status": "right",
        "explanation": "AWS OpsWorks is a configuration management service that automates server provisioning, configuration, and deployment on AWS. It supports Chef and Puppet, allowing users to define configurations as code for their infrastructure. OpsWorks automates tasks such as software installation, code deployment, and auto-scaling, streamlining operations for applications running on AWS. It provides flexibility in managing both EC2 instances and on-premises servers, and integrates with other AWS services like CloudWatch and Auto Scaling for monitoring and scaling resources. OpsWorks simplifies infrastructure management, enabling developers to focus more on building and improving their applications."
      }
    ]
  },
  {
    "title": "What is the difference between an AWS IAM policy and a role?",
    "type": "radio",
    "options": [
      {
        "title": "A policy is a set of permissions that determine what an AWS service can do, while a role is a temporary identity that can be assumed by a user or AWS service.",
        "status": "right",
        "explanation": "AWS Identity and Access Management (IAM) policy is a JSON document that is a formal statement of one or more permissions. Policies control who (user or service) can access what resources in what context. On the other hand, an IAM role is not associated with a specific user or group. Instead, trusted entities, like IAM users, applications, or AWS services like EC2, can assume a role to obtain temporary security credentials that can be used to make AWS API requests. This mechanism allows the delegation of permissions and secure access to services and resources"
      },
      {
        "title": "A policy is a set of permissions that determine what an AWS user can do, while a role is a set of permissions that determine what an AWS service can do.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A policy is a set of permissions that determine what an AWS user can do, while a role is a permanent identity that can access AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "A policy is a set of permissions that determine what an AWS service can do, while a role is a set of permissions that determine what an AWS user can do.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement most accurately describes the cloud design principle of loose coupling?",
    "type": "radio",
    "options": [
      {
        "title": "Components should be interdependent to enhance functionality.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Components should rely heavily on each other for data processing.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Components should interact with each other through middleware interfaces.",
        "status": "right",
        "explanation": "Components interacting through middleware interfaces encapsulate functionality, enabling the components to evolve independently without causing disruptions in the overall system. This approach adheres to the principle of loose coupling, which minimizes dependencies between components. Middleware provides a standard method for communication, enhancing system flexibility, scalability, and maintainability. By decoupling components, changes, updates, or replacements of individual parts can be done seamlessly, leading to a more resilient architecture. This design is particularly beneficial in cloud environments where scalability and rapid iteration are critical."
      },
      {
        "title": "Components should be tightly integrated to ensure maximum performance.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Under the AWS Shared Responsibility Model, which of the following is considered a shared control?",
    "type": "radio",
    "options": [
      {
        "title": "Patching of the guest OS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Configuring Network protection",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Awareness and training",
        "status": "right",
        "explanation": "AWS operates under a shared responsibility model, dividing security and compliance tasks between AWS and its customers. AWS manages the security \"of\" the cloud, including hardware, software, networking, and facilities. Customers are responsible \"in\" the cloud, managing security configurations, data encryption, access management, and compliance adherence. This collaborative approach ensures a secure cloud environment while allowing customers flexibility and control over their applications and data. \"Awareness and training\" is a shared control because while AWS can provide foundational security and compliance information about its cloud services, it is ultimately up to the customer to ensure their team understands and adheres to security and compliance practices. AWS can offer guidelines, but implementing these within user organizations requires a concerted effort from the customerâ€™s side. Therefore, it falls under shared responsibilities where both parties contribute towards proper awareness and training."
      },
      {
        "title": "Customer data",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement best describes the use of AWS Elastic Beanstalk?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Elastic Beanstalk provides a platform for deploying and scaling web applications with limited access to the underlying operating system.",
        "status": "right",
        "explanation": "AWS Elastic Beanstalk is a Platform as a Service (PaaS) that simplifies the deployment and scaling of web applications and services developed in Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker. It handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. You don't have full control over the underlying infrastructure as you would with an Infrastructure as a Service (IaaS) offering like Amazon EC2. Elastic Beanstalk does provide limited access to the underlying operating system. You can connect to the instances that run your application, view detailed log files, and make changes to your environment. This limited access allows you to focus on writing code without worrying about the infrastructure that runs your applications."
      },
      {
        "title": "AWS Elastic Beanstalk is not suitable for deploying PHP web applications.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Elastic Beanstalk provides a platform for deploying and scaling web applications and gives full access to the underlying operating system.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Elastic Beanstalk provides a platform for deploying and scaling web applications without giving access to the underlying operating system.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service enables the extension of AWS infrastructure, services, APIs, and tools into on-premises data centers or co-location spaces?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon EC2",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon VPC",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Direct Connect",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Outposts",
        "status": "right",
        "explanation": "AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to virtually any on-premises facility, including data centers and co-location environments, creating a truly consistent hybrid experience. This solution is ideal for applications that require low latency access to on-premises systems, local data processing, or local data storage. By bringing native AWS services to on-premises locations, Outposts enables customers to operate in a hybrid environment seamlessly, using the same AWS management console, APIs, and control services they're familiar with in the cloud. This ensures a unified way to manage both cloud and on-premises resources, simplifying operations and reducing the time to market for applications that need to span both environments."
      }
    ]
  },
  {
    "title": "A company wants to set up and manage a secure multi-account AWS environment with best practices. Which AWS service can do this in the easiest way?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Resource Access Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Control Tower",
        "status": "right",
        "explanation": "AWS Control Tower simplifies the setup and management of a multi-account AWS environment, following AWS best practices and security guidelines. It automates the process of provisioning new AWS accounts, configuring baseline networking, security, and logging settings, and continuously monitoring compliance. Control Tower provides a centralized dashboard for administrators to govern and manage all accounts, ensuring consistency and adherence to organizational policies. It integrates with AWS organizations, AWS Single Sign-On (SSO), and other AWS services to streamline user access and permissions management across the entire infrastructure. This enables organizations to scale securely and efficiently while maintaining governance and compliance standards."
      }
    ]
  },
  {
    "title": "Which AWS service provides Service Organization Control (SOC) and Payment Card Industry (PCI) reports?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Resource Access Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Artifact",
        "status": "right",
        "explanation": "AWS Artifact is a repository of compliance documentation, offering access to various audit reports, certifications, and agreements related to AWS services. It simplifies compliance and regulatory requirements for customers by providing on-demand access to documents such as Service Organization Control (SOC) reports, Payment Card Industry(PCI) DSS compliance packages, and HIPAA agreements. AWS Artifact assists in auditing, risk assessment, and meeting regulatory obligations by centralizing necessary documentation in a secure, easily accessible platform. This service ensures transparency and trustworthiness in AWS's security and compliance practices, aiding customers in their own compliance efforts."
      },
      {
        "title": "AWS Directory Service",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following AWS services help optimize Amazon EC2 instances for better performance and lower costs by suggesting the right sizes or types? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "AWS Billing Conductor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SageMaker",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cost Explorer",
        "status": "right",
        "explanation": "AWS Cost Explorer is an analytical tool that enables you to visualize, understand, and manage your AWS costs and usage over time. With Cost Explorer, you can identify trends, pinpoint cost drivers, and detect anomalies. Specifically, it provides detailed reports and insights into your spending patterns, including recommendations for how you might reduce costs by rightsizing your services. For Amazon EC2 instances, Cost Explorer can help identify instances that are underutilized or oversized, offering suggestions on how to adjust the size or type of these instances to better align with actual usage, thereby optimizing costs."
      },
      {
        "title": "Amazon CodeGuru",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Compute Optimizer",
        "status": "right",
        "explanation": "AWS Compute Optimizer uses machine learning to analyze historical utilization metrics and configuration data for your AWS resources, offering recommendations on how to optimize compute resources across Amazon EC2 instances, Amazon EBS volumes, and AWS Lambda functions. For EC2 instances, it provides recommendations on changing instance types or sizes to match your workload requirements more closely, potentially leading to cost savings and performance improvements. This tool is particularly useful for identifying rightsizing opportunities by suggesting configurations that could deliver the same or better performance at a lower cost."
      }
    ]
  },
  {
    "title": "A large company uses a multi-account environment in the AWS cloud for its multiple projects. To follow security best practices, they want to ensure that all IAM user access keys are rotated every 90 days. As a cloud practitioner, what solutions would you recommend for this need?",
    "type": "radio",
    "options": [
      {
        "title": "Implement AWS Lambda functions to automatically delete keys that are 90 days old.",
        "status": "right",
        "explanation": "Service Control Policy (SCPs) is a feature of AWS Organizations that allows administrators to define specific permissions for accounts under their management. SCPs enable central governance and restriction of services and actions across multiple AWS accounts. They act as a guardrail, ensuring accounts operate within defined parameters and can be used to enforce compliance, security practices, and cost controls by either whitelisting or blacklisting specific AWS service actions. An SCP that mandates the rotation of IAM user access keys every 90 days can be applied across all accounts in the organization, ensuring compliance. With this policy in place, IAM users will be unable to perform AWS actions with keys older than 90 days, prompting them to rotate their keys. This method offers a proactive and enforced approach to maintain security best practices."
      },
      {
        "title": "Use AWS CloudTrail to monitor access key usage and set up alerts to notify when keys are 90 days old.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Configure AWS CloudWatch to automatically reset keys after every 90 days of creation.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use AWS Organizations Service Control Policies (SCPs) to enforce mandatory access key rotation every 90 days.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which feature allows users to log into their AWS accounts using their existing workplace credentials?",
    "type": "radio",
    "options": [
      {
        "title": "AWS IAM Users",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Access keys",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Multi-Factor Authentication (MFA)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Federation",
        "status": "right",
        "explanation": "Federation allows users to access AWS services using credentials from an external identity provider. It supports identity federation with SAML (Security Assertion Markup Language), allowing integration with on-premises directories or other identity providers. This means existing workplace credentials can be used, making it easier for organizations to manage access and maintain security protocols without creating new credentials specifically for AWS."
      }
    ]
  },
  {
    "title": "Which capability from the AWS CAF governance perspective is required to define and track business outcomes during a cloud transformation journey?",
    "type": "radio",
    "options": [
      {
        "title": "Risk management",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security management",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Financial management",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Benefits management",
        "status": "right",
        "explanation": "The AWS Cloud Adoption Framework (CAF) is a comprehensive guide designed to assist organizations in adopting AWS cloud services effectively. It provides a structured approach covering business, people, and technology aspects to facilitate successful cloud migration and integration. CAF helps organizations align their cloud strategy with business objectives, establish a governance model, define roles and responsibilities, and implement best practices for security, compliance, and cost management. It supports organizations in navigating the complexities of cloud adoption while ensuring scalability, agility, and innovation. Benefits management within CAF focuses on identifying, quantifying, and realizing the expected outcomes from cloud adoption initiatives. This involves defining clear metrics, establishing monitoring mechanisms, and optimizing cloud usage to maximize cost savings, operational efficiency, scalability, and innovation within the organization's digital transformation journey."
      }
    ]
  },
  {
    "title": "A company is developing a business intelligence solution and wants to use a dashboard for reporting purposes. Which AWS service should be used?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Glue",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Redshift",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Athena",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon QuickSight",
        "status": "right",
        "explanation": "Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization. With QuickSight, you can create and publish interactive dashboards that include ML Insights. Dashboards can then be accessed from any device, and embedded into your applications, portals, and websites. Hence, for developing a business intelligence solution with a focus on dashboard reporting, Amazon QuickSight is the most suitable AWS service."
      }
    ]
  },
  {
    "title": "Which AWS service supports CI (continuous integration) that compiles source code, runs tests, and produces software packages?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CodeBuild",
        "status": "right",
        "explanation": "AWS CodeBuild is a fully managed build service that compiles your source code, runs tests, and produces software packages. It is a part of the Continuous Integration (CI) and Continuous Deployment (CD) process. AWS CodeBuild eliminates the need to set up, patch, update, and manage your own build servers, providing prepackaged build environments for popular programming languages and build tools. You can also define custom build environments to use your own build tools. By supporting the CI process, AWS CodeBuild enhances developer productivity and code quality by automating the build and test phases of your software release process."
      },
      {
        "title": "AWS CodeCommit",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeDeploy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodePipeline",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A startup wishes to move to the AWS cloud and wants to avoid complex forecasts in determining its compute resource usage. The CTO prefers to pay only for resources consumed and also needs the ability to scale up or down its usage as the business requirements change. Which AWS well-architected framework pillar refers to this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Cost Optimization Pillar",
        "status": "right",
        "explanation": "The Cost Optimization pillar of the AWS Well-Architected Framework addresses the requirements of the startup. This pillar provides best practices for avoiding unnecessary costs, understanding and controlling where money is being spent, analyzing costs over time, and scaling resources efficiently. In our case, the startup wants to avoid complex forecasts, pay only for resources consumed, and be able to scale resources up or down as per business needs, which aligns with the principles of the Cost Optimization pillar."
      },
      {
        "title": "Reliability Pillar",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational Excellence Pillar",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance Efficiency Pillar",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following should be used to improve content delivery performance to end-users? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Store media content in Amazon S3.",
        "status": "right",
        "explanation": "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Storing your media content in Amazon S3 allows you to organize and manage your data with a great deal of flexibility. It can serve as the origin for a content delivery network (CDN) solution like Amazon CloudFront."
      },
      {
        "title": "Increase server compute capacity.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use EBS for fastest IOPS transfer rate.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create a replica in a region near users.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use CloudFront to distribute content.",
        "status": "right",
        "explanation": "Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds within an environment that's easy to use. It works seamlessly with Amazon S3 to deliver content to end users. By caching copies of your content at edge locations around the world, it brings content closer to your users, reducing latency and improving load times."
      }
    ]
  },
  {
    "title": "What is used to control incoming and outgoing traffic at the subnet level?",
    "type": "radio",
    "options": [
      {
        "title": "Security Group",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Traffic Mirroring",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Network ACL",
        "status": "right",
        "explanation": "A Network Access Control List (ACL) is a security layer in AWS that acts as a firewall for controlling traffic at the subnet level. It operates by defining inbound and outbound rules that filter traffic based on IP addresses, protocols, and ports. Network ACLs provide an additional layer of defense to supplement security groups, allowing for more granular control over network traffic. It is stateless and evaluates rules in order, making them ideal for blocking specific types of traffic or implementing complex network security policies within a Virtual Private Cloud (VPC)."
      },
      {
        "title": "Routing policy",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is reviewing the AWS Cost and Usage report in the AWS Management Console and they have identified a billing issue. Which step should be taken to solve this issue?",
    "type": "radio",
    "options": [
      {
        "title": "Launch a right-sized Amazon EC2 to monitor new usage reports.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create a billing support case and submit it to AWS Support for assistance.",
        "status": "right",
        "explanation": "If a company identifies a billing issue, the most effective step to take is to create a billing support case and submit it to AWS Support for assistance. AWS Support is equipped to handle all types of inquiries, including billing issues. They can investigate the issue, provide explanations for the charges, and if there is indeed a mistake, they can fix it. It's a way to address the problem with the help of AWS experts who have access to your account and billing details."
      },
      {
        "title": "Upload billing data to a new Amazon S3 bucket for analysis with Amazon Athena.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create a new dashboard on Amazon QuickSight for looking at patterns and outliers.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following is key component of the value proposition offered by AWS Cloud?",
    "type": "radio",
    "options": [
      {
        "title": "Enhanced operational flexibility and agility.",
        "status": "right",
        "explanation": "Operational flexibility and agility are key components of AWS's value proposition. AWS provides a wide range of services and resources that can be quickly provisioned and scaled according to the changing needs of the business. This flexibility enables businesses to respond rapidly to market changes, test new ideas without substantial upfront costs, and scale operations up or down as needed. The agility offered by AWS supports innovation and helps businesses to stay competitive in their respective markets."
      },
      {
        "title": "Fixed long-term contracts for all services to ensure price stability.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Guaranteed increase in revenue for all types of businesses.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Inherent physical security of data centers.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A business uses AWS for several applications and needs to access AWS technical support within an hour. Which AWS support plan would be most cost-effective?",
    "type": "radio",
    "options": [
      {
        "title": "Basic Support Plan",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Developer Support Plan",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Business Support Plan",
        "status": "right",
        "explanation": "The Business Support plan is specifically designed for users who need rapid and comprehensive support. This plan ensures access to AWS technical support within one hour. It includes 24/7 phone, email, and chat support for critical issues, meaning businesses can receive timely assistance when they need it most. Additionally, this plan provides support for third-party applications, offers best practice recommendations, and proactively monitors AWS infrastructure. By opting for the Business Support plan, businesses can achieve a balance between cost and level of service, ensuring that their operational needs are met without the higher cost of the Enterprise plan."
      },
      {
        "title": "Enterprise Support Plan",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What should you consider when selecting an AWS Region for a service?",
    "type": "radio",
    "options": [
      {
        "title": "Popularity of the region among AWS users.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Highest number of availability zones.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Geographically located near the target users.",
        "status": "right",
        "explanation": "Placing resources close to the target users minimizes latency, improving application performance and user experience. Geographic proximity ensures faster data transfer rates as the data has a shorter distance to travel, thus reducing round-trip time. This factor is especially crucial for applications that require real-time interactions, like online gaming or video conferencing. AWS operates multiple regions globally, allowing businesses to strategically deploy their services where their users are concentrated. This choice can also impact the bottom line as faster, more reliable services can lead to increased customer satisfaction and retention."
      },
      {
        "title": "Proximity to the organization's headquarters.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "In the AWS Well-Architected Framework, which pillar provides guidance on selecting appropriate compute resources based on workload needs?",
    "type": "radio",
    "options": [
      {
        "title": "Operational Excellence",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance Efficiency",
        "status": "right",
        "explanation": "The Performance Efficiency pillar of the AWS Well-Architected Framework provides guidance to customers on selecting appropriate resources to meet their workload requirements. It focuses on using cloud technology to achieve the highest level of performance for a given workload. This includes considering how to use computing resources efficiently, and selecting the right type of resources (e.g., compute, storage, database, and network) for the workload. The pillar also encourages customers to experiment with different configurations to understand what delivers the best performance for specific needs."
      },
      {
        "title": "Cost Optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Sustainability",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What are the key areas covered by the AWS Acceptable Use Policy?",
    "type": "radio",
    "options": [
      {
        "title": "Uptime and reliability, pricing and billing, and customer support.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Business continuity, disaster recovery, and risk management.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security, compliance, data protection, and intellectual property.",
        "status": "right",
        "explanation": "The AWS Acceptable Use Policy (AUP) outlines the key areas that users must comply with when using AWS services. These areas include security, where AWS stipulates that users cannot carry out any actions that can compromise the security of AWS services or other users. Compliance refers to adhering to all applicable laws and regulations while using AWS services. Data protection means ensuring the privacy and security of personal data stored or processed on AWS. Intellectual property involves respecting the rights of others and not using AWS services to infringe upon these rights, such as copyrights, trademarks, or patents. In essence, the AUP guides AWS users on how to use the services responsibly and legally."
      },
      {
        "title": "Data storage, access control, and identity management.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to use AWS DataSync for transferring a substantial volume of data to Amazon S3. For compliance and security purposes, they must ensure the successful transfer and data consistency between the source and destination. How does AWS meet these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "AWS expects users to manually verify data consistency post-transfer.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS sends a confirmation email after successful data transfer.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS offers a separate service to validate data post-transfer.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS automatically performs data validation after every DataSync task.",
        "status": "right",
        "explanation": "AWS DataSync includes automatic data validation as a feature. After DataSync transfers the data to the destination, it automatically validates the data by comparing the metadata from the source and destination locations, ensuring that data was transferred accurately and completely. This helps users to have confidence that the transferred data is consistent with the source data."
      }
    ]
  },
  {
    "title": "Which AWS service allows you to test applications across multiple desktop browsers and mobile devices?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Polly",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeBuild",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Rekognition",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Device Farm",
        "status": "right",
        "explanation": "AWS Device Farm allows you to test your applications on real physical devices in the AWS Cloud. Device Farm supports testing web applications on a wide range of desktop browsers and mobile devices, including various models of Android, iOS, and FireOS devices. With Device Farm, you can simultaneously start testing your app on multiple device configurations, identify issues that might only surface in a few specific scenarios, and make your application more robust and reliable for your users. It helps you improve the quality of your application by testing and interacting with your Android, iOS, and web apps on many devices at once."
      }
    ]
  },
  {
    "title": "A company uses Amazon RDS for PostgreSQL. They are experiencing spikes in user traffic and want to ensure that the database remains available under heavy read traffic. As a cloud practitioner, which AWS solutions should you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "Increase the instance size of the existing RDS instance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Deploy an RDS Read Replica.",
        "status": "right",
        "explanation": "For read-heavy database workloads, Amazon RDS Read Replicas provide enhanced performance and durability. Read Replicas asynchronously replicate data from a source RDS instance, allowing for offloading read traffic from the primary database. They provide low-latency access to read-only copies of the data, enhancing performance for applications with high read demand. Read Replicas can be promoted to standalone instances in case of primary instance failure, ensuring data availability and disaster recovery. They contribute to improved scalability, performance, and fault tolerance in RDS deployments."
      },
      {
        "title": "Implement AWS Lambda to manage database traffic.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon Elasticache with Redis.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is planning to run a new application on an Amazon EC2 instance. They have limited funds and are very concerned about overspending. So they want to receive an alert when the monthly bill exceeds $500. As a cloud practitioner, which of the following options do you recommend for this requirement? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Configure the Amazon EC2 instance to receive an alert when the threshold is exceeded.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Configure AWS Budgets to receive an alert when the threshold is exceeded.",
        "status": "right",
        "explanation": "AWS Budgets enables you to set custom cost and usage budgets. It allows you to get alerts (via email or Amazon SNS) when your AWS usage or costs exceed the thresholds you've set, like the monthly limit of $500 in our case. These alerts can help to manage costs and reduce the risk of unexpected charges."
      },
      {
        "title": "Configure Amazon CloudWatch to trigger billing alerts when the threshold is exceeded.",
        "status": "right",
        "explanation": ""
      },
      {
        "title": "Configure the Amazon SES to get an email when the threshold is exceeded.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Configure the AWS Cost Explorer to get an alert when the threshold is exceeded.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service reroutes user traffic to the optimal endpoint to improve global application availability and performance?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon CloudFront",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Global Accelerator",
        "status": "right",
        "explanation": "AWS Global Accelerator is designed to improve the availability and performance of your applications for global users. It works by routing user traffic through the AWS global network infrastructure, which improves the performance of your internet applications by redirecting user connections to the nearest edge location, thus reducing internet latency and packet loss. Global Accelerator uses Anycast routing to direct user traffic to the closest healthy endpoint for your application, which could be a Network Load Balancer, an Application Load Balancer, an EC2 instance, or an Elastic IP address. If an endpoint becomes unavailable, Global Accelerator automatically reroutes user traffic to the next closest healthy endpoint, thereby improving your application's availability."
      },
      {
        "title": "AWS Outposts",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Direct Connect",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to run an application on the AWS cloud and needs to improve the resiliency of the application. They want to use an AWS-managed database instead of a traditional database system. Active-active configuration with cross-region support is the main criterion for the company's consideration for any database solution. Which AWS database should they use for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon DynamoDB with global tables",
        "status": "right",
        "explanation": "DynamoDB is a NoSQL database service that provides fast and predictable performance with seamless scalability. Global tables build upon the basic DynamoDB offerings to provide a fully managed, multi-region, and multi-master database. It replicates your data across multiple AWS regions, providing fast, local access to data for globally distributed applications. With global tables, you can update data in any region and have it automatically propagated to all other regions. This allows your application to remain highly available and resilient even in the face of region-wide disruptions. Amazon DynamoDB with global tables would be the ideal solution for the company for active-active database configuration with cross-region support."
      },
      {
        "title": "Amazon Aurora with multi-master cluster",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon DynamoDB with DynamoDB Accelerator",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS with Multi-AZ Deployments",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to migrate its existing infrastructure to the AWS cloud and is interested in using serverless services for its applications. What is the main advantage of serverless architecture by moving to the AWS cloud?",
    "type": "radio",
    "options": [
      {
        "title": "Better data storage options",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Greater control over infrastructure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Increased security and compliance",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Improved fault tolerance and reliability",
        "status": "right",
        "explanation": "Serverless services like AWS Lambda and Amazon S3 are designed to be highly reliable and fault tolerant. They automatically replicate your applications and data across multiple data centers in an AWS Region to help ensure they are available when needed. This improved fault tolerance and reliability, reducing the risk of application downtime."
      }
    ]
  },
  {
    "title": "A company is running a large microservices application on over 50 EC2 instances. The CTO wants an automated service that will scan for software vulnerabilities and unintended network exposures 24x7. As a cloud practitioner, which service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Shield",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Inspector",
        "status": "right",
        "explanation": "Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. It can continuously monitor the company's EC2 instances, and it will be a perfect fit for the needs. The Inspector agent that runs on the EC2 instances collects behavior-based data, which can help identify when and where you might have software vulnerabilities or unintended network exposures."
      }
    ]
  },
  {
    "title": "A company wants to assign a group of employees to manage EC2 instances without deleting permission. What is the most efficient method to ensure this specific permission?",
    "type": "radio",
    "options": [
      {
        "title": "Attach the AmazonEC2FullAccess policy to the IAM group but set service control policies to prevent EC2 termination.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create a custom IAM policy that allows ec2:StartInstances and ec2:StopInstances actions and deny ec2:TerminateInstances, then attach it to the IAM group.",
        "status": "right",
        "explanation": "An IAM (Identity and Access Management) policy in AWS is a JSON document that defines one or more permissions. These policies dictate what actions a user, group, or role can and cannot perform on specific AWS resources. By attaching these policies, administrators can grant or restrict access, ensuring fine-tuned control over their AWS environment based on the principle of least privilege. The most effective approach to achieve the given requirements is to create a custom IAM policy. By explicitly allowing ec2:StartInstances and ec2:StopInstances while denying ec2:TerminateInstances, you can ensure that the IAM group has the exact permissions they need and no more."
      },
      {
        "title": "Provide full EC2 access to the IAM group and instruct them not to terminate instances.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Attach a managed policy, like AmazonEC2ReadOnlyAccess, to the IAM group.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has highly secure sensitive data and wants to use a security device to manage cryptographic keys. As a cloud practitioner, which AWS service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudHSM",
        "status": "right",
        "explanation": "AWS CloudHSM provides hardware-based key storage and cryptographic operations within a tamper-resistant hardware device. This service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated HSM appliances within the AWS Cloud. CloudHSM empowers users to have complete control over their encryption keys by using FIPS 140-2 Level 3 validated HSMs. This service provides the flexibility to seamlessly integrate with applications using popular APIs like PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries. By using CloudHSM, companies can ensure the highest level of security for their cryptographic operations while enjoying the flexibility and convenience of industry-standard interfaces."
      },
      {
        "title": "AWS IAM",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS KMS",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which service lets you view and manage operational data from multiple AWS resources?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Systems Manager",
        "status": "right",
        "explanation": "AWS Systems Manage gives you visibility and control of your infrastructure on AWS. This service allows you to group your resources according to applications, view operational data for monitoring and troubleshooting, and take action on your groups of resources. AWS Systems Manager simplifies resource and application management, shortens the time to detect and resolve operational problems, and makes it easy to operate and manage your infrastructure securely at scale. It provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources."
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Pinpoint",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What are the key features of AWS Snowball? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Automated data backup to AWS on a daily basis",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Real-time analytics on the device",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Local data processing capabilities",
        "status": "right",
        "explanation": "AWS Snowball is equipped with built-in computing capabilities, allowing users to perform data processing tasks locally before transferring the data to AWS. This feature is particularly useful for situations where data needs to be pre-processed or filtered to meet specific requirements or to reduce the volume of data that needs to be transferred over the network. The local processing capabilities ensure that only the necessary data is uploaded to the cloud, optimizing transfer times and costs."
      },
      {
        "title": "High-speed and secure data migration to AWS Cloud",
        "status": "right",
        "explanation": "AWS Snowball provides a secure and efficient way to transfer large amounts of data into and out of the AWS Cloud. It addresses challenges associated with high network costs, long transfer times, and security concerns. By using Snowball, customers can significantly reduce the time and expense involved in moving large datasets by physically shipping the data on Snowball devices, which are designed to be secure and durable. The devices use encryption and other security measures to protect data during transit, making them a reliable choice for sensitive or regulated data."
      },
      {
        "title": "Unlimited data storage on the device",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "In the context of AWS Cloud, what aspects define the concept of elasticity? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "The ability to adjust resource allocation in response to changing demand.",
        "status": "right",
        "explanation": "Elasticity refers to the ability of the infrastructure to automatically scale resources up or down according to fluctuating demand. This means that applications can maintain optimal performance levels even as the number of requests increases or decreases, ensuring cost-efficiency by only using resources when they are needed. This capability supports dynamic business needs, allowing for seamless user experiences without manual intervention for resource adjustments."
      },
      {
        "title": "The ability to predict future resource requirements accurately.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The speed at which physical servers can be added to a data center.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The flexibility in changing cloud service providers.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The speed at which additional resources are made available on demand.",
        "status": "right",
        "explanation": ""
      }
    ]
  },
  {
    "title": "As a cloud practitioner, which AWS service would you recommend to view the most detailed information about AWS costs?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cost Explorer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cost & Usage Reports",
        "status": "right",
        "explanation": "AWS Cost & Usage Reports (CUR) is the most comprehensive tool for examining your AWS costs in detail. These reports deliver the most granular level of data about your AWS costs, and they enable you to drill down into your data to gain insights about your AWS usage and spending. The reports contain line item details of your costs, resource usage, and Reserved Instance usage across all AWS services. You can customize the CUR to aggregate data by the hour or day, by product or product resource, or by tags that you define yourself. AWS CUR provides the most extensive data to understand, analyze, and optimize your AWS costs."
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A large organization uses a single AWS account to operate and manage all its projects. Now they realize that it has become complicated, so the CTO wants to switch from this single account to multiple AWS accounts. Which of the following are the advantages of this reconfiguration? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "It makes it easier to keep different projects separate from each other.",
        "status": "right",
        "explanation": "Using multiple AWS accounts helps in creating a clear boundary between different projects or departments within a large organization. This separation is crucial for maintaining security and governance at a granular level. By isolating environments, resources, and permissions, organizations can ensure that one project or team's actions do not inadvertently affect another. This isolation also simplifies compliance with internal policies and external regulations, as each account can have its settings and controls tailored to specific needs."
      },
      {
        "title": "Using more than one account can protect against the damage caused by hackers focusing on just one account.",
        "status": "right",
        "explanation": "Having multiple AWS accounts is a strong strategy for enhancing security. If a hacker manages to compromise one account, the impact of this breach is contained within that single account, significantly reducing the risk to the organization's other assets and operations. This compartmentalization of resources acts as a barrier, making it harder for malicious activities to spread. Furthermore, AWS provides tools and services like AWS Organizations for managing multiple accounts, enabling centralized billing, governance, and security policy application across all accounts."
      },
      {
        "title": "The company can get discounts every year by asking for them through the AWS Management Console.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon QuickSight provides a special tool for providing advice on how to save money when using multiple accounts.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Moving files from Amazon S3 to Amazon S3 Glacier will less cost if used in different AWS accounts.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to the shared responsibility model, which operational controls does a customer fully inherit from AWS? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Physical controls",
        "status": "right",
        "explanation": "In the AWS shared responsibility model, AWS is responsible for the security 'of' the cloud, while the customer is responsible for security 'in' the cloud. Physical controls, which involve the physical security of data centers and infrastructure, fall under AWS's responsibility. This includes the physical security of the buildings, servers, networking hardware, and other physical assets used to provide AWS services. As such, customers effectively inherit these controls from AWS."
      },
      {
        "title": "Patch management controls",
        "status": "right",
        "explanation": "Environmental controls, which include factors such as HVAC systems, power supply systems, fire prevention and suppression, and others that maintain a secure and stable environment for the infrastructure, are also AWS's responsibility. AWS manages these controls in their data centers to ensure the continuity and reliability of their services. Thus, these are also fully inherited by AWS customers."
      },
      {
        "title": "Configuration management controls",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Awareness & Training controls",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Patch management controls",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is in the process of designing an application architecture on AWS. They need to build a system that is both resilient and efficient. What of the following are key principles for designing systems on AWS? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Manually scale resources based on demand",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Automate wherever possible",
        "status": "right",
        "explanation": "Automation is a key principle of cloud-native architecture that offers significant benefits in terms of efficiency, reliability, and scalability. By automating processes such as deployments, scaling, and backups, you not only reduce the potential for human error but also enable your system to respond more quickly to changes in demand or environment. Automation can be achieved through services like AWS CloudFormation for infrastructure as code, AWS Auto Scaling for automatic resource adjustment, and AWS Lambda for event-driven automation. Incorporating automation into your AWS system design ensures that your infrastructure can manage itself as much as possible."
      },
      {
        "title": "Store all data in a single availability zone for simplicity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Remove single points of failure",
        "status": "right",
        "explanation": "Designing systems that are resilient against failures requires the elimination of single points of failure. This means architecting your application in a way that no single component's failure can render the entire system unusable. On AWS, this can be achieved by using multiple Availability Zones for critical components, employing load balancing with Amazon Elastic Load Balancing (ELB), and replicating databases across regions or availability zones with services like Amazon RDS or DynamoDB. By removing single points of failure, you ensure that your application remains available and functional, even in the event of component failures, contributing to a better user experience and higher system reliability."
      },
      {
        "title": "Use a single large instance for better performanceUse a single large instance for better performance",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A multinational company operates multiple VPCs across multiple regions. They need a centralized and secure network communication between these VPCs. Which AWS service would be most suitable to achieve this goal?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Direct Connect",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS PrivateLink",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS VPN",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Transit Gateway",
        "status": "right",
        "explanation": "AWS Transit Gateway is a network transit hub that connects Amazon VPCs and on-premises networks through a central hub. This simplifies network architectures, reducing the complexity and management overhead associated with connecting multiple VPCs and networks. By providing a scalable and high-performance solution, AWS Transit Gateway enables efficient routing and connectivity, supporting thousands of VPCs and VPN connections. It integrates seamlessly with AWS services, enhances security, and supports features like multicast and inter-region peering, making it a versatile choice for large-scale, dynamic cloud environments."
      }
    ]
  },
  {
    "title": "A company wants to develop an application that requires sending, storing, and receiving messages across its components in a first-in, first-out (FIFO) delivery process. Which AWS service should be used?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Step Functions",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Glue",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Kinesis Data Streams",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SQS",
        "status": "right",
        "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Importantly, Amazon SQS offers two types of message queues: Standard and FIFO. FIFO queues have the exact ordering of messages and ensure that exactly-once processing is there, thus serving the company's requirements in the best possible way."
      }
    ]
  },
  {
    "title": "Which AWS tool provides an easy-to-use graphical user interface for managing AWS Snowball devices?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Command Line Interface (CLI)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Data Transfer Hub",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS OpsHub",
        "status": "right",
        "explanation": "AWS OpsHub simplifies the management of AWS Snowball devices, enabling easy deployment of edge computing workloads and data migration to the cloud. It offers a graphical interface to configure devices, transfer data, launch applications, and monitor metrics, replacing traditional CLI and API methods. OpsHub supports offline operations and integrates with AWS Systems Manager for automated tasks, making it accessible for users of varying technical expertise to manage Snow Family Devices effectively."
      },
      {
        "title": "AWS Management events",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which service can be used for geographical restrictions of incoming traffic?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Shield",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS WAF",
        "status": "right",
        "explanation": "AWS Web Application Firewall (WAF) allows you to control the incoming traffic to your web applications. One of the features it offers is the ability to set geographical restrictions on incoming traffic. With AWS WAF, you can create rules that block, allow, or monitor (count) web requests based on conditions that you define. These conditions can include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting attacks, and the geographical locations from which requests originate."
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudWatch",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service or feature enables you to receive alerts for unauthorized or unusual billing activity that indicates potential fraud or mismanagement?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Identity and Access Management (IAM) policies",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon GuardDuty",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cost Anomaly Detection",
        "status": "right",
        "explanation": "AWS Cost Anomaly Detection service is designed to monitor for unusual spending patterns within an AWS account. It uses machine learning to detect anomalies in your AWS spending, and alerts you when unexpected changes in cost and usage occur, which could indicate fraudulent activity or mismanagement. This service is ideal for those looking to keep track of their AWS billing and be alerted to potential issues promptly."
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following tasks are AWS's responsibility under the AWS Shared Responsibility Model? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Ensuring the physical security of AWS data centers globally.",
        "status": "right",
        "explanation": "AWS is fully responsible for safeguarding the physical premises of its data centers around the world. This encompasses a variety of security measures such as surveillance, security staff , and access control mechanisms to prevent unauthorized access, thereby ensuring the integrity and availability of the hardware and services they provide to their customers."
      },
      {
        "title": "Setting up the operating system's firewall on an AWS Elastic Beanstalk environment.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Managing the encryption of data at rest within AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Conducting background checks on staff with data center access privileges.",
        "status": "right",
        "explanation": "AWS is responsible for the security and maintenance of its data centers, including the staff who have direct access to the hardware and facilities. Conducting background checks on these individuals is an AWS responsibility to ensure that the infrastructure remains secure from insider threats and that only trustworthy and verified staff can access critical infrastructure components."
      },
      {
        "title": "Implementing application-side encryption for sensitive data.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "In the AWS Shared Responsibility Model, which of the following are the customer's responsibilities? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Physical security of AWS data centers.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Patching and upgrading the network infrastructure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Maintenance of the underlying hardware infrastructure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Management of user access and identity permissions.",
        "status": "right",
        "explanation": "In the AWS Shared Responsibility Model, customers are responsible for managing user access and identity permissions. This includes the use of AWS Identity and Access Management (IAM) to control who can access specific AWS resources and what actions they can perform. Effective management of access permissions is crucial for maintaining the security of applications and data on the AWS platform. Customers need to implement strong authentication mechanisms, regularly review and update access policies, and ensure that the principle of least privilege is applied."
      },
      {
        "title": "Encryption of data in transit and at rest.",
        "status": "right",
        "explanation": "The responsibility for encrypting data in transit and at rest falls to the customer in the AWS Shared Responsibility Model. AWS provides the tools and services necessary to implement encryption, such as AWS Key Management Service (KMS) and AWS Certificate Manager, but it is up to the customer to implement these solutions. Encrypting data protects sensitive information from unauthorized access and ensures compliance with various regulatory standards."
      }
    ]
  },
  {
    "title": "A media agency has recently onboarded a team of graphic designers and wants to grant them read access to specific Amazon S3 buckets where design assets are stored. At the same time, it's required to restrict their access to other buckets that contain confidential client information. What's the optimal way to achieve this on AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Create an S3 bucket policy that allows public read access and attach it to the specific buckets.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create an IAM policy granting read access to the specific S3 buckets and a deny statement for all other buckets, and then attach this policy to the designer's IAM group.",
        "status": "right",
        "explanation": "AWS IAM policy defines permissions and dictates what actions are allowed or denied on specific AWS resources. These policies are JSON-formatted documents that specify the actions, resources, and effect (Allow or Deny). By attaching policies to users, groups, or roles, AWS administrators can grant or restrict access to services and resources, ensuring fine-grained control over the AWS environment and bolstering security by adhering to the principle of least privilege. In this case, creating a single IAM policy that grants read access to the designated S3 buckets and explicitly denies access to all other buckets ensures a centralized and manageable approach. Associating this policy with an IAM group, and then adding the designers to this group, streamlines the process, ensuring every member inherits the necessary permissions without individual configurations."
      },
      {
        "title": "Implement an S3 Transfer Acceleration to speed up the access for the designers.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create a new IAM user for each designer and manually assign permissions for every S3 bucket.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A global digital media company is launching a website on AWS targeting worldwide users. Which AWS deployment strategy should be implemented to ensure global content delivery with the lowest possible latency?",
    "type": "radio",
    "options": [
      {
        "title": "Use Amazon S3 Transfer Acceleration for content upload.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Deploy application stack in multiple Availability Zones.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Deploy the application in a single AWS region.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon Route 53 with latency-based routing.",
        "status": "right",
        "explanation": "Amazon Route 53 is a scalable and highly available Domain Name System (DNS) web service. With its latency-based routing, it routes end users to the nearest AWS endpoint based on perceived latency, thereby serving global users from the closest and most optimal location, ensuring low latency."
      }
    ]
  },
  {
    "title": "What approaches are included in the \"6 R's\" migration strategy for migrating applications to AWS Cloud?",
    "type": "radio",
    "options": [
      {
        "title": "Rehost, Replicate, Refactor, Replatform, Retain, and Review",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Rehost , Replatform, Repurchase, Refactor, Retire and Retain",
        "status": "right",
        "explanation": "The \"6 R's\" migration strategy in AWS refers to six approaches for migrating applications to the cloud: Rehost (lift and shift), Replatform (lift, tinker, and shift), Repurchase (drop and shop), Refactor (re-architect), Retire, and Retain. Each R represents a different method tailored to specific application needs and goals during migration.Rehost: Also known as \"lift and shift,\" this approach involves moving applications to the cloud without making any significant changes to their architecture. It's often the quickest and simplest method, suitable for applications that don't require significant modifications and need to be migrated rapidly. Rehosting reduces the time and effort required for migration, making it a preferred choice for many organizations.Replatform: Also known as \"lift, tinker, and shift,\" this approach involves making some adjustments to the application architecture to optimize it for the cloud environment. While not as extensive as refactoring, replatforming may involve updating the operating system, middleware, or database to versions compatible with the cloud. It allows for better performance, scalability, and cost optimization compared to rehosting, without requiring a complete overhaul of the application. Repurchase: Also known as \"drop and shop,\" this approach involves replacing an existing application with a similar one available as a service. For example, instead of hosting an email server on-premises, an organization might choose to use a cloud-based email service like Amazon WorkMail. Repurchasing can provide benefits such as reduced maintenance overhead, increased scalability, and access to advanced features offered by cloud services. Refactor: Also known as \"re-architect,\" this approach involves redesigning and restructuring the application to take full advantage of cloud-native features and capabilities. It often leads to improved performance, scalability, and cost efficiency compared to traditional on-premises deployments. Refactoring may involve breaking monolithic applications into microservices, adopting serverless architectures, or using managed services provided by the cloud provider.Retire: This involves decommissioning applications or services that are no longer needed or relevant. It helps streamline the migration process by reducing complexity and eliminating unnecessary resources and costs. Retiring legacy applications frees up resources that can be allocated to more critical workloads or initiatives. Retain: This approach involves keeping certain applications or components on-premises due to regulatory requirements, performance considerations, or other constraints. It allows organizations to maintain hybrid environments and gradually migrate workloads to the cloud at their own pace, ensuring minimal disruption to operations. These six approaches offer a comprehensive strategy for migrating applications to the AWS Cloud, enabling organizations to optimize performance, scalability, and cost efficiency while minimizing disruption and risk."
      },
      {
        "title": "Repurchase, Rehost, Refactor, Resize, Reiterate, and Retire",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Retry, Refactor, Resize, Repurchase, Rehost, and Replatform",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to enhance its application scalability and resilience by migrating to a microservices approach for its next-generation software suite. Which AWS service would be best suited to meet the requirement? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Amazon Chime",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon API Gateway",
        "status": "right",
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. In a microservices architecture, each function can be seen as an individual microservice. Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Together, they provide a powerful combo to build and expose microservices without the need to manage the underlying infrastructure. AWS Lambda handles the execution, and API Gateway exposes these as HTTPS endpoints."
      },
      {
        "title": "Amazon EC2 Spot Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "right",
        "explanation": ""
      },
      {
        "title": "Amazon Redshift Spectrum",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company operates an application deployed on EC2 instances across multiple Availability Zones. The application is built on a disaster recovery strategy. They are concerned about overspending and want to cut unnecessary costs. What action may reduce Amazon EBS costs?",
    "type": "radio",
    "options": [
      {
        "title": "Delete unnecessary snapshots",
        "status": "right",
        "explanation": "Deleting unnecessary snapshots can significantly reduce Amazon EBS costs. Snapshots are incremental backups of EBS volumes stored in Amazon S3. While snapshots are useful for data recovery and backups, over time, unused or old snapshots can accumulate and incur substantial storage costs. Regularly reviewing and deleting snapshots that are no longer needed or have been superseded can help minimize these expenses. Additionally, implementing lifecycle policies for snapshots can automate this process and ensure that only relevant snapshots are retained for disaster recovery purposes."
      },
      {
        "title": "Increasing the EBS volume size",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use standard magnetic volume",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Increase snapshot frequency",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service should be used to identify and resolve operational issues in a short period across multiple AWS resources?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Systems Manager",
        "status": "right",
        "explanation": "AWS Systems Manager allows you to view and control your AWS resources. It helps to identify and resolve operational issues quickly across multiple AWS resources, thereby simplifying operational tasks and reducing the time it takes to resolve them. With AWS Systems Manager, you can group resources, like Amazon EC2 instances or Amazon S3 buckets, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. AWS Systems Manager helps you to maintain security and compliance by scanning your instances against your patch, configuration, and custom policies."
      }
    ]
  },
  {
    "title": "In terms of pricing, what is the difference between S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier storage class?",
    "type": "radio",
    "options": [
      {
        "title": "S3 Standard-IA has a lower storage cost but a higher retrieval cost, while S3 Glacier has a higher storage cost but a lower retrieval cost.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "S3 Standard-IA has a higher storage cost but a lower retrieval cost, while S3 Glacier has a lower storage cost but a higher retrieval cost.",
        "status": "right",
        "explanation": "Amazon S3 offers a range of storage classes designed for different use cases. S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier are two such storage classes. S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. On the other hand, S3 Glacier is designed for long-term archiving of data that is accessed very infrequently. In terms of pricing, S3 Standard-IA has a higher storage cost than S3 Glacier because it is designed to provide fast, millisecond latency access to data. However, the retrieval cost of S3 Standard-IA is lower than that of S3 Glacier. S3 Glacier, while being a low-cost storage solution for long-term archiving, has higher costs for data retrieval, especially if the data needs to be accessed quickly. This pricing model reflects the intended use cases of these storage classes: S3 Standard-IA for infrequently accessed data that still requires rapid access, and S3 Glacier for rarely accessed data where retrieval speed is not a high priority."
      },
      {
        "title": "S3 Standard-IA and S3 Glacier have the same storage cost, but S3 Glacier has a higher retrieval cost.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "S3 Standard-IA and S3 Glacier have the same retrieval cost, but S3 Glacier has a lower storage cost.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company manages an application deployed on Amazon EC2 instances across multiple Availability Zones and uses Amazon S3 to store media files. They need to convert these media files for mobile device playback. Which AWS services should they use to meet these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Elastic Transcoder",
        "status": "right",
        "explanation": "Amazon Elastic Transcoder is a scalable media transcoding service in AWS that converts audio and video files into formats suitable for various devices. It supports a wide range of input and output formats, codecs, and resolutions, making it versatile for different media processing needs. With its pay-as-you-go pricing model, users can efficiently manage transcoding costs based on usage. Elastic Transcoder integrates seamlessly with other AWS services, offering flexibility and scalability to handle transcoding tasks from small to large scale applications, including streaming services, media distribution, and content management systems. By using Amazon Elastic Transcoder, the company can automate the media conversion process, ensuring that files stored in Amazon S3 are converted into compatible formats for mobile playback. The service integrates seamlessly with Amazon S3 for input and output storage, and it provides numerous presets to handle various formats and devices."
      },
      {
        "title": "Amazon Poly",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Kinesis Video Streams",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Transcribe",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "How does AWS IAM Identity Center (AWS Single Sign-On) enhance security and user management within an AWS environment?",
    "type": "radio",
    "options": [
      {
        "title": "AWS IAM Identity Center reduces costs by automatically scaling down permissions when not in use.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS IAM Identity Center automates the encryption of data at rest and in transit for all AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS IAM Identity Center offers detailed logging of data access which can be used for data lifecycle management.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS IAM Identity Center centralizes user access to multiple AWS accounts and applications with a single credential.",
        "status": "right",
        "explanation": "AWS IAM Identity Center (AWS SSO) simplifies the management of access to multiple AWS accounts and business applications by allowing users to sign in with a single set of credentials. This service enables administrators to easily manage user access at scale and improve security by eliminating the need for multiple passwords. It also supports automatic user provisioning, which further simplifies the process and ensures users have access only to the resources they need for their job functions."
      }
    ]
  },
  {
    "title": "Which AWS service allows you to define and provision infrastructure as code that enables automated and consistent deployment across different environments?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CodePipeline",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "right",
        "explanation": "AWS CloudFormation simplifies infrastructure management by enabling the creation and provisioning of AWS resources using declarative templates. These templates, written in JSON or YAML, define the desired state of the infrastructure, including EC2 instances, databases, networking components, and more. You create a template that describes all the AWS resources you want, and CloudFormation takes care of provisioning and configuring those resources for you. This service ensures consistency across different environments, meaning you can create identical copies of environments for development, testing, and production. It also supports automated rollbacks if errors are detected during deployment, enhancing reliability and reducing risk."
      },
      {
        "title": "AWS OpsWorks",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Elastic Beanstalk",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS Support plan offers architectural guidance to your specific use-cases?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Enterprise Support",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Basic Support",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Business Support",
        "status": "right",
        "explanation": "AWS Business Support provides detailed architectural guidance contextual to your use-cases. This service includes a range of features like architecture reviews, use-case guidance, and best practices. AWS-certified experts help assess your application architecture and offer recommendations aligned with AWSâ€™s best practices. This plan is well-suited for organizations that require a higher level of support than the Developer plan but do not need all the features of the Enterprise plan."
      },
      {
        "title": "AWS Developer Support",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An organization must log, store, and access all actions on its AWS resources to meet compliance. Which AWS service would be best suited for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Access Logs",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudTrail",
        "status": "right",
        "explanation": "AWS CloudTrail provides the event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history can be used to review actions taken on your account and to track changes to your AWS resources. CloudTrail captures detailed event information including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. CloudTrail logs can be analyzed, stored, and archived to meet compliance requirements. The logs can be directly integrated with Amazon S3 for storage, making them retrievable for any future audits or investigations."
      }
    ]
  },
  {
    "title": "A company is developing a nearly real-time financial trading application using AWS Lambda that must process transactions with single-digit millisecond latency (<10ms) while maintaining high availability and scale during unpredictable traffic spikes. Which AWS service is best suited for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Aurora with Provisioned Capacity Mode",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Redshift with Concurrency Scaling",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon DynamoDB with On-Demand Capacity Mode",
        "status": "right",
        "explanation": "Amazon DynamoDB is a highly scalable, low-latency NoSQL database service that is well-suited for nearly real-time applications. Its on-demand capacity mode automatically adjusts to handle sudden and unpredictable increases in traffic without requiring any manual intervention, making it ideal for use cases where spikes in traffic are common. DynamoDB is designed to deliver single-digit millisecond performance at any scale, which aligns with the requirement of processing transactions with minimal latency (<10ms). Furthermore, DynamoDB's high availability architecture ensures that it can handle high-scale workloads while providing consistent performance. Its managed nature means developers can focus on application logic rather than database management, and its built-in replication mechanisms enhance data durability and availability. Thus, DynamoDB with on-demand capacity mode is the best choice for ensuring low-latency, highly available, and scalable performance for the financial trading application."
      },
      {
        "title": "Amazon RDS with Multi-AZ deployment",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A business is planning to move its operations to AWS. It wants to ensure that the system can recover automatically in the event of a system failure. Which AWS Well-Architected Framework principle covers this need?",
    "type": "radio",
    "options": [
      {
        "title": "Performance efficiency",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reliability",
        "status": "right",
        "explanation": "The Reliability pillar focuses on the ability to recover from infrastructural or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues. An AWS system designed with reliability in mind would include automated recovery from failure, ensuring that any disruptions are quickly rectified without requiring manual intervention. This allows businesses to provide a consistent level of service, maintaining customer trust and business continuity, both critical aspects of modern cloud-based solutions."
      },
      {
        "title": "Cost optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational excellence",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A healthcare organization plans to store patient records in Amazon S3. To comply with regulatory standards, the organization must ensure that data is encrypted when stored in an S3 bucket. Which option should be implemented to maintain encryption?",
    "type": "radio",
    "options": [
      {
        "title": "Implement S3 Lifecycle policiesImplement S3 Lifecycle policies",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enable Server-Side Encryption (SSE)",
        "status": "right",
        "explanation": "Amazon S3 Server-Side Encryption (SSE) handles all the encryption, decryption, and key management in a transparent manner. There are multiple ways to use SSE with S3: SSE-S3: Uses keys that Amazon S3 manages. SSE-KMS: Uses AWS Key Management Service (KMS) which provides additional audit and policy-based controls. SSE-C: Where the customer provides the encryption keys. By using SSE, the organization ensures that the stored data is encrypted at rest. When an object is put into the bucket, Amazon S3 will automatically encrypt it. When the object is retrieved, Amazon S3 will decrypt it and send the decrypted object."
      },
      {
        "title": "Configure S3 Cross-Region Replication",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use S3 Transfer Acceleration",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following services does Amazon Route 53 provide? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Domain registration",
        "status": "right",
        "explanation": "Amazon Route 53 is a scalable Domain Name System (DNS) web service designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications. One of the key services it provides is domain registration, allowing you to register new domain names directly with the service, and it supports a wide variety of domain extensions."
      },
      {
        "title": "DNS management",
        "status": "right",
        "explanation": "Another feature of Amazon Route 53 is DNS management. It enables you to translate domain names into the numeric IP addresses that computers use to connect to each other. This is a part of how the internet functions, as it allows users to use easy-to-remember domain names instead of trying to remember numeric IP addresses."
      },
      {
        "title": "Access policy configuration",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Block unauthorized traffic",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Users & Group management",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },

  {
    "title": "Which statement best describes the concept of regions in the AWS cloud?",
    "type": "radio",
    "options": [
      {
        "title": "An AWS Region is a single, massive data center where all AWS services are hosted.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An AWS Region is a physical location consists of one Availability Zone.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An AWS Region is a physical point to deliver static contents with fast and low latency.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An AWS Region is a geographical location with a collection of Availability Zones.",
        "status": "right",
        "explanation": "AWS defines a region as a geographical area, which consists of two or more availability zones. Each region is entirely independent and is designed to be isolated from the other regions. This helps achieve the greatest possible fault tolerance and stability. Each AWS Region is designed to be isolated from the other AWS Regions. This achieves the greatest possible fault tolerance and stability. This design also helps to reduce inter-region latency. Availability Zones within a region are connected through low-latency links. An Availability Zone usually contains one or more data centers, each with redundant power, networking, and cooling. Each AZ is designed to be resilient to any issues in another AZ. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location."
      }
    ]
  },
  {
    "title": "A company plans to move to the AWS cloud to rapidly deploy new features by using various AWS services as needed. What benefits will the company gain from this move?",
    "type": "radio",
    "options": [
      {
        "title": "Cost Savings",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Scalability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Agility",
        "status": "right",
        "explanation": "Agility refers to the ability to rapidly and efficiently develop, test, deploy, and scale applications and services. AWS provides a wide range of cloud computing services, including computing power, storage options, and databases, which can be quickly provisioned and deprovisioned as needed. This flexibility allows organizations to innovate faster, respond to market changes, and optimize costs. Additionally, AWS's global infrastructure ensures low latency and high availability, supporting business continuity and enabling teams to focus on delivering value rather than managing infrastructure."
      },
      {
        "title": "Elasticity",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company deploys a web application in private subnets within a single VPC. The application servers need to fetch updates from another server without exposed to the internet. What is the most secure method to enable internet access for the application servers?",
    "type": "radio",
    "options": [
      {
        "title": "Use a VPN connection between the application servers and the patch server.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use a NAT Gateway in the private subnet.",
        "status": "right",
        "explanation": "AWS NAT Gateway is a managed service that allows instances in private subnets to access the internet while keeping them secure by filtering inbound traffic. It provides high availability, automatically scaling to meet demand without user intervention. NAT Gateway simplifies outbound-only internet connectivity. It's crucial for scenarios where private subnet resources require secure internet access, leveraging AWS infrastructure for reliability and scalability in handling outbound traffic from AWS resources."
      },
      {
        "title": "Place a NAT instance in the private subnet.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Assign public IP addresses to the servers.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What should you do if you receive a notification that your AWS account has been compromised? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Delete any potentially unauthorized IAM users, and then change the password for all other IAM users.",
        "status": "right",
        "explanation": "You should also review your IAM users for any potentially unauthorized accounts. If you identify any suspicious or unauthorized IAM users, promptly delete them. Additionally, change the passwords for all other IAM users to ensure that unauthorized individuals can no longer access your resources. This helps protect your account from further unauthorized access and secures your IAM user credentials."
      },
      {
        "title": "Change the email address associated with the AWS account.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Delete all running services and create again what you need.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enable multi-factor authentication (MFA) on the root user with Hardware MFA device.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Delete any resources on your account that you didn't create, such as Amazon EC2 instances and AMIs.",
        "status": "right",
        "explanation": "Another important action to take is to thoroughly review your account for any unauthorized resources. If you find any resources that you didn't create, such as EC2 instances, AMIs, or other services, it is crucial to delete them immediately. Removing unauthorized resources helps eliminate potential vulnerabilities and stops any ongoing malicious activities."
      }
    ]
  },
  {
    "title": "Which of the following affects the price of an EC2 instance? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "The Availability Zone",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security Group",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Instance type",
        "status": "right",
        "explanation": "The choice of Amazon EC2 instance type is a major factor affecting the cost. Instance types determine the hardware of the host computer used for the instance. Each type offers different compute, memory, and storage capabilities and are grouped in instance families based on these capabilities. Therefore, the more powerful and resource-rich the instance type, the higher the cost will be."
      },
      {
        "title": "Number of private IP",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Storage capacity",
        "status": "right",
        "explanation": "The cost of an EC2 instance is also influenced by the amount of storage capacity used. Amazon EC2 provides a variety of storage options that you can attach to your instances. Depending on the storage option used (e.g., EBS volumes, instance storage), the cost can increase as you add more storage capacity. For example, EBS volume pricing is based on the amount of storage provisioned and consumed."
      }
    ]
  },
  {
    "title": "A company uses AWS cloud to manage its operations efficiently and continuously works to improve its processes. Which AWS Well-Architected Framework principle best describes this use case?",
    "type": "radio",
    "options": [
      {
        "title": "Cost optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance efficiency",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational excellence",
        "status": "right",
        "explanation": "The operational excellence pillar provides guidance on running and managing systems to deliver business value. It focuses on automating processes, continuous improvement, and monitoring operations to achieve operational resilience, efficiency, and effectiveness. It includes practices like defining processes, using automation, and implementing metrics and monitoring for operational success. In this case, the company has not only designed its AWS Cloud infrastructure to efficiently manage its operations but also emphasizes continual process improvement. This clearly indicates that they are operating in alignment with the practices and concepts defined in the Operational Excellence pillar."
      },
      {
        "title": "Security",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are the most cost-effective options when using EC2 instances? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "On-Demand Instances for sustained workloads",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Set spending limit using AWS Budgets",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reserved Instances for sustained workloads",
        "status": "right",
        "explanation": "Amazon EC2 Reserved Instances provide a significant discount (up to 75%) compared to On-Demand instance pricing and offer a capacity reservation when used in a specific Availability Zone. They're a good choice for workloads with steady-state usage, predictable usage patterns, or long term commitments, making them a cost-effective choice for many types of applications, especially ones with predictable workloads."
      },
      {
        "title": "Spot Instances for stateless and flexible workloads",
        "status": "right",
        "explanation": "Amazon EC2 Spot Instances allow you to use unused Amazon EC2 computing capacity at up to a 90% discount compared to On-Demand prices. Spot instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. They're ideal for various fault-tolerant, flexible workloads, and are one of the most cost-effective ways to use EC2."
      },
      {
        "title": "Memory optimized instances for high-compute workloads",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service supports CD (Continuous Delivery) that automates the release process whenever you change code?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CodeDeploy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeBuild",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeBuild",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodePipeline",
        "status": "right",
        "explanation": "AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy stages of your release process every time there is a code change, based on the release model you define. This enables you to rapidly and reliably deliver features and updates, making it the suitable choice for continuous delivery that automates the release process when code is changed."
      },
      {
        "title": "AWS CodeCommit",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A business wants to manage its deployed IT services and govern infrastructure as code (IaC) templates. Which AWS service would meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Organizations",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Systems Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Resource Explorer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Service Catalog",
        "status": "right",
        "explanation": "AWS Service Catalog enables organizations to create and manage catalogs of IT services for use on AWS. This service allows administrators to govern the usage of these services, ensuring that everything deployed in the AWS environment meets the company's compliance and management standards. With AWS Service Catalog, customers can manage their infrastructure as code (IaC) templates, making it easier to deploy and manage resources consistently across a wide range of services. It provides a centralized location where specific versions of products can be made available, with configurations that adhere to the company's policies, thereby streamlining the deployment process and enforcing governance."
      }
    ]
  },
  {
    "title": "Which statement is correct about AWS Shield?",
    "type": "radio",
    "options": [
      {
        "title": "Continuously monitors AWS accounts and workloads for malicious activity.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Provide a firewall that protects from compromised web attacks like bots.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Provide a protection service for Distributed Denial of Service (DDoS) attacks.",
        "status": "right",
        "explanation": "AWS Shield provides a protection service for Distributed Denial of Service (DDoS) attacks. AWS Shield is a managed Distributed Denial of Service (DDoS) protection service. It provides advanced safeguards to protect AWS resources, such as EC2 instances, Elastic Load Balancers, and Amazon CloudFront distributions, from DDoS attacks. AWS Shield helps detect and mitigate volumetric, state-exhaustion, and application-layer DDoS attacks, ensuring the availability and performance of applications and services running in the AWS cloud. It offers both Standard and Advanced tiers, with AWS Shield Advanced providing additional DDoS protection and enhanced features for more complex scenarios."
      },
      {
        "title": "Detects and provides safeguards from SQL injection or cross-site scripting.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service can be used to evaluate the configurations for your AWS resources?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Systems Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Config",
        "status": "right",
        "explanation": "AWS Config continuously monitors and records the configuration changes of your AWS resources. It provides a detailed view of the configuration state of your resources and allows you to assess compliance against desired configurations and predefined rules. With AWS Config, you can define custom or pre-configured rules to evaluate resource configurations and check for compliance with industry standards and best practices. AWS Config can also be integrated with AWS Lambda to automate remediation actions when non-compliant resources are detected, enabling you to automatically correct configuration drift and maintain compliance."
      }
    ]
  },
  {
    "title": "What is the key principle of the Reliability pillar of the AWS Well-Architected Framework?",
    "type": "radio",
    "options": [
      {
        "title": "Implement automation to reduce manual tasks",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Implement monitoring and logging to detect and diagnose problems",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Implement automated backup and disaster recovery processes",
        "status": "right",
        "explanation": "The reliability principle emphasizes the importance of implementing automated backup and recovery processes to minimize the impact of outages and ensure that your systems can recover quickly from disruptions. This includes regularly backing up your data and configurations and implementing automated processes for restoring your systems in a disaster. By doing this, you can improve reliability."
      },
      {
        "title": "Use optimization techniques to improve system performance",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Under the AWS Shared Responsibility Model, which task falls under the customer's responsibility ?",
    "type": "radio",
    "options": [
      {
        "title": "Applying appropriate security levels of assets stored in the AWS environment.",
        "status": "right",
        "explanation": "Customers are responsible for the security of the data they store in the AWS environment, which includes applying appropriate security levels to their assets. This encompasses a wide range of actions such as encrypting data at rest and in transit, managing access controls using AWS Identity and Access Management (IAM), and ensuring that the data is only accessible to authorized users. AWS provides the tools and services to facilitate these security measures, but it is the customer's responsibility to implement them."
      },
      {
        "title": "Selecting the Amazon EC2 instance type to execute the AWS Lambda function.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Choosing specific Availability Zones for deploying Amazon S3 storage.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Applying software patches or performing upgrades to Amazon DynamoDB.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which service should be used to create interactive graph applications using popular open-source APIs such as Gremlin?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon ElastiCache",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Neptune",
        "status": "right",
        "explanation": "Amazon Neptune is a fully managed graph database service that makes it easy to build and run applications working with highly connected datasets. It's built for storing billions of relationships and querying the graph with milliseconds latency. Neptune supports popular graph models Property Graph and RDF, and their respective query languages Apache TinkerPop Gremlin and SPARQL, allowing you to create interactive graph applications using familiar open-source APIs like Gremlin. So, for creating interactive graph applications with Gremlin API, Amazon Neptune would be the right choice."
      },
      {
        "title": "Amazon Aurora",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Redshift",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An organization is planning to migrate its infrastructure to the AWS Cloud. As a part of the financial analysis, they want to identify key aspects of AWS Cloud economics. Which statement is true about the economic benefits of using AWS Cloud?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cloud automatically increases the costs as you scale out.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cloud eliminates the need to guess about future capacity needs.",
        "status": "right",
        "explanation": "One of the key aspects of AWS Cloud economics is the elimination of the need to guess about future capacity requirements. AWS provides scalable resources that can be adjusted according to the fluctuating needs of your business. This means you can start with the resources you need and then scale up or down based on demand, ensuring cost-efficiency and better resource management."
      },
      {
        "title": "AWS Cloud requires high upfront costs for infrastructure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cloud charges for the services even when they are not being used.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to the shared responsibility, which of the following are AWS's responsibilities for using Amazon RDS? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Building the relational schema",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Encrypting data at rest",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Managing Operating System",
        "status": "right",
        "explanation": "AWS manages the underlying operating system for Amazon RDS instances. It is responsible for tasks such as installing, configuring, and maintaining the operating system patches and updates. AWS ensures that the operating system running on the RDS instances is secure and up to date, relieving customers of this responsibility."
      },
      {
        "title": "Patching Database regularly",
        "status": "right",
        "explanation": "AWS is responsible for patching the underlying infrastructure and ensuring that Amazon RDS (Relational Database Service) is up to date with the latest security patches. This includes patching the database engine and addressing vulnerabilities to protect the underlying infrastructure."
      },
      {
        "title": "Configuring IAM Permission",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service allows you to store, manage, and deploy container images?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Simple Storage Service (S3)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic Kubernetes Service (EKS)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic Compute Cloud (EC2)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic Container Registry (ECR)",
        "status": "right",
        "explanation": "Amazon Elastic Container Registry (ECR) is a fully managed container image registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow, making it a go-to service for storing, managing, and deploying container images."
      }
    ]
  },
  {
    "title": "Which AWS service provides recent events to help you manage active events and shows proactive notifications to plan for scheduled activities?",
    "type": "radio",
    "options": [
      {
        "title": "AWS OpsWorks",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Organizations",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Personal Health Dashboard",
        "status": "right",
        "explanation": "The AWS Personal Health Dashboard provides recent events to help you manage active events and shows proactive notifications to plan for scheduled activities. It gives you a personalized view into the performance and availability of the AWS services you are using. The Personal Health Dashboard provides real-time information on service health events that may be impacting your resources. It notifies you of any ongoing issues, scheduled maintenance, or other events that might require your attention. The Personal Health Dashboard allows you to view the status of AWS services in your account, access detailed information about events and their impacts, and receive notifications via email or the AWS Management Console. It helps you stay informed about the health of your AWS services, enabling you to plan and manage your resources effectively"
      }
    ]
  },
  {
    "title": "What is the best practice for the Performance Efficiency pillar of the AWS Well-Architected Framework?",
    "type": "radio",
    "options": [
      {
        "title": "Use synchronous communication between microservices to minimize latency.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use the right type and size of resources for your workload.",
        "status": "right",
        "explanation": "It is also a best practice because it ensures that the system uses the appropriate resources for the specific workload. By using the correct type and size of resources, the system can achieve optimal performance and reduce unnecessary costs. For example, using a resource that is too small for a workload can cause performance issues, while using a resource that is too large can be wasteful and result in unnecessary costs."
      },
      {
        "title": "Use the largest instance size available to ensure maximum performance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use relational databases for all workloads to ensure data consistency.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service provides a dedicated physical device to store and protect cryptographic keys used for data encryption?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CloudHSM",
        "status": "right",
        "explanation": "AWS CloudHSM (Hardware Security Module) provides a dedicated physical device to store and protect cryptographic keys used for data encryption. It offers secure key storage and cryptographic operations within a tamper-resistant hardware module. CloudHSM provides FIPS 140-2 Level 3 validated hardware security modules and allows you to generate, store, and manage cryptographic keys securely. With CloudHSM, you have full control over your keys and can use them for various encryption purposes, such as securing sensitive data at rest or in transit. CloudHSM integrates with other AWS services and offers a highly secure and scalable solution for key management. By using CloudHSM, you can meet regulatory and compliance requirements while maintaining control over your encryption keys."
      },
      {
        "title": "AWS Secrets Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Key Management Service (KMS)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Macie",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An organization is looking to optimize its storage costs on AWS for a mix of frequently and infrequently accessed data, requiring immediate access when needed. Which AWS storage services and features would best meet these requirements at the lowest cost?",
    "type": "radio",
    "options": [
      {
        "title": "Use Amazon FSx",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon EBS Provisioned IOPS SSD (io1)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon S3 Glacier Deep Archive",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon S3 Intelligent-Tiering",
        "status": "right",
        "explanation": "Amazon S3 Intelligent-Tiering is a storage class designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It's suitable for data with unknown or changing access patterns. S3 Intelligent-Tiering monitors access patterns and moves data between two access tiers â€“ one for frequently accessed data and another for infrequently accessed data. This feature ensures that data is stored in the most cost-efficient manner, reducing storage costs without requiring manual intervention to analyze access patterns. For organizations with a mix of frequently and infrequently accessed data requiring immediate access when needed, S3 Intelligent-Tiering provides a flexible and cost-effective solution."
      }
    ]
  },
  {
    "title": "Which AWS service can be used to store and manage secrets such as database credentials or API keys with strong encryption?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Directory Service",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Secrets Manager",
        "status": "right",
        "explanation": "Secrets Manager provides a secure and centralized location for storing and managing secrets such as database credentials or API keys. It encrypts the secrets using AWS Key Management Service (KMS) and allows you to define fine-grained access controls for secrets based on IAM policies. With Secrets Manager, you can easily retrieve secrets programmatically, either directly through the AWS SDKs or by integrating with other AWS services such as Amazon RDS or Amazon DocumentDB. AWS Secrets Manager also provides automatic rotation capabilities for secrets, which can help enhance security by automatically generating and updating credentials at specified intervals."
      }
    ]
  },
  {
    "title": "Which factor contributes to the agility of AWS cloud computing?",
    "type": "radio",
    "options": [
      {
        "title": "The rapid deployment of AWS services and resources",
        "status": "right",
        "explanation": "Agility refers to the ability to quickly deploy and manage resources, enabling businesses to respond rapidly to their needs and market changes. This aspect of agility allows organizations to launch new applications, scale up or down based on demand, and experiment with new ideas without the lengthy procurement and setup times associated with traditional IT infrastructure. The cloud's on-demand nature means that resources like compute instances, storage, and databases can be provisioned within minutes, offering a level of flexibility and speed that is critical for businesses looking to maintain a competitive edge."
      },
      {
        "title": "The low cost required to start using cloud services",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The speed at which AWS introduces new geographic regions",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reducing unused computing resources",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Reducing unused computing resources",
    "type": "radio",
    "options": [
      {
        "title": "AWS CodeCommit",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodeDeploy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CodePipeline",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cloud9",
        "status": "right",
        "explanation": "AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes pre-packaged with essential tools for popular programming languages and the environment is customizable to individual coding styles. Since it's cloud-based, you can work on your projects from your office, home, or anywhere using an internet-connected machine."
      }
    ]
  },
  {
    "title": "A business wants to use AWS Batch to run various concurrent stateless simulations that are fault tolerant and run up to 3 hours. What pricing model should be used to optimize costs and align these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Reserved Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "On-Demand Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Dedicated Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Spot Instances",
        "status": "right",
        "explanation": "Spot Instances allow customers to purchase unused AWS capacity at reduced rates, offering significant cost savings compared to On-Demand pricing. For workloads like the stateless simulations described, which are fault-tolerant and can handle interruptions, Spot Instances are an ideal choice. AWS Batch is designed to efficiently manage and run batch computing workloads, and when combined with Spot Instances, it can dynamically allocate resources based on availability and cost preferences. This flexibility makes Spot Instances perfect for running large-scale simulations that do not require continuous compute power, allowing the company to optimize their cloud expenditure significantly while still meeting their operational requirements."
      }
    ]
  },
  {
    "title": "What is the best practice of the AWS Cloud architecture?",
    "type": "radio",
    "options": [
      {
        "title": "Build for monolithic architectures",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use tightly coupled services",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use loosely coupled communication",
        "status": "right",
        "explanation": "Using loosely coupled communication is another best practice in AWS Cloud architecture. Loosely coupled systems decouple components or services from each other, reducing dependencies and allowing for independent development, scaling, and fault isolation. AWS provides various services, such as Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS), which enable loosely coupled communication patterns. These services decouple the sender and receiver, allowing them to operate independently and ensuring greater flexibility, scalability, and resilience in your architecture."
      },
      {
        "title": "Deploy into a single Availability Zone",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following features is free for all AWS support plans?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Personal Health Dashboard",
        "status": "right",
        "explanation": "The AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact your account. It gives you a personalized view of the performance and availability of the AWS services underlying your AWS resources. It is freely available to all AWS customers."
      },
      {
        "title": "AWS Support API",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Infrastructure Event Management",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Phone, email, and chat access",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS storage service provides file storage that is accessible by multiple EC2 instances with automatic scaling, high availability, and performance?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Simple Storage Service (S3)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic Block Store (EBS)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic File System (EFS)",
        "status": "right",
        "explanation": "Amazon Elastic File System (EFS) is a scalable file storage solution for use with Amazon EC2 instances. It is designed to provide massively parallel shared access to thousands of Amazon EC2 instances, allowing your applications to achieve high levels of aggregate throughput and IOPS with low and consistent latencies. This makes EFS a great solution for applications and workflows that require shared access to file data, need data persistence, and high availability. With EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files, thus eliminating the need to provision and manage capacity to accommodate growth."
      },
      {
        "title": "Amazon Storage Gateway",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A student needs a pre-configured virtual private server for his graduation project. Which AWS service should he use for a lower monthly cost?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon ECS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Elastic Beanstalk",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Lightsail",
        "status": "right",
        "explanation": "Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server. It provides developers compute, storage, and networking capacity and capabilities to deploy and manage websites, web applications, and databases in the cloud. Lightsail includes everything you need for your project at a low, predictable price, making it an excellent choice for students or small projects where cost is a significant concern."
      }
    ]
  },
  {
    "title": "A business stores its data in an Amazon S3 bucket and wants to enhance security. A client needs encrypted data before sending it to Amazon S3. Which of the following methods should be used to meet the requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Use client-side encryption by using the AWS Encryption SDK.",
        "status": "right",
        "explanation": "The AWS Encryption SDK simplifies the task of encrypting data on the client side before uploading it to S3. With client-side encryption, the client retains control over the encryption process and the encryption keys, enhancing the security of the data. The AWS Encryption SDK provides an easy-to-use interface for incorporating encryption into client applications, ensuring that sensitive data is protected before it leaves the customer's environment. This method ensures end-to-end encryption, as the data remains encrypted during transmission to and storage in Amazon S3."
      },
      {
        "title": "Configure Amazon S3 Bucket Policies to enforce HTTPS connections.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enable Amazon S3 Server-Side Encryption with Customer-Provided Keys.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use AWS Key Management Service (KMS) for serverside encryption.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A business wants to move its operations to the AWS Cloud. It wants to avoid the challenge of estimating infrastructure capacity needs before deployment and only pay for what it uses. Which benefit of the AWS Cloud aligns with this goal?",
    "type": "radio",
    "options": [
      {
        "title": "Global reach",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reliability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Pay-per-use pricing",
        "status": "right",
        "explanation": "The pay-per-use or pay-as-you-go pricing model allows businesses to pay only for the cloud resources they consume, effectively eliminating the need to predict infrastructure capacity before deployments. It brings flexibility and cost-effectiveness, as business can scale their infrastructure up or down based on their needs, and they only pay for what they use."
      },
      {
        "title": "Economies of scale",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following should be used to achieve high availability in a multi-tier web application on AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Deploying the applications across multiple Availability Zones with Amazon RDS Multi-AZ deployments.",
        "status": "right",
        "explanation": "For a multi-tier web application, deploying across multiple Availability Zones is a key strategy for achieving high availability. This approach ensures that if one Availability Zone becomes unavailable, the application can still operate from another zone. Using Amazon RDS (Relational Database Service) with Multi-AZ deployments for databases adds further resilience. With Multi-AZ RDS, there's an automatic failover to a standby instance in a different Availability Zone if the primary instance fails, ensuring continuous database operation and data availability."
      },
      {
        "title": "Consolidating all applications on a single Amazon EC2 instance for centralized management.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Using a single Elastic Load Balancer (ELB) for all the traffic regardless of the applications.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Deploying the applications to a single AWS Region to simplify network configurations.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement is true according to Amazon RDS Multi-AZ deployment?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon RDS replicates data in a synchronous way to different AZ.",
        "status": "right",
        "explanation": "Amazon RDS replicates data in a synchronous way to different Availability Zones (AZ). When you enable Multi-AZ deployment for your RDS instance, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different AZ. This replica is continuously updated with the latest changes from the primary database instance. The synchronous replication ensures that data is replicated to the standby replica in near real-time, providing high availability and data durability. In the event of a primary database failure, Amazon RDS automatically fails over to the standby replica, minimizing downtime and ensuring continuity of your database operations."
      },
      {
        "title": "Amazon RDS asynchronously creates replicates to different AZ.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS does not support replicates for multi-AZ.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS creates replicates in both synchronous and asynchronous ways.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to the AWS Shared Responsibility Model, which of the following is the responsibility of customers?",
    "type": "radio",
    "options": [
      {
        "title": "Configuring infrastructure devices",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Physical and Environmental controls",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Patching the Network Infrastructure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Network & firewall configuration",
        "status": "right",
        "explanation": "According to the AWS Shared Responsibility Model, customers are responsible for configuring and managing network and firewall settings. This includes defining and implementing appropriate network access controls, firewall rules, routing configurations, and network segmentation. Customers have the flexibility to configure their network infrastructure based on their specific requirements and security policies."
      }
    ]
  },
  {
    "title": "According to the AWS Shared Responsibility Model, customers are responsible for configuring and managing network and firewall settings. This includes defining and implementing appropriate network access controls, firewall rules, routing configurations, and network segmentation. Customers have the flexibility to configure their network infrastructure based on their specific requirements and security policies.",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Route 53",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Auto Scaling",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Elastic Load Balancing",
        "status": "right",
        "explanation": "Elastic Load Balancing (ELB) effortlessly handles incoming network traffic distribution across multiple EC2 instances. This service is designed to increase the application's availability by automatically distributing incoming traffic across multiple targets such as EC2 instances, and containers. If one instance fails or becomes overloaded, ELB will reroute the traffic to another instance that's capable of handling the load, maintaining the application's performance and reducing the risk of downtime. Hence, for an application that is hosted on multiple EC2 instances and requires efficient traffic distribution, ELB would be the appropriate solution."
      },
      {
        "title": "Amazon Cloudfront",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Your company has multiple EC2 instances and sometimes they fail. You want to receive email notifications when an EC2 instance is down. Which service should be used to meet this need?",
    "type": "radio",
    "options": [
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudWatch",
        "status": "right",
        "explanation": "CloudWatch provides monitoring and observability for your AWS resources, including EC2 instances. You can set up CloudWatch alarms to monitor the status of your EC2 instances and trigger actions when specific conditions are met. In this case, you can create an alarm to notify you by email when an EC2 instance goes into the \"stopped_\" or \"unreachable\" state. By configuring the appropriate alarm thresholds and actions, you can receive timely notifications about the health and availability of your EC2 instances. CloudWatch also provides detailed metrics and logs for monitoring and troubleshooting purposes, helping you ensure the reliability and performance of your EC2 instances."
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are customer responsibilities in the AWS Cloud according to the AWS Shared Responsibility Model? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Configuring IAM policies to manage access to AWS resources.",
        "status": "right",
        "explanation": "In the AWS Shared Responsibility Model, the customer is responsible for security configuration and management tasks for the AWS services they use. This includes managing and configuring the Identity and Access Management (IAM) policies. IAM allows customers to manage access to AWS services and resources securely. By setting up IAM policies, customers define who is authenticated and authorized to use resources, which is vital for maintaining the security of their AWS environment."
      },
      {
        "title": "Encrypting sensitive data before uploading it to Amazon S3.",
        "status": "right",
        "explanation": "Customers are responsible for the protection of their data in the cloud. This includes encryption of sensitive data before it is uploaded to Amazon Simple Storage Service (S3) and managing the encryption keys, whether they use AWS-managed keys or bring their own keys (BYOK). While AWS provides the tools to encrypt data, it is the customer's responsibility to implement these measures to protect their data at rest."
      },
      {
        "title": "Ensuring the environmental control systems within AWS data centers.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Managing the underlay network infrastructure connecting AWS regions.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Installing physical security measures at AWS data center facilities",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An organization wants to use the AWS Route 53 routing policy to route traffic to a primary endpoint. They also need to automatically switch to a secondary endpoint when the primary endpoint returns specific HTTP status codes or specific patterns are detected in the response. Which options would you suggest for this scenario?",
    "type": "radio",
    "options": [
      {
        "title": "Latency routing with HTTP status checks",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Geolocation routing with patterns checks",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Weighted routing with patterns checks",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Failover routing with health checks",
        "status": "right",
        "explanation": "Failover Routing with Health Checks in Amazon Route 53 is designed to meet the exact requirements of the described scenario. In a failover routing policy, Route 53 will route traffic to a primary target. If health checks detect an issue, such as specific HTTP status codes or detected patterns in the response, Route 53 will automatically route to a secondary target. This ensures high availability and fault tolerance, which seems to be the main concern for the organization in this scenario."
      }
    ]
  },
  {
    "title": "Your company wants to move to the AWS cloud. Which of the following benefits will you get? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Pay only when and how much you consume.",
        "status": "right",
        "explanation": "Another significant benefit of migrating to the AWS cloud is the pay-as-you-go pricing model. Companies only pay for the resources they consume, whether it's compute instances, storage, network usage, or other services. This eliminates the need for large upfront investments in infrastructure and allows companies to align their costs with their actual usage. Additionally, AWS provides cost management tools and services that help optimize spending and control costs by monitoring resource utilization and identifying areas for optimization."
      },
      {
        "title": "Have access to resources when you need them.",
        "status": "right",
        "explanation": "Moving to the AWS cloud provides companies with the benefit of having access to resources when they need them. AWS offers a vast array of services and resources that can be provisioned on-demand. This means that companies can quickly scale up or down their infrastructure and services based on their current requirements. They can easily launch new instances, provision storage, set up databases, and deploy various other services with just a few clicks or API calls. This flexibility allows companies to adapt to changing workloads and handle sudden increases or decreases in demand without upfront investments in hardware or lengthy procurement processes."
      },
      {
        "title": "Can reduce company's promotions costs.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Can reduce application development costs.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "In and out both data transfers are free.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which architectural concepts can scale resources based on user traffic or data volume without compromising performance?",
    "type": "radio",
    "options": [
      {
        "title": "Design for agility",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Design for failure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Implement elasticity",
        "status": "right",
        "explanation": "Elasticity is the ability of an architecture to automatically and seamlessly scale up or down in response to changing demands, such as user traffic or data volume, without compromising performance. This is achieved through the use of cloud computing resources and tools that provide automated scaling capabilities based on pre-defined rules or triggers. Elasticity ensures that the architecture can handle varying workloads without requiring manual intervention or compromising performance."
      },
      {
        "title": "Decouple components",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service should be used to set spending limits and receive alerts when costs exceed a certain threshold?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Simple Monthly Calculator",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cost Explorer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Budgets",
        "status": "right",
        "explanation": "AWS Budgets gives you the ability to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. AWS Budgets uses your cost and usage data from AWS Cost Explorer to provide you with a comprehensive look at your spending patterns. You can set spending limits for different AWS services, accounts, tags, and more, which will help you manage costs and keep them within your desired threshold."
      },
      {
        "title": "AWS Billing",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service recommends following best practices to improve security and performance?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Artifact",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "right",
        "explanation": "AWS Trusted Advisor provides the recommendation to improve security and performance. It analyzes your AWS environment and provides real-time guidance based on AWS best practices and architectural recommendations. Trusted Advisor evaluates various aspects of your AWS account, including cost optimization, security, performance, and fault tolerance, and provides actionable recommendations to help you optimize your resources and enhance the overall security and performance of your applications and infrastructure. Trusted Advisor offers insights and suggestions to improve resource utilization, cost efficiency, security configurations, and overall operational excellence. It provides proactive notifications and ongoing monitoring to ensure that you adhere to AWS best practices, helping you identify potential security vulnerabilities, performance bottlenecks, or cost-saving opportunities."
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "According to AWS cloud architecture principles, which best practice can improve the elasticity of an application?",
    "type": "radio",
    "options": [
      {
        "title": "Deploy workload in multiple Regions to provide a low latency experience.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Automatically Adjust the required resources based on demand changes.",
        "status": "right",
        "explanation": "According to AWS cloud architecture principles, automatically adjusting the required resources based on demand changes is a best practice to improve the elasticity of an application. Elasticity refers to the ability of an application or system to dynamically scale resources up or down based on demand. By automatically adjusting resources, such as compute instances or storage capacity, in response to changes in workload or user demand, an application can efficiently and effectively utilize resources and ensure optimal performance. This approach enables the application to handle fluctuations in traffic or workload without manual intervention, providing scalability and responsiveness."
      },
      {
        "title": "Limit Scaling AWS resources to fully utilize resources and reduce cost.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reduce interdependency between application components.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which authentication method can be used when using Amazon IAM from CLI?",
    "type": "radio",
    "options": [
      {
        "title": "Access key",
        "status": "right",
        "explanation": "An Access Key in the context of Amazon Web Services (AWS) is a component of the credentials used to authenticate and authorize API requests to AWS services. It comprises two parts: an Access Key ID and a Secret Access Key. The Access Key ID is a unique identifier for the credentials, while the Secret Access Key is a secret token used in conjunction with the Access Key ID for signing requests securely. These keys are essential for programmatic access to AWS resources, allowing users and applications to interact with AWS services securely via the AWS SDKs, the AWS Command Line Interface (CLI), or direct API calls. Access Keys are typically associated with an IAM (Identity and Access Management) user or role, dictating the permissions and access levels for the entities that use them."
      },
      {
        "title": "RSA Key",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS KMS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Route Key",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which service is used to analyze data in Amazon S3 using standard SQL?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Athena",
        "status": "right",
        "explanation": "Amazon Athena is used to analyze data in Amazon S3 using standard SQL. It allows you to run interactive queries on data stored in S3 without the need for infrastructure provisioning or data loading. With Athena, you can directly query structured, semi-structured, and unstructured data in S3 using SQL syntax. Athena provides results quickly, scales automatically to handle large datasets, and charges you only for the amount of data scanned by your queries."
      },
      {
        "title": "Amazon Redshift",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon FinSpace",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statements are true regarding the root user? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "The root user has unlimited privileges and should only be used for initial account setup and maintenance tasks.",
        "status": "right",
        "explanation": "The root user has complete access and control over all AWS resources associated with the account. It is recommended that the root user should be only used for initial account setup and maintenance tasks. It should not be used for everyday tasks in the AWS account."
      },
      {
        "title": "The root user can be deleted and recreated as needed.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The root user should be used when launching an EC2 instance in the account.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It is recommended to use multi-factor authentication (MFA) for the root user to increase security.",
        "status": "right",
        "explanation": "Since the root user has complete access to the AWS account, it is essential to secure the root user's account by enabling MFA. MFA adds an extra layer of security to the user's login process and reduces the chances of unauthorized access to the account."
      },
      {
        "title": "The root user has the same access permissions as any other IAM user in the account.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the benefit of creating snapshots of Amazon EBS volumes to back up data?",
    "type": "radio",
    "options": [
      {
        "title": "Elasticity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Flexibility",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Scalability",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Durability",
        "status": "right",
        "explanation": "Creating snapshots of Amazon Elastic Block Store (EBS) volumes provides the benefit of durability. When you create an EBS snapshot, AWS stores it in Amazon S3, which is designed for durability. Amazon S3 replicates data across multiple availability zones within a region, ensuring that your snapshots are highly durable and protected against data loss. In case of failures or errors, you can rely on the durability of snapshots to recover your data and restore your EBS volumes to a previous state."
      }
    ]
  },
  {
    "title": "Your company wants to migrate infrastructure to the AWS cloud. Which service helps you to identify the right solutions you need?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Simple Monthly Calculator",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Partner Network (APN)",
        "status": "right",
        "explanation": "The AWS Partner Network (APN) is the global partner program for Amazon Web Services. It focuses on assisting AWS Partner organizations in building successful AWS-based businesses or solutions by providing them with business, technical, marketing, and go-to-market support. If a company is looking to migrate its infrastructure to the AWS cloud, leveraging the expertise and offerings from APN partners can provide valuable insights and tailored solutions. APN partners are knowledgeable about AWS services and best practices, having gained a proven track record in delivering successful AWS solutions. Therefore, by collaborating with APN, companies can better identify the most suitable AWS services and architectures for their unique needs."
      },
      {
        "title": "AWS Marketplace",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An e-commerce startup uses AWS for its projects They reconfigured the AWS infrastructure to release a new feature last month. They noticed that the AWS bill was significantly higher than expected this month. Which of the following could be a possible reason for this increased bill?",
    "type": "radio",
    "options": [
      {
        "title": "They configure several AWS CloudFormation stacks that were created for new infrastructure setups.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "They use multiple AWS Lambda functions that were executed more frequently.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "They keep unused elastic IPs that are not associated with any Resources.",
        "status": "right",
        "explanation": "Amazon Elastic IP (EIP) offers a static IPv4 address for dynamic cloud computing needs. It allows remapping to different instances within your Virtual Private Cloud (VPC), ensuring flexibility and continuity during instance changes. Regardless of whether the public IPv4 address is actively associated with an AWS resource or remains idle within your AWS account, you're charged an hourly rate for each IP address. This pricing model applies equally, ensuring fair treatment for both active and inactive IP addresses."
      },
      {
        "title": "They have set overly permissive S3 bucket policies.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is considering migrating its on-premises infrastructure to AWS. As part of their assessment, they want to know how cloud economics will impact their costs. Which statement is true regarding the economic advantages of using AWS?",
    "type": "radio",
    "options": [
      {
        "title": "AWS offers a pay-as-you-go model that can reduce upfront capital expenses.",
        "status": "right",
        "explanation": "AWS's pay-as-you-go model allows companies to pay only for the computing resources they consume. This pricing model shifts capital expenditure (CapEx) to operational expenditure (OpEx), providing the flexibility to scale up or down based on demand. It helps companies avoid large upfront costs associated with purchasing and managing physical servers and data center infrastructure. By moving to the cloud, businesses can convert fixed costs into variable costs, which can lead to significant cost savings, especially when the demand is unpredictable."
      },
      {
        "title": "Using dedicated instances on AWS is always more cost-effective than on-premises solutions.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Migrating to AWS will automatically eliminate the need for compliance and licensing costs.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "On AWS, the operational costs are fixed regardless of the company's usage patterns.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following is a feature of Amazon EC2 that allows users to launch instances in multiple Availability Zones and manage them as a single logical unit?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon EC2 Spot Instances",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 Placement Groups",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 Auto Scaling",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 Fleet",
        "status": "right",
        "explanation": "Amazon EC2 Fleet is a feature that simplifies the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, Availability Zones, and purchase models (On-Demand, Reserved, and Spot Instances) in a single API call. This service is designed to maintain the high availability of applications in the face of unpredictable demand by deploying instances in multiple Availability Zones and managing them as a single logical unit. This way, EC2 Fleet allows users to optimize their cost and performance, while ensuring capacity is balanced across the specified Availability Zones."
      }
    ]
  },
  {
    "title": "Which statement is true about AWS Auto Scaling?",
    "type": "radio",
    "options": [
      {
        "title": "Auto Scaling deploys AWS Shield when a DDoS attack is detected.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Auto Scaling can automatically remove unhealthy instances.",
        "status": "right",
        "explanation": "AWS Auto Scaling monitors applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. It has the ability to automatically remove unhealthy instances and replace them with new ones. This helps ensure that your application is receiving the required computing resources at all times. AWS Auto Scaling can automatically adjust the capacity of your AWS resources based on predefined scaling policies. It enables you to automatically scale your applications and infrastructure in response to changing demand, ensuring optimal performance, cost-efficiency, and high availability. These policies can be based on various metrics such as CPU utilization, network traffic, or custom application metrics. Auto Scaling continuously monitors these metrics and makes scaling decisions accordingly."
      },
      {
        "title": "An Auto Scaling group can be used to span multiple regions.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "An Auto Scaling group cannot be configured to scale automatically.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which support plan should you choose if you want to start a startup?",
    "type": "radio",
    "options": [
      {
        "title": "Basic Support Plan",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enterprise Support Plan",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Developer Support Plan",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Business Support Plan",
        "status": "right",
        "explanation": "For a startup, the Business support plan would be an appropriate choice. This plan offers 24/7 access to Cloud Support Engineers via email, chat, and phone. It also provides a response time of less than one hour for business-critical system downtime. Further, it includes Infrastructure Event Management, which can provide architectural and scaling guidance for planned events. This plan strikes a good balance between cost and comprehensive support, which is usually ideal for startups that may need more support than what's provided by lower-tier plans, without incurring the cost of the Enterprise plan."
      }
    ]
  },
  {
    "title": "A company is running an application on EC2 instances. Which design principle ensures that the application performs well even if an EC2 instance is unavailable?",
    "type": "radio",
    "options": [
      {
        "title": "Increase capacity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Low latency",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Fault Tolerance",
        "status": "right",
        "explanation": "Fault tolerance ensures that the application remains operational and continues to deliver its services even in the presence of failures or disruptions. By implementing fault tolerance, the application is designed to withstand the failure of individual EC2 instances without experiencing significant downtime or performance degradation. With fault tolerance, the application is typically deployed across multiple EC2 instances, forming a redundant and resilient architecture. If one instance fails, the workload seamlessly shifts to other available instances, ensuring uninterrupted service delivery. This approach reduces the risk of a single point of failure and provides high availability."
      },
      {
        "title": "Load Balancing",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statement accurately describes the capabilities of the AWS Pricing Calculator?",
    "type": "radio",
    "options": [
      {
        "title": "It provides detailed technical specifications.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It can estimate the cost of AWS services.",
        "status": "right",
        "explanation": "The AWS Pricing Calculator is designed to help users estimate the cost of AWS services based on a variety of inputs. This feature is particularly useful for organizations looking to forecast their AWS costs based on past consumption patterns. By inputting historical usage data, users can get a more accurate estimate of future costs, helping in budget planning and cost optimization. This capability is crucial for cloud financial management and aligns with the AWS cost optimization pillar, which emphasizes the importance of understanding and controlling where money is being spent."
      },
      {
        "title": "It can automatically adjust resource allocation.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "It provides direct billing and invoice management.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A large company wants to send a bulk marketing email to customers. Which service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Connect",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Pinpoint",
        "status": "right",
        "explanation": "Amazon Pinpoint is a scalable and flexible service that allows you to engage with your customers through multiple channels, including email, SMS, push notifications, and more. It provides features specifically designed for marketing communications, such as targeted campaigns, personalized messaging, and analytics. With Amazon Pinpoint, you can create and manage customer segments based on specific criteria, ensuring that your marketing emails reach the right audience. It offers templates and tools for designing visually appealing emails and supports advanced features like A/B testing and campaign analytics to measure the effectiveness of your marketing efforts. Pinpoint also integrates with other AWS services and third-party platforms, making it a comprehensive solution for managing and optimizing your marketing communications."
      },
      {
        "title": "AWS Fargate",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SQS",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Who is responsible for applying patches to the underlying operating system for Amazon Aurora?",
    "type": "radio",
    "options": [
      {
        "title": "Who is responsible for applying patches to the underlying operating system for Amazon Aurora?",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The database administrator (DBA) for your organization.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The application development team within your company.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The AWS Product Team manages this automatically.",
        "status": "right",
        "explanation": "Amazon Aurora is a fully managed relational database service. One of the key benefits of using Amazon Aurora is that the AWS Product Team takes on the responsibility of managing the underlying infrastructure, including the operating system. This includes tasks such as applying patches, updating the OS, and ensuring the servers are secure and up to date. This allows organizations to focus on their core business and application logic while AWS handles the heavy lifting of infrastructure maintenance. By handling these administrative tasks automatically, AWS ensures that your Aurora database instances are always running on the latest, most secure, and stable environments, which is crucial for maintaining optimal performance and security."
      }
    ]
  },
  {
    "title": "The company wants to develop an automated system that processes documents as soon as they are uploaded to AWS storage services. They prefer a solution that requires no server management and offers auto-scaling features. Which AWS service would be the best choice for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon RDS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2 Instances with Auto Scaling Groups",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "right",
        "explanation": "AWS Lambda is a serverless computing service that allows developers to run code in response to specific events without provisioning or managing servers. With Lambda, you only pay for the compute time you consume, making it cost-effective. The service automatically scales applications by running code in response to each trigger, handling various tasks simultaneously. Users can set up their code to automatically trigger from other AWS services or call it directly from any web or mobile app. Lambda supports multiple programming languages, including Python, Node.js, Java, and C#. It integrates with AWS services for logging, monitoring, and advanced configurations."
      },
      {
        "title": "AWS Batch",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },

  {
    "title": "What is the difference between pricing for data transfer IN and OUT on AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Data transfer IN and data transfer OUT are both charged per GB.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Data transfer IN is charged per GB and data transfer OUT is charged per Mbps.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Data transfer IN is charged per Mbps and data transfer OUT is charged per GB.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Data transfer IN is free and data transfer OUT is charged per GB.",
        "status": "right",
        "explanation": "AWS charges for data transfer in different ways based on the direction of the transfer. For data transfer \"IN\" to AWS, it is free of charge, meaning that you can transfer data into your AWS resources from the internet without any costs. This includes data transfer into EC2 instances, S3 buckets, and other AWS services. On the other hand, data transfer \"OUT\" from AWS is charged on a per-GB basis. The charges can vary depending on the destination (e.g., the internet, another AWS region, or another AWS service in the same region) and the volume of data transferred. For example, there is usually a higher charge for data transfer to the internet compared to transferring data between AWS services within the same region."
      }
    ]
  },
  {
    "title": "A software development company has developed an application that requires a shared storage solution accessible to multiple Amazon EC2 instances simultaneously. Additionally, they want to ensure that infrequently used files are moved to a more cost-effective storage class. Which AWS services or features best address these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon FSx for Windows File Server",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 with S3 Standard-IA storage class",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EFS with Lifecycle Management",
        "status": "right",
        "explanation": "Amazon Elastic File System (EFS) is a scalable, elastic, cloud-native file storage service for Linux-based workloads. It seamlessly integrates with AWS cloud services and on-premise resources, providing a simple, serverless, set-and-forget elastic file system. Amazon EFS is designed to be highly available and durable, offering a file system interface and file system semantics. It allows multiple EC2 instances to access the data concurrently. EFS Lifecycle Management is a feature within Amazon EFS that automates the moving of files not accessed according to the lifecycle policy from the standard storage class to the cost-effective EFS Infrequent Access (EFS IA) storage class. This feature helps in optimizing storage costs by automatically transitioning older, less frequently accessed files to a lower-cost storage tier, while still ensuring that these files are accessible without any application modifications. Lifecycle Management policies can be easily configured to suit different access patterns and retention requirements, making it an effective tool for saving storage costs without sacrificing data availability or performance."
      },
      {
        "title": "Amazon Glacier",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service/feature enables a secure private data transfer between two AWS accounts?",
    "type": "radio",
    "options": [
      {
        "title": "Virtual Private Network (VPN)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "VPC Peering",
        "status": "right",
        "explanation": "VPC Peering allows you to create a direct network route between two Virtual Private Clouds (VPCs) across regions, enabling private communication without bandwidth bottlenecks or data traveling the public internet. This service is particularly suitable for inter-account communication, as it facilitates secure, low-latency, and high-bandwidth data transfer. VPC Peering connections are established between VPCs using private IP addresses, using the robust security features inherent to VPCs, such as security groups and network ACLs. This ensures that data remains within the AWS network, providing an additional layer of security."
      },
      {
        "title": "Network Load Balancer (NLB)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Direct Connect",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service sends a notification whenever the software configuration changes on an EC2 instance?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Config",
        "status": "right",
        "explanation": "AWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. It can send a notification whenever a change is detected in the software configuration of an EC2 instance, making it a powerful tool for tracking configuration changes and ensuring compliance with your organization's rules and guidelines."
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has an application running on an Amazon EC2 instance and needs to improve application security for inbound and outbound traffic. What should be used to add security to an EC2 instance? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "IAM Policy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Internet Gateways",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IAM Access key",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security Groups",
        "status": "right",
        "explanation": "Security groups act as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance, you can associate one or more security groups with the instance. You add rules to each security group that allows traffic to or from its associated instances. These rules can be modified at any time, the changes are automatically applied to all instances that are associated with the security group."
      },
      {
        "title": "Network ACL",
        "status": "right",
        "explanation": "A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC."
      }
    ]
  },
  {
    "title": "According to the AWS Well-Architected Framework, what is the recommended practice for achieving cost optimization?",
    "type": "radio",
    "options": [
      {
        "title": "Encrypt all data at rest and in transit.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Purchase Reserved Instances for all workloads.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Right-size services to meet capacity demands.",
        "status": "right",
        "explanation": "Right-sizing is a core practice within the Cost Optimization pillar of the AWS Well-Architected Framework. It involves continually matching the types and sizes of your computing resources to your workload's requirements at the lowest cost. This includes analyzing the demand and not over-provisioning resources, thereby saving costs without sacrificing performance or capacity."
      },
      {
        "title": "Deploy a multi-region architecture.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to run a financial-based application in the AWS cloud. The application requires a database that supports relationships between records and complex queries. As a cloud practitioner, which database service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Neptune",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS",
        "status": "right",
        "explanation": "Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS supports several popular relational databases such as MySQL, PostgreSQL, Oracle, and SQL Server, which are widely used in financial applications due to their robustness and reliability."
      },
      {
        "title": "Amazon DynamoDB",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Redshift",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which statements are true regarding AWS Organizations and consolidated billing? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "All member accounts can share their reserved instances when needed.",
        "status": "right",
        "explanation": "AWS Organizations allows member accounts within the organization to share their reserved instances (RIs). This means that if one account has purchased RIs, other accounts within the same organization can use those reserved instances, provided that they are in the same availability zone and meet other applicable criteria. This sharing capability helps in maximizing the ROI on RIs purchased and ensures better utilization of resources across the organization. By pooling resources, it reduces the likelihood of underutilized RIs and enables cost savings."
      },
      {
        "title": "AWS Organizations allow member accounts to share AWS Support plans across the organization.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Organizations offer a 10% discount on all services for member accounts.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Consolidated billing combines incurred costs across accounts into a single invoice.",
        "status": "right",
        "explanation": "Consolidated billing is a feature of AWS Organizations that allows users to combine the billing for multiple AWS accounts into a single invoice. This makes it easier to manage and track the costs across various departments or projects within a company. By consolidating the billing, organizations can also benefit from volume pricing discounts, which are applied across all account usage. Each account still operates independently but the financial benefits and consolidated tracking can help streamline financial management and cost optimization efforts."
      },
      {
        "title": "AWS Organizations limit linked accounts to a maximum of 20.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which pillar of the AWS Well-Architected Framework emphasizes operating and monitoring systems to deliver business value and continually improve processes and procedures?",
    "type": "radio",
    "options": [
      {
        "title": "Security",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Cost Optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Performance Efficiency",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Operational Excellence",
        "status": "right",
        "explanation": "Operational Excellence emphasizes the importance of operating and monitoring systems to deliver business value and continually improve processes and procedures. It includes best practices for managing and automating changes, responding to events, and defining standards to manage daily operations. This pillar ensures that systems are running efficiently and are designed to evolve as needs change, helping businesses to operate at peak efficiency while maintaining the ability to innovate and improve continuously."
      }
    ]
  },
  {
    "title": "An organization needs to enhance the scalability of its legacy application by breaking it into smaller, distributed services hosted on AWS. What migration approach should they follow?",
    "type": "radio",
    "options": [
      {
        "title": "Refactor",
        "status": "right",
        "explanation": "Refactoring, also known as re-architecting, involves rewriting significant portions of the application to better leverage cloud-native services and architectures. This approach is best in scenarios where an application needs to be broken down into smaller, more manageable, and scalable distributed services. By refactoring the legacy application, the organization can take advantage of modern cloud services such as microservices, serverless computing, managed databases, and more, which significantly enhances the scalability and agility of the application. Refactoring allows the application to be more resilient, easier to manage, and capable of handling increased loads seamlessly."
      },
      {
        "title": "Replatform",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Repurchase",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Rehost",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Why is the serverless services more cost-effective than server-based services?",
    "type": "radio",
    "options": [
      {
        "title": "Serverless is a global service for high performance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Serverless services require less maintenance and administration.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The resource is only used when the code is executed.",
        "status": "right",
        "explanation": "Another key aspect that makes serverless services more cost-effective is the pay-for-execution pricing model, where you are charged only for the computing time you consume. This model contrasts with traditional server-based services where you pay for server instances and computing resources regardless of whether they are actively being used. In a serverless environment, if your code isn't running, you're not being charged. This approach is particularly cost-effective for applications with variable traffic or for workloads that do not need to run 24/7. By paying only for the actual compute time used, businesses can significantly reduce their operational costs, making serverless services an attractive option for cost-conscious organizations."
      },
      {
        "title": "Caching has applied automatically for low latency.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which activity is specifically prohibited by the AWS Acceptable Use Policy?",
    "type": "radio",
    "options": [
      {
        "title": "Using AWS services to host a bulk messaging application for a small business.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Sharing AWS account information with unauthorized individuals or entities.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Using AWS services to send unsolicited emails, including spam or phishing emails.",
        "status": "right",
        "explanation": "The AWS Acceptable Use Policy outlines guidelines for the acceptable use of AWS services, prohibiting illegal or harmful activities such as malicious hacking, spamming, and distributing malware. It emphasizes compliance with laws and regulations and respecting the rights of others. Users are responsible for their content and actions on AWS, with AWS reserving the right to suspend or terminate accounts violating the policy. Additionally, AWS may report illegal activities to authorities and cooperate with investigations. AWS maintains a strict policy against the transmission of spam or other unsolicited emails via its services. AWS customers are required to follow all relevant email legislation, such as the CAN-SPAM Act in the United States. Moreover, AWS's policy includes provisions against the distribution of phishing emails or other types of deceptive messaging that attempt to collect sensitive information under false pretenses. This policy is in place to protect the integrity of AWS services and the safety of its customers."
      },
      {
        "title": "Using AWS services to store or process data that is subject to specific regulatory requirements, such as healthcare data or financial data.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the main difference between the AWS TCO Calculator and the AWS Pricing Calculator?",
    "type": "radio",
    "options": [
      {
        "title": "The AWS TCO Calculator helps estimate the cost of running an application on AWS, while the AWS Pricing Calculator helps estimate the cost of individual AWS services.",
        "status": "right",
        "explanation": "The AWS Total Cost of Ownership (TCO) Calculator is designed to assist users in quantifying the savings when migrating their IT infrastructure to AWS. It considers various factors, including server, storage, networking hardware, and data center operations costs, to provide an estimate of the cost of running applications on AWS versus on-premises or in a colocation environment. On the other hand, the AWS Pricing Calculator is more granular and focused on individual AWS services. It allows you to estimate the cost of using specific AWS services based on your expected usage, helping you make cost-effective choices when planning your AWS architecture."
      },
      {
        "title": "The AWS TCO Calculator helps estimate the cost of individual AWS services, while the AWS Pricing Calculator helps estimate the cost of running an application on AWS.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The AWS TCO Calculator provides a detailed breakdown of costs, while the AWS Pricing Calculator only provides a rough estimate.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The AWS TCO Calculator is free, while the AWS Pricing Calculator requires a paid AWS account.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service provides the current status of all AWS services in the AWS Global Infrastructure?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Personal Health Dashboard",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Service Health Dashboard",
        "status": "right",
        "explanation": "AWS Service Health Dashboard provides real-time information on the status of AWS services across different regions in the AWS Global Infrastructure. It offers a transparent view of any current operational issues with AWS services that might impact customer workloads. This allows customers to monitor the overall performance of the services they are using and identify any disruptions that may be affecting their applications."
      }
    ]
  },
  {
    "title": "A company wants to run an application on the AWS cloud. They need an estimation of the monthly bill based on AWS services. Which AWS service meets the requirements?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cost Explorer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Pricing Calculator",
        "status": "right",
        "explanation": "The AWS Pricing Calculator helps customers to estimate the cost of using AWS services. It provides a user-friendly interface that allows individuals to model and configure AWS services according to their specific requirements, including options for regions, services, and usage estimates. The calculator supports a wide range of AWS services, enabling users to project costs for computing, storage, database, and many other services. This service is particularly useful for understanding the financial implications of deploying, maintaining, and scaling applications on AWS, allowing for more informed decision-making."
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Billing",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service continuously monitors and protects your data stored in Amazon Simple Storage Service (Amazon S3) from malicious activity?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon GuardDuty",
        "status": "right",
        "explanation": "Amazon GuardDuty is a threat detection service that continuously monitors and analyzes data from various sources within AWS environments, such as data stored in S3, AWS CloudTrail logs, VPC Flow Logs, and DNS logs, to detect potential malicious activity and unauthorized behavior. GuardDuty uses machine learning algorithms and threat intelligence to analyze the collected data and identify common attack patterns, such as unauthorized access, privilege escalation, malware infections, and data exfiltration. It also provides insights into unusual API calls, unusual network traffic, and compromised instances."
      },
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS WAF",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Shield",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An e-commerce company wants to enable product searches through voice commands. Which AWS service would meet this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Pinpoint",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Transcribe",
        "status": "right",
        "explanation": "Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy for developers to add speech-to-text capability to their applications. It uses advanced machine learning technologies that can accurately transcribe audio files and live audio streams into text. It supports various audio formats and works with low-quality audio files, making it versatile for different use cases. This service offers features such as speaker identification, custom vocabulary, and real-time transcription, enhancing the accessibility and analysis of audio data. Amazon Transcribe is used across a wide range of applications, including customer service, transcription of meetings and conferences, and creating searchable archives of audio content. It supports multiple languages, making it a powerful tool for global businesses and developers looking to incorporate speech recognition into their solutions."
      },
      {
        "title": "Amazon SQS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Polly",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is preparing to launch a new web application that will use a PostgreSQL database. They aim to maintain high availability and enable automatic backups for the database while minimizing management efforts. Which solution would be the best recommendation?",
    "type": "radio",
    "options": [
      {
        "title": "Deploy PostgreSQL on Amazon Lightsail.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use Amazon RDS with PostgreSQL.",
        "status": "right",
        "explanation": "Amazon RDS (Relational Database Service) is a managed relational database service. It supports multiple database engines like MySQL, PostgreSQL, SQL Server, and others, handling routine database tasks such as provisioning, patching, backup, and scaling automatically. RDS simplifies database administration, allowing users to focus on application development. It offers features like high availability with Multi-AZ deployments, read replicas for scaling read operations, and security features such as encryption at rest and in transit. RDS is suitable for a wide range of applications that require scalable and reliable database solutions. Amazon RDS with PostgreSQL is an ideal solution for launching a web application with a PostgreSQL database while ensuring high availability and minimizing management efforts."
      },
      {
        "title": "Set up PostgreSQL on an on-premises server.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Install PostgreSQL on an Amazon EC2 instance.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service allows you to manage centralized operational data from multiple AWS services?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Systems Manager",
        "status": "right",
        "explanation": "AWS Systems Manager offers a unified user interface that allows you to view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. AWS Systems Manager provides a centralized store to manage your configuration data, whether plain-text data such as database strings or secrets such as passwords. This is beneficial because it allo"
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has approximately 80 Petabytes of data. Which service is the best option to transfer from an on-premises data center to the AWS cloud?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Snowmobile",
        "status": "right",
        "explanation": "AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. For a data transfer as large as 80 Petabytes, it is the most suitable choice among the options provided. Snowmobile can transfer up to 100 Petabytes of data per single data transfer job, making it efficient for moving massive volumes of data to the cloud. It addresses challenges of high network costs, long transfer times, and security concerns to migrate data as efficiently as possible. Snowmobile uses multiple layers of security designed to protect your data including dedicated security personnel, GPS tracking, alarm monitoring, 24/7 video surveillance, and an optional escort security vehicle while in transit."
      },
      {
        "title": "AWS Snowball",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "S3 Transfer Acceleration",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Snowcone",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which accurately describes the Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) of AWS cloud computing models?",
    "type": "radio",
    "options": [
      {
        "title": "PaaS provides raw compute, storage, and network connectivity, while IaaS offers pre-built database services and middleware.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "PaaS offerings include physical data centers on-premises that clients can use to develop applications.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IaaS allows clients full control over infrastructures, while PaaS abstracts the infrastructure away for developers to focus on code.",
        "status": "right",
        "explanation": "In the IaaS model, clients have full control over their virtualized infrastructure, including networks, virtual servers, and storage. This means they manage the operating system, applications, runtime, and data. Conversely, PaaS provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app."
      },
      {
        "title": "PaaS customers must manage operating systems, while IaaS customers do not have access to operating systems.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is planning to develop an application using a message broker service. Which AWS service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Step Functions",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon MQ",
        "status": "right",
        "explanation": "Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. It's ideal for application developers who are using open source message brokers and want a fully managed service that works seamlessly with their existing applications. It also simplifies migrating on-premises message brokers to the cloud, as you can use your existing code and messaging protocols like JMS and NMS."
      },
      {
        "title": "Amazon SQS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon SNS",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which AWS service should you use to encrypt data at rest for an Amazon RDS instance?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Key Management Service (KMS)",
        "status": "right",
        "explanation": "AWS Key Management Service (KMS) is a managed service that allows you to create and control the cryptographic keys used to encrypt your data. When you enable encryption for an Amazon RDS instance, KMS generates and manages the master keys that are used to secure your database. By integrating with AWS KMS, you can apply encryption at rest to protect sensitive and valuable data stored in RDS. KMS simplifies the process of encryption key management through features like automatic key rotation, audit logs via AWS CloudTrail, and fine-grained access control policies."
      },
      {
        "title": "AWS CloudTrail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Config",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A business has made its operations more efficient and cost-effective by choosing the right AWS resources. Which cost management strategy does this action represent?",
    "type": "radio",
    "options": [
      {
        "title": "Usage tracking",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Consolidated billing",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Budget alerts",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Architecture optimization",
        "status": "right",
        "explanation": "Optimizing architecture involves carefully selecting and configuring AWS services to meet specific business needs while also aiming to reduce costs and enhance efficiency. In this scenario, the company has fine-tuned its use of AWS services, which likely includes adopting serverless architectures, choosing the right database services, and scaling resources to match demand. This approach not only improves performance but also minimizes waste, leading to lower operational costs. Architecture optimization is a strategic way to ensure that resources are used effectively, aligning spending with actual needs and eliminating unnecessary expenses. It's a proactive measure that combines the principles of good design with cost-efficiency, showcasing a deep understanding of how AWS pricing works and how to leverage AWS offerings for optimal outcomes."
      }
    ]
  },
  {
    "title": "A tech startup wants to standardize its AWS setup process for different application environments and keep track of deployment versions for easy rollback and review. Which of the following AWS services best meets the requirement? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "AWS Elastic Beanstal",
        "status": "right",
        "explanation": "AWS Elastic Beanstalk is a Platform as a Service (PaaS) that allows developers to deploy and manage applications without dealing with the underlying infrastructure. It abstracts some of the infrastructure complexities, it also provides ways to customize the infrastructure using configuration files. This ensures consistent deployment across various environments. Versioning of application deployments is also a built-in feature, allowing for easy rollback and management."
      },
      {
        "title": "Amazon Athena",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon CloudWatch",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS CloudFormation",
        "status": "right",
        "explanation": "AWS CloudFormation allows users to define and deploy infrastructure as code (IaC). This means organizations can automate the provisioning of resources, ensuring that environments are consistently deployed. It uses templates, written in JSON or YAML, which describe the desired resources and their dependencies. Versioning becomes straightforward since these templates can be stored in version control systems like Git. This ensures that replicating environments, from development to production, or even across multiple regions, becomes a systematic and repeatable process."
      },
      {
        "title": "AWS Glue",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following best describes the concept of fault tolerance?",
    "type": "radio",
    "options": [
      {
        "title": "The ability of a system to adapt to changing workloads",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The ability of a system to operate without downtime",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The ability of a system to handle errors gracefully",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The ability of a system to detect and recover from faults",
        "status": "right",
        "explanation": "Fault tolerance refers to the ability of a system to continue operating, even in the presence of one or more faults or failures. A fault-tolerant system is designed to detect faults and recover from them without significant interruption to the service. These systems typically include redundancies and mechanisms for automatic failover to ensure service continuity in the event of faults."
      }
    ]
  },
  {
    "title": "What AWS services or features can be used to implement disaster recovery strategies for Amazon EC2 instances? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Amazon EC2 Auto Scaling",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon S3 Glacier",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "EC2 Amazon Machine Images (AMI)",
        "status": "right",
        "explanation": "An Amazon Machine Images (AMI) is a template used to create virtual machines (EC2 instances) in Amazon Web Services (AWS). It encapsulates the operating system, application server, and applications. AMIs enable easy replication of environments, simplifying deployment and scaling of infrastructure within AWS cloud computing services. An Amazon Machine Images (AMI) can be used for disaster recovery solutions. By creating the AMI of your instance, you capture the entire server's state, including the operating system, the installed software, and all the instance configurations. When you need to recover, you can launch new instances from this AMI in any region, ensuring that your applications can be quickly restored to their previous state or moved to a new region in response to geographical-specific disruptions."
      },
      {
        "title": "Amazon Elastic Block Store (Amazon EBS) snapshots",
        "status": "right",
        "explanation": "Amazon EBS snapshots offer a durable and secure way to back up the volumes of your EC2 instances at a specific point in time. These snapshots are stored in Amazon S3, which provides 99.999999999% durability, ensuring that your data is safe and recoverable even in the face of a disaster. Snapshots can be used to quickly restore EBS volumes and attach them to new or existing EC2 instances, facilitating a quick recovery process. The ability to automate the creation of EBS snapshots via Amazon Data Lifecycle Manager enhances the disaster recovery strategy by ensuring that backups are created at regular intervals without manual intervention, thus providing a reliable method for data recovery."
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which option should be used to get programmatic access to AWS services using the AWS Command Line Interface (CLI) or AWS Software Development Kit (SDK)?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Access keys",
        "status": "right",
        "explanation": "Access keys are part of the security credentials used to authenticate requests to AWS services. AWS Access Keys consist of two parts: an Access Key ID and a Secret Access Key, which are used to sign programmatic requests to the AWS services. They serve as a secure means of authentication when you are accessing AWS services through the AWS API, CLI, or SDKs. These keys are alternative to a username and password pair which allows AWS to identify the user or service making the request and to ensure that the requester has the appropriate permissions to perform the operation."
      },
      {
        "title": "Secure Shell (SSH) Public Keys",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Encryption Keys",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the best practice for granting an Amazon EC2 instance the required permissions to access an Amazon S3 bucket?",
    "type": "radio",
    "options": [
      {
        "title": "Modify the S3 bucket policy to allow unrestricted uploads from any AWS service at any time.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Embed IAM access and secret keys within the application's code for uploading files.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Store IAM access and secret keys on the EC2 instance for S3 bucket access and file uploads.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Assign an IAM Role to the EC2 instance and grant required permissions for S3 bucket file uploads.",
        "status": "right",
        "explanation": "This approach is highly recommended for several reasons. First, using IAM roles with EC2 instances to access S3 buckets is a best practice for managing AWS resources securely. IAM roles provide a way to grant permissions to AWS services without the need static access keys. When you attach an IAM role to an EC2 instance, the role's permissions are automatically applied to the instance, allowing applications running on it to use the role's credentials to make AWS API calls. This method is secure, efficient, and eliminates the risk associated with managing and rotating access keys manually. Moreover, IAM roles can be designed to offer fine-grained permissions tailored to the specific needs of the application, enhancing the security posture by adhering to the principle of least privilege."
      }
    ]
  },
  {
    "title": "A company wants to run an application that needs a high level of access to the underlying virtual infrastructure. As a cloud practitioner, which service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon Lightsail",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Lambda",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Fargate",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2",
        "status": "right",
        "explanation": "Amazon Elastic Compute Cloud (Amazon EC2) provides secure, resizable compute capacity in the cloud. EC2 provides the user with full control of their computing resources and allows them to run on Amazon's proven computing environment. With Amazon EC2, the company can gain the control and flexibility it needs for its application. Amazon EC2 provides a variety of instance types optimized to fit different use cases and gives users the ability to configure their own VMs. It allows direct access to the server and enables users to manage things like the operating system, security patches, applications, and network settings. This is why EC2 would be the best fit for an application that needs a high level of access to the underlying virtual infrastructure."
      }
    ]
  },
  {
    "title": "Which of the following is part of the value proposition of AWS Cloud?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cloud reduces the need for upfront capital expenditure on data centers and physical servers.",
        "status": "right",
        "explanation": "AWS Cloud's value proposition includes reducing the need for upfront capital expenditure on data centers and physical servers. This is achieved by providing a pay-as-you-go model, where users only pay for the computing resources they use. This eliminates the significant initial investment typically required for hardware and infrastructure, making it accessible for businesses of all sizes. Additionally, AWS Cloud offers scalability, allowing companies to adjust their resources according to demand without overprovisioning. This flexibility and cost efficiency are key components of AWS Cloud's appeal, enabling businesses to innovate and grow without the constraints of traditional IT infrastructure."
      },
      {
        "title": "AWS Cloud offers a fixed pricing model that reduces cost variability for businesses.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cloud requires customers to purchase separate licenses for operating systems and software.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Users must maintain hardware and perform software updates manually in the AWS Cloud.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is planning to expand its AWS usage significantly and requires advanced billing support to manage its increasing costs effectively. Which AWS features should they consider to ensure comprehensive billing support?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Billing Conductor",
        "status": "right",
        "explanation": "AWS Billing Conductor allows customers to customize and share detailed cost reports with showback and chargeback information. It provides advanced billing support by enabling enterprises to map their AWS usage and costs to their internal structures. This tool is especially useful for growing companies needing to allocate costs accurately across different departments or projects. It also offers the flexibility to create custom pricing for internal or external customers, helping companies understand and manage their AWS spending more effectively."
      },
      {
        "title": "AWS Free Tier",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Account Managers",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Connect",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has an application running on its own on-premise data center. They want to store data in the cloud to reduce costs. Which AWS service provides hybrid cloud storage that gives on-premises access with virtually unlimited storage?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon S3",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EC2",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon EFS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Storage Gateway",
        "status": "right",
        "explanation": "AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. It seamlessly integrates on-premises IT environments with cloud storage for backup and restore, archiving, disaster recovery, cloud data processing, storage tiering, and migration. It connects an on-premises software appliance with AWS cloud-based storage for seamless integration between your on-premises IT environment and AWS storage infrastructure."
      }
    ]
  },
  {
    "title": "Which statement is true when using AWS Web Application Firewall (AWS WAF)?",
    "type": "radio",
    "options": [
      {
        "title": "Continually scans AWS workloads for software vulnerabilities and unintended network exposure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Protects from common attacks like SQL injection and Cross-Site Scripting (XSS).",
        "status": "right",
        "explanation": "Web Application Firewall (AWS WAF) is a cloud-based security service that protects web applications from common web exploits. It allows users to create rules to filter web traffic based on IP addresses, HTTP headers, or custom conditions. With AWS WAF, you can mitigate threats such as SQL injection, cross-site scripting, and more, helping to safeguard your applications against attacks. It integrates seamlessly with other AWS services, providing scalable and customizable protection for your web applications hosted on AWS infrastructure."
      },
      {
        "title": "Provides detailed security recommendations for AWS accounts and workloads for malicious activity.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Provides protection against Distributed Denial of Service (DDoS) attacks for applications.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Regarding AWS billing and pricing, which statement is true?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Free Tier provides some resources for free but only for the first 12 months after account creation.",
        "status": "right",
        "explanation": "The AWS Free Tier allows new users to explore and use AWS services for free up to certain limits. It includes offers that are always free, 12 months free, and short-term trials. This tier enables experimenting with services like Amazon EC2, S3, and DynamoDB without upfront costs, making it ideal for learning and building initial projects on AWS. It is particularly beneficial for new customers looking to get started with AWS, allowing them to learn, experiment, and build solutions on AWS without upfront costs."
      },
      {
        "title": "Data transfer across AWS regions is always free of charge.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS provides a detailed billing report which can be integrated with third-party accounting software.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reserved Instances require upfront payment and don't have discounts over On-Demand pricing.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company wants to use some services of the AWS cloud. However, due to some legal issues, they need to run these services on their own data center. Which AWS service allows users to run native AWS services in on-premises data centers?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Outposts",
        "status": "right",
        "explanation": "AWS Outposts brings native AWS services, infrastructure, and APIs to on-premises data centers, co-location spaces, or edge locations. It extends the AWS cloud capabilities to customers' own environments, allowing them to securely run and manage applications using familiar AWS tools and services. Outposts consist of pre-configured racks of compute and storage equipment that are installed and managed by AWS, providing a consistent hybrid cloud experience. Customers can use the same APIs, services, and operational practices as in the AWS cloud to build, deploy, and manage applications on Outposts. It enables organizations to seamlessly integrate on-premises infrastructure with the AWS ecosystem for hybrid cloud deployments. With Outposts, you can use the same AWS APIs, tools, and security controls to run, manage, and secure your applications on-premises and in the cloud. Outposts can be used to support a variety of applications, including those that need to process and act on data locally in real-time, communicate with other on-premises systems, and meet data residency requirements."
      },
      {
        "title": "AWS VPN",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon VPC",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Direct Connect",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has 10 terabytes of data stored on Amazon S3 and intends to perform infrequent data analyses. Which AWS service ensures the most cost-efficient query execution?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Glue",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Redshift",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Athena",
        "status": "right",
        "explanation": "Amazon Athena is a serverless interactive query service that makes it easy to analyze data stored in Amazon S3 using standard SQL. It is the most cost-effective choice for companies like the one in question, which has a significant amount of data stored in S3 and needs to run occasional queries. With Athena, users are charged based on the amount of data scanned by the queries they run, making it highly cost-effective for workloads where queries are not constant or continuous. This pricing model allows for flexibility and cost savings, as there's no need for data loading or infrastructure management. Athena's serverless nature also means that there's no infrastructure to manage or set up, reducing operational costs and complexity. For occasional analysis tasks, Athena provides the simplicity and cost-efficiency needed, allowing the company to query their data without the overhead of managing a complex data warehouse or database system."
      }
    ]
  },
  {
    "title": "A company is deploying Amazon EC2 instances across multiple Availability Zones (AZs) to enhance high availability. Which pillar of AWS Well-Architected refers to this?",
    "type": "radio",
    "options": [
      {
        "title": "Operational Excellence",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Security",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Reliability",
        "status": "right",
        "explanation": "The Reliability pillar of the AWS Well-Architected Framework refers to the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues. By deploying Amazon EC2 instances across multiple Availability Zones (AZs), you're creating a setup that's resilient to the failure of a single location, thereby improving the reliability of your system. This ensures that your applications remain available and users have consistent access, aligning directly with the goals of the Reliability pillar."
      },
      {
        "title": "Performance Efficiency",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which factors should be included in a Total Cost of Ownership (TCO) analysis when considering a migration to AWS Cloud? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "The depreciation of capital expenditures over time",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The cost of training staff on AWS Cloud services",
        "status": "right",
        "explanation": "When calculating the TCO for a migration to AWS, it's important to consider the cost of training staff on AWS Cloud services. This ensures that the team can effectively manage and operate within the AWS ecosystem, using its full potential to achieve operational efficiency and cost savings. Training can lead to better resource management, improved security posture, and optimal application performance, all of which contribute to a more accurate TCO calculation."
      },
      {
        "title": "Cost of office space rental for IT staff",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Upfront investment in physical hardware for the data center",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Estimated operational costs for maintaining on-premises servers",
        "status": "right",
        "explanation": "Operational costs for maintaining on-premises servers should be a core component of TCO analysis. These costs include not only direct expenses such as power, cooling, and maintenance staff but also indirect costs like system administration, network management, and software licenses. When considering AWS Cloud, it's crucial to compare these ongoing costs to the variable, consumption-based pricing of AWS services to determine potential savings."
      }
    ]
  },
  {
    "title": "Which AWS service can be used to centrally manage security policies across multiple AWS accounts for AWS WAF, AWS Shield Advanced, and VPC security groups?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon InspectorAmazon Inspector",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Firewall Manager",
        "status": "right",
        "explanation": "AWS Firewall Manager is a security management service that centrally configures and manages firewall rules across multiple AWS accounts and resources. It automates the deployment and enforcement of security policies, ensuring consistent protection for applications deployed in AWS. Firewall Manager supports AWS WAF, AWS Shield Advanced, and VPC security groups, allowing administrators to create and apply rules based on security best practices. It simplifies compliance by providing a unified view of security policies and their enforcement status, thereby enhancing the overall security posture of AWS environments."
      },
      {
        "title": "AWS Shield Advanced",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Shield Standard",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "The pricing model of AWS Lambda depends on what factors?",
    "type": "radio",
    "options": [
      {
        "title": "Hourly rate fixed for the number of functions used.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Total duration of function execution.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Number of function requests and duration of execution.",
        "status": "right",
        "explanation": "AWS Lambda pricing is based on the number of requests (function invocations) and the duration of execution. The request cost is based on the total number of requests across all your functions. The duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. The price depends on the amount of memory you allocate to your function. You are charged for the total time that you consume, in increments of 1 ms."
      },
      {
        "title": "Amount of memory allocated to the function.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "How does AWS Cost Explorer assist in managing your expenses?",
    "type": "radio",
    "options": [
      {
        "title": "Prescriptive cost management advice",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Automated resource optimization",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Budget forecasting and cost visualization",
        "status": "right",
        "explanation": "AWS Cost Explorer provides graphical tools for managing and understanding AWS costs and usage. Through budget forecasting, it helps users project future costs based on historical usage, enabling better financial planning. Cost visualization tools allow users to create custom reports, charts, and graphs, providing a clear view of where their money is going. This functionality helps identify trends, pinpoint unexpected spikes in usage or cost, and explore opportunities for savings. By using AWS Cost Explorer, organizations can make informed decisions about their cloud expenditures, ensuring they stay within budget while optimizing their resource utilization."
      },
      {
        "title": "Real-time data analytics",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following AWS resources provide expert guidance and assistance for migrating to the AWS cloud? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Amazon Simple Storage Service (S3)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Elastic Compute Cloud (EC2)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Professional Services",
        "status": "right",
        "explanation": "AWS Professional Services is an ensemble of experts from AWS that helps customers achieve their business outcomes when migrating to the cloud. This team provides enterprise-level support by offering proven methods and expertise in cloud computing. By working with AWS Professional Services, customers can benefit from personalized guidance tailored to their specific needs, ensuring a smooth and efficient migration process. The team assists with planning, implementing best practices, optimizing performance and cost, and ensuring security and compliance standards are met. For customers who are looking to migrate applications to the cloud, AWS Professional Services is a valuable resource that can help navigate the complexities of cloud migration using AWS's best practices and tools."
      },
      {
        "title": "AWS Cost Explorer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Partner Network (APN)",
        "status": "right",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the benefit of automating the deployment process in DevOps?",
    "type": "radio",
    "options": [
      {
        "title": "Increased manual intervention and complexity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Increased deployment failures and downtime",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Improved consistency and repeatability of deployments",
        "status": "right",
        "explanation": "Automating the deployment process in DevOps can significantly improve the consistency and repeatability of deployments. Automation allows the same steps to be performed in the same order every time a deployment occurs, reducing the likelihood of human error and ensuring consistent application behavior. Moreover, it also speeds up the deployment process as tasks are executed quickly by automation tools, and it can also provide a historical record of what changes were made, when, and by whom, enhancing traceability and accountability."
      },
      {
        "title": "Decreased need for infrastructure scaling and optimization",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company has recently migrated to AWS and wants to optimize cost management strategies. They require a detailed breakdown of costs and usage to understand where resources are consumed. As an AWS Cloud Practitioner, which AWS service would you suggest for this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Cost and Usage Report",
        "status": "right",
        "explanation": "The AWS Cost and Usage Report provides the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, and reservations (e.g., Reserved Instances). This report enables businesses to understand their AWS bill better, manage their costs, and optimize spending. The report generates a detailed spreadsheet that breaks down costs by each service, linked accounts, and the specific usage that incurred the costs, which allows for precise chargeback and showback processes. The report can be tailored to show hourly or daily granularity and can be integrated with Amazon Athena for complex queries. Additionally, it supports the allocation of costs to specific business units, projects, or environments using cost allocation tags. Due to its detailed nature and customization options, the AWS Cost and Usage Report is the best choice for a company seeking to optimize cost management and gain a deep understanding of their AWS spending."
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Cost Explorer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Budgets",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "In the context of AWS, which of the following is the recommended migration strategy?",
    "type": "radio",
    "options": [
      {
        "title": "Gradually migrating workloads while maintaining legacy systems in parallel.",
        "status": "right",
        "explanation": "A gradual migration, where workloads are moved incrementally while legacy systems are still operational, is an effective strategy. This phased approach reduces risk by not disrupting existing operations and provides a fallback option in case of unforeseen issues. It also allows for continuous refinement of the migration process based on early experiences. Maintaining legacy systems in parallel ensures business continuity and gives time for business to adapt to the new cloud environment."
      },
      {
        "title": "Skipping the assessment phase to accelerate the migration process.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Focusing only on the technical aspects and ignoring business and operational considerations.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Migrating all critical applications simultaneously to maximize efficiency.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What factors influence the cost of AWS Shield Advanced for AWS organizations with multiple AWS accounts?",
    "type": "radio",
    "options": [
      {
        "title": "The type of AWS services used in each account",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The average usage of each AWS account",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "The number of AWS resources protected across all accounts",
        "status": "right",
        "explanation": "AWS Shield Advanced pricing is based on the number of resources (such as Amazon CloudFront distributions, Elastic Load Balancing load balancers, Amazon Route 53 hosted zones, and Elastic IP addresses) protected across all accounts. This service provides cost-effective DDoS protection for applications running on AWS, and is particularly beneficial for organizations with a high risk of DDoS attacks, as it offers financial protection in the form of DDoS cost protection for scaling charges incurred during an attack. The number of resources protected is a more accurate measure of the level of protection needed than the total number of AWS accounts or the average usage of each account. For example, an organization may have many AWS accounts with low resource usage, but if they have critical resources that require high levels of protection, then they will need to pay more for AWS Shield Advanced."
      },
      {
        "title": "The total number of AWS accounts in use",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A global company has multiple applications across multiple AWS accounts and regions. To enhance security, the company wants to ensure that these applications use temporary credentials when accessing AWS services. Which AWS service or feature meets these requirements?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Identity and Access Management (IAM)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon Cognito",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Security Token Service (AWS STS)",
        "status": "right",
        "explanation": "AWS Security Token Service (AWS STS) allows you to request temporary, limited-privilege credentials to access AWS services. This is ideal for applications that interact with AWS services and require credentials that are automatically rotated and short-lived, minimizing the risk of long-term credential compromise. STS is particularly useful in scenarios where applications need to assume different roles for accessing resources across multiple AWS services, enabling adherence to the principle of least privilege. By using STS, the application can request credentials dynamically as needed, which not only enhances security but also simplifies credential management. This approach aligns with AWS best practices for security and identity management, making AWS STS the best choice for applications that require secure, temporary access to AWS APIs without embedding long-term access keys within the application."
      },
      {
        "title": "AWS Key Management Service (KMS)",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is developing an application and wants an integrated solution for write access to an Amazon RDS database. What method should be used to ensure that the application has the required database permissions without sharing long-term AWS credentials?",
    "type": "radio",
    "options": [
      {
        "title": "Use an Amazon RDS IAM authentication.",
        "status": "right",
        "explanation": "Amazon RDS supports IAM authentication that can authenticate to a DB instance or DB cluster using IAM credentials. By using IAM database authentication, you donâ€™t need to use a password when connecting to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. This provides a more secure way to access the RDS instance without distributing or embedding long-term AWS credentials with the application."
      },
      {
        "title": "Hardcode the AWS IAM credentials in the application code.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Use AWS Lambda environment variables to store credentials.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Share the AWS root account credentials with the application.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An organization wants to redesign a monolithic application into microservices on AWS. What will be the main benefits of adopting the microservices architecture?",
    "type": "radio",
    "options": [
      {
        "title": "This architecture significantly decreases management overhead.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "This architecture reduces the complexity of component-to-component communication.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "This architecture enables independent scaling of application components.",
        "status": "right",
        "explanation": "Microservices architecture allows an application to be divided into smaller, independent components, each performing a distinct function. These components, or microservices, can be deployed, updated, and scaled independently. This modularity means that if one service experiences a surge in demand, only that particular microservice needs to be scaled, which leads to more efficient resource utilization and cost savings. This feature is especially useful for organizations that experience variable load patterns, ensuring they can handle peak loads without over-provisioning resources during regular operation."
      },
      {
        "title": "This architecture provides a maximum threshold limit to save costs.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which service enables you to effectively organize and manage AWS resources across different environments such as development, testing, and production?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Resource Groups",
        "status": "right",
        "explanation": "AWS Resource Groups helps you to group your AWS resources, which can then be managed and automated as a collection. This service is especially useful for environments such as development, testing, and production because it allows you to apply tags and organize resources according to their specific criteria. For example, you can group all resources that belong to a particular application, project, or environment, enabling easier management and automation of tasks like monitoring and cost allocation. The tagging system provided by AWS Resource Groups enhances clear categorization and simplified operational workflows across different environments."
      },
      {
        "title": "AWS CloudFormation",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Trusted Advisor",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Placement Groups",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A startup is developing an agricultural application using advanced analytics and managing large volumes of data sets. Which AWS service is most suitable for meeting this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Redshift",
        "status": "right",
        "explanation": "Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. It provides fast query performance by using columnar storage technology and parallelizing queries across multiple nodes. For a startup in the agricultural sector looking to perform advanced analytics and forecasting on large datasets, Redshift would be the most suitable as it is designed specifically for heavy-duty analytics on large datasets. It scales according to the needs of the company and provides the flexibility to analyze data with your preferred analysis tools."
      },
      {
        "title": "Amazon RDS",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS S3",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS EC2",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A gaming company wants to develop an online game. They need a database to store session history and leaderboards that support low latency and high consistency. Which AWS service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "Amazon MemoryDB",
        "status": "right",
        "explanation": "Amazon MemoryDB is a fully managed in-memory database service built on an architecture designed for durability and fault tolerance. It is designed to support applications requiring microsecond read latency and high-speed data ingestion, making it a perfect fit for a gaming application like the one described. Amazon MemoryDB supports data structures such as strings, lists, sets, sorted sets, hashes, and streams â€“ features that will be beneficial for maintaining gaming sessions and leaderboards. It ensures high availability by replicating data across multiple Availability Zones, providing a multi-AZ fault-tolerant architecture that makes it suitable for use cases demanding high reliability and business continuity."
      },
      {
        "title": "Amazon DynamoDB",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon ElastiCache",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon RDS",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An organization wants to expand its cloud infrastructure on AWS. They must ensure that the development team has access to the latest AWS technologies and best practices for implementing secure, high-performing, resilient, and efficient infrastructure solutions. Which AWS resources would you recommend for guidance on architectural best practices? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "AWS Knowledge Center",
        "status": "right",
        "explanation": "The AWS Knowledge Center is a valuable resource that offers answers to many of the questions about how to operate and troubleshoot various AWS services. It compiles insights and best practices from AWS support engineers, based on the common issues and solutions they encounter. This makes it an excellent tool for developers seeking to quickly resolve operational challenges or understand best practices for using AWS services. The Knowledge Center helps teams avoid common pitfalls and use AWS capabilities more effectively, ensuring they can maintain secure, high-performing, and reliable applications while optimizing cost."
      },
      {
        "title": "AWS Service Health Dashboard",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Well-Architected Tool",
        "status": "right",
        "explanation": "AWS Well-Architected Tool helps to build secure, high-performing, resilient, and efficient infrastructure for applications. Based on the AWS Well-Architected Framework, this provides a consistent approach for evaluating architectures and implementing designs that will scale over time. It offers guidance across five pillars: operational excellence, security, reliability, performance efficiency, and cost optimization. By using the AWS Well-Architected Tool, developers can assess their workloads against best practices and receive recommendations for improvements, making it an indispensable resource for ensuring well-architected solutions on AWS."
      },
      {
        "title": "Amazon Virtual Private Cloud (VPC)",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Amazon DynamoDB",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "An e-commerce company is deploying an application in a VPC that will work with an Amazon RDS database. They want the RDS instance to be secure from online threats but remain connected to the application. Which action meets this requirement?",
    "type": "radio",
    "options": [
      {
        "title": "Use AWS IAM roles to limit access to the RDS database from the application.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Place both the application and RDS database in public subnets but ensure the database's endpoint isn't publicly accessible.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Place the RDS database in a private subnet and configure the route tables to ensure the application can access it.",
        "status": "right",
        "explanation": "Amazon Virtual Private Cloud (VPC) enables users to provision a logically isolated section of the AWS Cloud where they can launch resources in a virtual network that they define. Within a VPC, you can have multiple subnets, which are distinct blocks of IP addresses. These subnets can be designated as public (accessible from the internet) or private (not directly accessible from the internet). Route tables in a VPC determine how traffic is directed between the subnets, the internet, and other AWS services. Placing the RDS database in a private subnet ensures it's not directly reachable from the public internet. A private subnet in a VPC doesn't allow direct inbound traffic from outside the VPC. By adjusting the route tables and security group configurations, the application, even if it's in a different subnet, can access the RDS database. This approach meets the requirements of keeping the database secure from internet threats while making it available to the necessary internal applications."
      },
      {
        "title": "Place the RDS database in a public subnet and use a security group to restrict inbound traffic.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following AWS Support Plans provides programmatic case management to create, manage, and close support cases? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Basic",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Developer",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Corporate",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Enterprise",
        "status": "right",
        "explanation": "AWS Enterprise Support provides the highest level of support available from AWS and includes programmatic case management. It provides 24x7 technical support from high-quality engineers, tools and technology to automatically manage health and support cases, as well as a designated Technical Account Manager who provides proactive advice and strategic planning."
      },
      {
        "title": "Business",
        "status": "right",
        "explanation": "The AWS Business Support plan provides programmatic case management. This means you can use the AWS Support API actions to create, describe, list, and resolve AWS Support cases programmatically. This allows you to manage your support cases without going to the AWS Support Center website."
      }
    ]
  },
  {
    "title": "AWS offers five support plans for customers. Which of the following can be accessed from the Developer support plan? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Application architecture guidance",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Service health checks",
        "status": "right",
        "explanation": "Service health checks are part of the Developer Support plan. These checks allow users to monitor the health of AWS services that could affect their AWS resources. They provide visibility into the ongoing status of the AWS services and are an integral part of maintaining application health and performance."
      },
      {
        "title": "Technical account manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Client-side diagnostic tools",
        "status": "right",
        "explanation": "The Developer Support plan provides access to AWS Trusted Advisor and AWS Personal Health Dashboard. Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. The Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you."
      },
      {
        "title": "Management business reviews",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "What is the AWS's responsibility under the shared responsibility model in cloud environments?",
    "type": "radio",
    "options": [
      {
        "title": "Configuring security group settings for an Amazon EC2 instance.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Implementing identity federation between corporate directories and AWS services.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Patching the guest operating system of Amazon EC2 instances.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Ensuring the infrastructure for Amazon S3 is highly available and durable.",
        "status": "right",
        "explanation": "AWS is responsible for maintaining the infrastructure that supports cloud services. For Amazon S3, this means ensuring that the service is always available and that the data stored is durable, meaning it is maintained with high integrity and can be reliably retrieved. AWS designs and maintains the network, hardware, and facilities that run the S3 service, which provides customers with a robust platform for storage solutions."
      }
    ]
  },
  {
    "title": "What can be used to grant permission to access AWS resources from members' accounts in the AWS organization?",
    "type": "radio",
    "options": [
      {
        "title": "Service control policies (SCPs)",
        "status": "right",
        "explanation": "AWS Service Control Policies (SCPs) are a type of policy that you can use to manage permissions in your AWS organization. SCPs enable you to define the maximum permissions for member accounts in the organization. Unlike Identity and Access Management (IAM) policies, which grant permissions to users, groups, and roles, SCPs act as guardrails that limit what actions members of an organization can and cannot perform, regardless of their IAM policies. SCPs are useful for ensuring compliance with data governance and security standards by centrally controlling access to AWS services and resources across multiple AWS accounts within an AWS Organization. They provide a way to enforce policy compliance at the organization, organizational unit (OU), or account level."
      },
      {
        "title": "Routing policy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "IAM Policy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Access key",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following are the advantages of adopting a decoupled design? (Select TWO.)",
    "type": "checkbox",
    "options": [
      {
        "title": "Ensures automatic compliance with all regulations.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Decoupled architectures will always reduce costs.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Supports rapid development and deployment.",
        "status": "right",
        "explanation": "Decoupled architectures break down a large application into smaller, independent components, often known as microservices. Each component or service focuses on a specific function. By doing this, developers can work on separate components concurrently, accelerating development cycles. Deployments can be quicker since only the modified component might need to be redeployed, not the entire application."
      },
      {
        "title": "One component outage leads to complete system failure.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Allowing components to scale independently.",
        "status": "right",
        "explanation": "One of the primary advantages of a decoupled design is the capacity to scale components based on demand. If a particular service experiences increased load, only that service can be scaled, without affecting or needing to scale other parts of the application. This approach offers more efficient resource utilization and can manage peak traffic situations more effectively."
      }
    ]
  },
  {
    "title": "A company operates an application running on an Amazon EC2 instance in the Europe region. Now they want to move to North America. What should they do to deploy the application to another region?",
    "type": "radio",
    "options": [
      {
        "title": "Donâ€™t need to do anything, just deploy a new application to the region.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create an Amazon Machine Image and deploy that region.",
        "status": "right",
        "explanation": "Amazon Machine Images (AMI) serve as the templates for launching instances in the Amazon Elastic Compute Cloud (EC2). They include the operating system, application server, and applications required to launch an instance, which is a virtual server in the AWS cloud. If a company plans to move its application running on an EC2 instance to a different region, creating an AMI of the existing EC2 instance and then deploying it in the new region is an effective and efficient way to achieve this. The process involves creating an AMI from the existing EC2 instance in the current region, copying that AMI to the new region, and then launching a new EC2 instance from the copied AMI in the new region. This method ensures that the new EC2 instance has the same configuration, installed software, and file system as the original instance."
      },
      {
        "title": "Create a separate AWS account for that region.",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Create a support case to get this migration help.",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "Which of the following is the recommended design principle for achieving performance efficiency in AWS?",
    "type": "radio",
    "options": [
      {
        "title": "Use serverless architectures",
        "status": "right",
        "explanation": "Using serverless architectures is a key design principle recommended by AWS for achieving performance efficiency. Serverless architectures allow you to build and run applications and services without having to manage servers. AWS handles the infrastructure management tasks such as server and operating system maintenance, capacity provisioning, automatic scaling, code monitoring, and logging. By using services like AWS Lambda, you can run your code without provisioning or managing servers, which can improve system performance and enable you to focus on your application code."
      },
      {
        "title": "Maximize server capacity",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Maximize data redundancy",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Minimize data processing",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company is moving its IT infrastructure from an on-premises data center to the AWS cloud. Which costs fall directly on the company's responsibility?",
    "type": "radio",
    "options": [
      {
        "title": "The cost associated with AWS's hardware infrastructure",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Electricity expenses for AWS's servers",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "Expenses related to licenses for application software",
        "status": "right",
        "explanation": "When a company moves its operations to the AWS Cloud, it is still responsible for the cost of application software licenses. This means if the company uses any proprietary software that requires a license such as a database or a specific application server, the company must manage and pay for these licenses separately. The software licenses fall under the company's financial obligations that are not included in AWS Marketplace offerings or are not part of an open-source or AWS-owned solution. This aspect is crucial for budgeting and compliance, ensuring that the company adheres to licensing agreements while using the cloud's scalability and flexibility."
      },
      {
        "title": "The cost of ensuring physical security at AWS data centers",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company's security team observed that certain IAM Roles were shared with external entities. Which AWS service or feature is capable of detecting such configurations?",
    "type": "radio",
    "options": [
      {
        "title": "AWS IAM Access Analyzer",
        "status": "right",
        "explanation": "AWS IAM Access Analyzer is a feature designed to help you identify the resources in your AWS account such as Amazon S3 buckets or IAM roles that are shared with an external entity. It does this by analyzing resource-based policies to show you which policies allow access to your resources from outside your AWS account. This feature is incredibly useful for maintaining the security and privacy of your data, ensuring that only the intended parties have access. When you enable Access Analyzer, it automatically starts analyzing policies and generating findings. These findings provide detailed information about the resources accessible from outside your AWS account, the type of access allowed, and the external entity that has access. This allows you to review and take action to restrict access if necessary, helping you adhere to the principle of least privilege and enhance your security posture."
      },
      {
        "title": "AWS Managed Services",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Systems Manager",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Control Tower",
        "status": "wrong",
        "explanation": ""
      }
    ]
  },
  {
    "title": "A company needs to migrate 70 TB of data offline to the AWS cloud. As a Cloud Practitioner, which service would you recommend?",
    "type": "radio",
    "options": [
      {
        "title": "AWS Snowcone",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Storage Gateway",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Snowmobile",
        "status": "wrong",
        "explanation": ""
      },
      {
        "title": "AWS Snowball",
        "status": "right",
        "explanation": "AWS Snowball is a data transport solution that uses secure devices to transfer large amounts of data into and out of the AWS Cloud. It addresses common challenges with large-scale data transfers such as high network costs, long transfer times, and security concerns. Transferring data with Snowball is simple, fast, more secure, and can be one-fifth the cost of high-speed internet. Snowball uses end-to-end encryption and tamper-resistant enclosures to help ensure the security of your data. This is important when you're dealing with large amounts of sensitive data, as it's crucial to keep that data secure during transit. For a data migration scenario involving 70 TB of data, Snowball is the ideal choice. A single Snowball device can support data transfers up to 80 TB, which is well within the requirements of this task. The process is relatively straightforward: AWS ships a Snowball device to your location, you load your data onto the device using the Snowball client, and then ship it back to AWS. Once the device is back at an AWS data center, your data is imported into Amazon S3."
      }
    ]
  }
]
